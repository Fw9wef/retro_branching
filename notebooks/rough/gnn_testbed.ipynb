{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ecole\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "\n",
    "import networkx as nx\n",
    "# from networkx.drawing.nx_pydot import graphviz_layout\n",
    "import numpy as np\n",
    "import ml_collections\n",
    "import copy\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backend `retro_branching` stuff\n",
    "\n",
    "Putting minimal `retro_branching` implementation here so can reproduce in this notebook without needing to setup whole of `retro_branching`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "######################### ENV ##################################\n",
    "default_scip_params = {'separating/maxrounds': 0,\n",
    "                       'separating/maxroundsroot': 0,\n",
    "                       'separating/maxcuts': 0,\n",
    "                       'separating/maxcutsroot': 0,\n",
    "                       'presolving/maxrounds': 0,\n",
    "                       'presolving/maxrestarts': 0,\n",
    "                       'propagating/maxrounds':0,\n",
    "                       'propagating/maxroundsroot':0,\n",
    "                       'lp/initalgorithm':'d',\n",
    "                       'lp/resolvealgorithm':'d',\n",
    "                       'limits/time': 3600}\n",
    "\n",
    "class EcoleBranching(ecole.environment.Branching):\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_function='default',\n",
    "        information_function='default',\n",
    "        reward_function='default',\n",
    "        scip_params='default',\n",
    "        pseudo_candidates=False,\n",
    "    ):\n",
    "        # save string names so easy to initialise new environments\n",
    "        if type(observation_function) == str:\n",
    "            self.str_observation_function = observation_function\n",
    "        else:\n",
    "            self.str_observation_function = None\n",
    "        if type(information_function) == str:\n",
    "            self.str_information_function = information_function\n",
    "        else:\n",
    "            self.str_information_function = None\n",
    "        if type(reward_function) == str:\n",
    "            self.str_reward_function = reward_function\n",
    "        else:\n",
    "            self.str_reward_function = None\n",
    "        if type(scip_params) == str:\n",
    "            self.str_scip_params = scip_params\n",
    "        else:\n",
    "            self.str_scip_params = None\n",
    "\n",
    "        self.pseudo_candidates = pseudo_candidates\n",
    "\n",
    "        # init functions from strings if needed\n",
    "        if reward_function == 'default':\n",
    "            reward_function = ({\n",
    "                     'num_nodes': -ecole.reward.NNodes(),\n",
    "                     'lp_iterations': -ecole.reward.LpIterations(),\n",
    "                     'primal_integral': -ecole.reward.PrimalIntegral(),\n",
    "                     'dual_integral': ecole.reward.DualIntegral(),\n",
    "                     'primal_dual_integral': -ecole.reward.PrimalDualIntegral(),\n",
    "                     'solving_time': -ecole.reward.SolvingTime(),\n",
    "                     'normalised_lp_gain': NormalisedLPGain(use_prev_primal_bound=True)\n",
    "                 })\n",
    "        if information_function == 'default':\n",
    "            information_function=({\n",
    "                     'num_nodes': ecole.reward.NNodes().cumsum(),\n",
    "                     'lp_iterations': ecole.reward.LpIterations().cumsum(),\n",
    "                     'solving_time': ecole.reward.SolvingTime().cumsum(),\n",
    "                 })\n",
    "        if observation_function == 'default':    \n",
    "            observation_function = (ecole.observation.NodeBipartite())\n",
    "        elif observation_function == '43_var_features':\n",
    "            observation_function = (NodeBipariteWith43VariableFeatures())\n",
    "        if scip_params == 'default':\n",
    "            scip_params = default_scip_params\n",
    "        \n",
    "        super(EcoleBranching, self).__init__(\n",
    "            observation_function=observation_function,\n",
    "            information_function=information_function,\n",
    "            reward_function=reward_function,\n",
    "            scip_params=scip_params,\n",
    "            pseudo_candidates=pseudo_candidates,\n",
    "        )\n",
    "        \n",
    "class NodeBipariteWith43VariableFeatures(ecole.observation.NodeBipartite):\n",
    "    '''\n",
    "    Adds (mostly global) features to variable node features.\n",
    "\n",
    "    Adds 24 extra variable features to each variable on top of standard ecole\n",
    "    NodeBipartite obs variable features (19), so each variable will have\n",
    "    43 features in total.\n",
    "\n",
    "    '''\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def before_reset(self, model):\n",
    "        super().before_reset(model)\n",
    "        \n",
    "        self.init_dual_bound = None\n",
    "        self.init_primal_bound = None\n",
    "        \n",
    "        \n",
    "    def extract(self, model, done):\n",
    "        # get the NodeBipartite obs\n",
    "        obs = super().extract(model, done)\n",
    "        \n",
    "        m = model.as_pyscipopt()\n",
    "        \n",
    "        if self.init_dual_bound is None:\n",
    "            self.init_dual_bound = m.getDualbound()\n",
    "            self.init_primal_bound = m.getPrimalbound()\n",
    "            \n",
    "        # dual/primal bound features\n",
    "        dual_bound_frac_change = abs(self.init_dual_bound - m.getDualbound()) / self.init_dual_bound\n",
    "        primal_bound_frac_change = abs(self.init_primal_bound - m.getPrimalbound()) / self.init_primal_bound\n",
    "\n",
    "        primal_dual_gap = abs(m.getPrimalbound() - m.getDualbound())\n",
    "        max_dual_bound_frac_change = primal_dual_gap / self.init_dual_bound\n",
    "        max_primal_bound_frac_change = primal_dual_gap / self.init_primal_bound\n",
    "\n",
    "        curr_primal_dual_bound_gap_frac = m.getGap()\n",
    "        \n",
    "        # global tree features\n",
    "        num_leaves_frac = m.getNLeaves() / m.getNNodes()\n",
    "        num_feasible_leaves_frac = m.getNFeasibleLeaves() / m.getNNodes()\n",
    "        num_infeasible_leaves_frac = m.getNInfeasibleLeaves() / m.getNNodes()\n",
    "        num_lp_iterations_frac = m.getNNodes() / m.getNLPIterations()\n",
    "        \n",
    "        # focus node features\n",
    "        num_siblings_frac = m.getNSiblings() / m.getNNodes()\n",
    "        curr_node = m.getCurrentNode()\n",
    "        best_node = m.getBestNode()\n",
    "        if best_node is not None:\n",
    "            if curr_node.getNumber() == best_node.getNumber():\n",
    "                is_curr_node_best = 1\n",
    "            else:\n",
    "                is_curr_node_best = 0\n",
    "        else:\n",
    "            # no best node found yet\n",
    "            is_curr_node_best = 0\n",
    "        parent_node = curr_node.getParent()\n",
    "        if parent_node is not None and best_node is not None:\n",
    "            if parent_node.getNumber() == best_node.getNumber():\n",
    "                is_curr_node_parent_best = 1\n",
    "            else:\n",
    "                is_curr_node_parent_best = 0\n",
    "        else:\n",
    "            # node has no parent node or no best node found yet\n",
    "            is_curr_node_parent_best = 0\n",
    "        curr_node_depth = m.getDepth() / m.getNNodes()\n",
    "        curr_node_lower_bound_relative_to_init_dual_bound = self.init_dual_bound / curr_node.getLowerbound()\n",
    "        curr_node_lower_bound_relative_to_curr_dual_bound =  m.getDualbound() / curr_node.getLowerbound()\n",
    "        num_branching_changes, num_constraint_prop_changes, num_prop_changes = curr_node.getNDomchg()\n",
    "        total_num_changes = num_branching_changes + num_constraint_prop_changes + num_prop_changes\n",
    "        try:\n",
    "            branching_changes_frac = num_branching_changes / total_num_changes\n",
    "        except ZeroDivisionError:\n",
    "            branching_changes_frac = 0\n",
    "        try:\n",
    "            constraint_prop_changes_frac = num_constraint_prop_changes / total_num_changes\n",
    "        except ZeroDivisionError:\n",
    "            constraint_prop_changes_frac = 0\n",
    "        try:\n",
    "            prop_changes_frac = num_prop_changes / total_num_changes\n",
    "        except ZeroDivisionError:\n",
    "            prop_changes_frac = 0\n",
    "        parent_branching_changes_frac = curr_node.getNParentBranchings() / m.getNNodes()\n",
    "        best_sibling = m.getBestSibling()\n",
    "        if best_sibling is None:\n",
    "            is_best_sibling_none = 1\n",
    "            is_best_sibling_best_node = 0\n",
    "        else:\n",
    "            is_best_sibling_none = 0\n",
    "            if best_node is not None:\n",
    "                if best_sibling.getNumber() == best_node.getNumber():\n",
    "                    is_best_sibling_best_node = 1\n",
    "                else:\n",
    "                    is_best_sibling_best_node = 0\n",
    "            else:\n",
    "                is_best_sibling_best_node = 0\n",
    "        if best_sibling is not None:\n",
    "            best_sibling_lower_bound_relative_to_init_dual_bound = self.init_dual_bound / best_sibling.getLowerbound()\n",
    "            best_sibling_lower_bound_relative_to_curr_dual_bound = m.getDualbound() / best_sibling.getLowerbound()\n",
    "            best_sibling_lower_bound_relative_to_curr_node_lower_bound = best_sibling.getLowerbound() / curr_node.getLowerbound()\n",
    "        else:\n",
    "            best_sibling_lower_bound_relative_to_init_dual_bound = 0\n",
    "            best_sibling_lower_bound_relative_to_curr_dual_bound = 0\n",
    "            best_sibling_lower_bound_relative_to_curr_node_lower_bound = 0\n",
    "        \n",
    "        # add feats to each variable\n",
    "        feats_to_add = np.array([[dual_bound_frac_change,\n",
    "                                 primal_bound_frac_change,\n",
    "                                 max_primal_bound_frac_change,\n",
    "                                 max_dual_bound_frac_change,\n",
    "                                 curr_primal_dual_bound_gap_frac,\n",
    "                                 num_leaves_frac,\n",
    "                                 num_feasible_leaves_frac,\n",
    "                                 num_infeasible_leaves_frac,\n",
    "                                 num_lp_iterations_frac,\n",
    "                                 num_siblings_frac,\n",
    "                                 is_curr_node_best,\n",
    "                                 is_curr_node_parent_best,\n",
    "                                 curr_node_depth,\n",
    "                                 curr_node_lower_bound_relative_to_init_dual_bound,\n",
    "                                 curr_node_lower_bound_relative_to_curr_dual_bound,\n",
    "                                 branching_changes_frac,\n",
    "                                 constraint_prop_changes_frac,\n",
    "                                 prop_changes_frac,\n",
    "                                 parent_branching_changes_frac,\n",
    "                                 is_best_sibling_none,\n",
    "                                 is_best_sibling_best_node,\n",
    "                                 best_sibling_lower_bound_relative_to_init_dual_bound,\n",
    "                                 best_sibling_lower_bound_relative_to_curr_dual_bound,\n",
    "                                 best_sibling_lower_bound_relative_to_curr_node_lower_bound] for _ in range(obs.column_features.shape[0])])\n",
    "        \n",
    "        obs.column_features = np.column_stack((obs.column_features, feats_to_add))\n",
    "\n",
    "                \n",
    "        return obs\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "################################## CUSTOM REWARD ##################################\n",
    "class NormalisedLPGain:\n",
    "    def __init__(self, use_prev_primal_bound=True):\n",
    "        '''\n",
    "        Args:\n",
    "            use_prev_primal_bound (bool): If True, will normalise the reward w.r.t.\n",
    "                the previous step's primal bound (i.e. the primal bound of the\n",
    "                instance at the step when the agent took the branching action).\n",
    "                Otherwise, will normalise w.r.t. the primal bound of the new step.\n",
    "        '''\n",
    "        self.use_prev_primal_bound = use_prev_primal_bound\n",
    "\n",
    "    def before_reset(self, model):\n",
    "        self.prev_node = None\n",
    "        self.prev_node_id = None\n",
    "        self.prev_primal_bound = None\n",
    "\n",
    "    def extract(self, model, done):\n",
    "        m = model.as_pyscipopt()\n",
    "\n",
    "        if self.prev_node_id is None:\n",
    "            # not yet started, update prev node for next step\n",
    "            self.prev_node = m.getCurrentNode()\n",
    "            if self.prev_node is not None:\n",
    "                self.tree = SearchTree(model)\n",
    "                self.prev_node_id = copy.deepcopy(self.prev_node.getNumber())\n",
    "                self.prev_primal_bound = m.getPrimalbound()\n",
    "            return 0\n",
    "\n",
    "        # update search tree with current model state\n",
    "        self.tree.update_tree(model)\n",
    "        \n",
    "        # collect node stats from children introduced from previous branching decision\n",
    "        prev_node_lb = self.tree.tree.nodes[self.prev_node_id]['lower_bound']\n",
    "        prev_node_child_ids = [child for child in self.tree.tree.successors(self.prev_node_id)]\n",
    "        prev_node_child_lbs = [self.tree.tree.nodes[child]['lower_bound'] for child in prev_node_child_ids]\n",
    "\n",
    "        # calc reward for previous branching decision\n",
    "        if len(prev_node_child_lbs) > 0:\n",
    "            # use child lp gains to retrospectively calculate a score for the previous branching decision\n",
    "            score = -1\n",
    "            for child_node_lb in prev_node_child_lbs:\n",
    "                if self.use_prev_primal_bound:\n",
    "                    # use primal bound of step branching action was taken\n",
    "                    score *= (self.prev_primal_bound - child_node_lb) / (self.prev_primal_bound - prev_node_lb)\n",
    "                else:\n",
    "                    # use primal bound of new step\n",
    "                    score *= (m.getPrimalbound() - child_node_lb) / (m.getPrimalbound() - prev_node_lb)\n",
    "        else:\n",
    "            # previous branching decision led to all child nodes being pruned, infeasible, or outside bounds -> don't punish brancher\n",
    "            score = 0\n",
    "\n",
    "        if m.getCurrentNode() is not None:\n",
    "            # update stats for next step\n",
    "            self.prev_node = m.getCurrentNode()\n",
    "            self.prev_node_id = copy.deepcopy(self.prev_node.getNumber())\n",
    "            self.prev_primal_bound = m.getPrimalbound()\n",
    "        else:\n",
    "            # instance completed, no current focus node\n",
    "            pass\n",
    "        \n",
    "        return score\n",
    "\n",
    "class SearchTree:\n",
    "    '''\n",
    "    Tracks SCIP search tree. Call SearchTree.update_tree(ecole.Model) each\n",
    "    time the ecole environment (and therefore the ecole.Model) is updated.\n",
    "\n",
    "    N.B. SCIP does not store nodes which were pruned, infeasible, or outside\n",
    "    the search tree's optimality bounds, therefore these nodes will not be\n",
    "    stored in the SearchTree. This is why m.getNTotalNodes() (the total number\n",
    "    of nodes processed by SCIP) will likely be more than the number of nodes in\n",
    "    the search tree when an instance is solved.\n",
    "    '''\n",
    "    def __init__(self, model):\n",
    "        self.tree = nx.DiGraph()\n",
    "        self.update_tree(model)\n",
    "    \n",
    "    def update_tree(self, model):\n",
    "        '''\n",
    "        Call this method after each update to the ecole environment. Pass\n",
    "        the updated ecole.Model, and the B&B tree tracker will be updated accordingly.\n",
    "        '''\n",
    "        m = model.as_pyscipopt()\n",
    "        \n",
    "        _curr_node = m.getCurrentNode()\n",
    "        if _curr_node is not None:\n",
    "            curr_node_id = _curr_node.getNumber()\n",
    "        else:\n",
    "            # branching finished, no curr node\n",
    "            curr_node_id = None\n",
    "        self.curr_node = {curr_node_id: _curr_node}\n",
    "        if curr_node_id is not None:\n",
    "            self._add_nodes(self.curr_node)\n",
    "        \n",
    "        if curr_node_id is not None:\n",
    "            _parent_node = list(self.curr_node.values())[0].getParent()\n",
    "            if _parent_node is not None:\n",
    "                parent_node_id = _parent_node.getNumber()\n",
    "            else:\n",
    "                # curr node is root node\n",
    "                parent_node_id = None\n",
    "            self.parent_node = {parent_node_id: _parent_node}\n",
    "        else:\n",
    "            self.parent_node = {None: None}\n",
    "            \n",
    "        open_leaves, open_children, open_siblings = m.getOpenNodes()\n",
    "        self.open_leaves = {node.getNumber(): node  for node in open_leaves}\n",
    "        self.open_children = {node.getNumber(): node for node in open_children}\n",
    "        self.open_siblings = {node.getNumber(): node for node in open_siblings}\n",
    "        \n",
    "        self._add_nodes(self.open_leaves)\n",
    "        self._add_nodes(self.open_children)\n",
    "        self._add_nodes(self.open_siblings)\n",
    "        \n",
    "    def _add_nodes(self, nodes):\n",
    "        '''Adds nodes if not already in tree.'''\n",
    "        for node_id, node in nodes.items():\n",
    "            if node_id not in self.tree:\n",
    "                # add node\n",
    "                self.tree.add_node(node_id,\n",
    "                                   _id=node_id,\n",
    "                                   lower_bound=node.getLowerbound())\n",
    "\n",
    "                # add edge\n",
    "                _parent_node = node.getParent()\n",
    "                if _parent_node is not None:\n",
    "                    parent_node_id = _parent_node.getNumber()\n",
    "                    self.tree.add_edge(parent_node_id,\n",
    "                                       node_id)\n",
    "                else:\n",
    "                    # is root node, has no parent\n",
    "                    pass\n",
    "                \n",
    "    def render(self):\n",
    "        '''Renders B&B search tree.'''\n",
    "        fig = plt.figure()\n",
    "        \n",
    "        pos = graphviz_layout(self.tree, prog='dot')\n",
    "        node_labels = {node: node for node in self.tree.nodes}\n",
    "        nx.draw_networkx_nodes(self.tree,\n",
    "                               pos,\n",
    "                               label=node_labels)\n",
    "        nx.draw_networkx_edges(self.tree,\n",
    "                               pos)\n",
    "        \n",
    "        nx.draw_networkx_labels(self.tree, pos, labels=node_labels)\n",
    "        \n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BipartiteGCN(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 device, \n",
    "                 config=None,\n",
    "                 emb_size=64,\n",
    "                 num_rounds=1,\n",
    "                 aggregator='add',\n",
    "                 activation=None,\n",
    "                 cons_nfeats=5,\n",
    "                 edge_nfeats=1,\n",
    "                 var_nfeats=19,\n",
    "                 num_heads=1,\n",
    "                 linear_weight_init=None,\n",
    "                 linear_bias_init=None,\n",
    "                 layernorm_weight_init=None,\n",
    "                 layernorm_bias_init=None,\n",
    "                 head_aggregator=None,\n",
    "                 name='gnn'):\n",
    "        '''\n",
    "        Args:\n",
    "            config (str, ml_collections.ConfigDict()): If not None, will initialise \n",
    "                from config dict. Can be either string (path to config.json) or\n",
    "                ml_collections.ConfigDict object.\n",
    "            activation (None, 'sigmoid', 'relu', 'leaky_relu', 'inverse_leaky_relu', 'elu', 'hard_swish',\n",
    "                'softplus', 'mish', 'softsign')\n",
    "            num_heads (int): Number of heads (final layers) to use. Will use\n",
    "                head_aggregator to reduce all heads.\n",
    "            linear_weight_init (None, 'uniform', 'normal', \n",
    "                'xavier_uniform', 'xavier_normal', 'kaiming_uniform', 'kaiming_normal')\n",
    "            linear_bias_init (None, 'zeros', 'normal')\n",
    "            layernorm_weight_init (None, 'normal')\n",
    "            layernorm_bias_init (None, 'zeros', 'normal')\n",
    "            head_aggregator (None, 'add', 'mean'): Reduce operation to use to aggregate outputs\n",
    "                of heads. If None, forward() will return output of each head.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        if config is not None:\n",
    "            self.init_from_config(config)\n",
    "        else:\n",
    "            self.name = name\n",
    "            self.init_nn_modules(emb_size=emb_size, \n",
    "                                 num_rounds=num_rounds, \n",
    "                                 cons_nfeats=cons_nfeats, \n",
    "                                 edge_nfeats=edge_nfeats, \n",
    "                                 var_nfeats=var_nfeats, \n",
    "                                 aggregator=aggregator, \n",
    "                                 activation=activation, \n",
    "                                 num_heads=num_heads,\n",
    "                                 linear_weight_init=linear_weight_init,\n",
    "                                 linear_bias_init=linear_bias_init,\n",
    "                                 layernorm_weight_init=layernorm_weight_init,\n",
    "                                 layernorm_bias_init=layernorm_bias_init,\n",
    "                                 head_aggregator=head_aggregator)\n",
    "\n",
    "        self.printed_warning = False\n",
    "        self.to(self.device)\n",
    "\n",
    "    def init_from_config(self, config):\n",
    "        if type(config) == str:\n",
    "            # load from json\n",
    "            with open(config, 'r') as f:\n",
    "                json_config = json.load(f)\n",
    "                config = ml_collections.ConfigDict(json.loads(json_config))\n",
    "        self.name = config.name\n",
    "        if 'activation' not in config.keys():\n",
    "            config.activation = None\n",
    "        if 'num_heads' not in config.keys():\n",
    "            config.num_heads = 1\n",
    "        if 'linear_weight_init' not in config.keys():\n",
    "            config.linear_weight_init = None\n",
    "        if 'linear_bias_init' not in config.keys():\n",
    "            config.linear_bias_init = None\n",
    "        if 'layernorm_weight_init' not in config.keys():\n",
    "            config.layernorm_weight_init = None\n",
    "        if 'layernorm_bias_init' not in config.keys():\n",
    "            config.layernorm_bias_init = None\n",
    "\n",
    "            config.linear_bias_init = None\n",
    "\n",
    "        if 'head_aggregator' not in config:\n",
    "            config.head_aggregator = None\n",
    "\n",
    "        self.init_nn_modules(emb_size=config.emb_size, \n",
    "                             num_rounds=config.num_rounds, \n",
    "                             cons_nfeats=config.cons_nfeats, \n",
    "                             edge_nfeats=config.edge_nfeats, \n",
    "                             var_nfeats=config.var_nfeats, \n",
    "                             aggregator=config.aggregator, \n",
    "                             activation=config.activation,\n",
    "                             num_heads=config.num_heads,\n",
    "                             linear_weight_init=config.linear_weight_init,\n",
    "                             linear_bias_init=config.linear_bias_init,\n",
    "                             layernorm_weight_init=config.layernorm_weight_init,\n",
    "                             layernorm_bias_init=config.layernorm_bias_init,\n",
    "                             head_aggregator=config.head_aggregator)\n",
    "\n",
    "    def get_networks(self):\n",
    "        return {'network': self}\n",
    "\n",
    "    def init_model_parameters(self):\n",
    "\n",
    "        def init_params(m):\n",
    "            if isinstance(m, torch.nn.Linear):\n",
    "                # weights\n",
    "                if self.linear_weight_init is None:\n",
    "                    pass\n",
    "                elif self.linear_weight_init == 'uniform':\n",
    "                    torch.nn.init.uniform_(m.weight, a=0.0, b=1.0)\n",
    "                elif self.linear_weight_init == 'normal':\n",
    "                    torch.nn.init.normal_(m.weight, mean=0.0, std=0.01)\n",
    "                elif self.linear_weight_init == 'xavier_uniform':\n",
    "                    torch.nn.init.xavier_uniform_(m.weight, gain=torch.nn.init.calculate_gain(self.activation))\n",
    "                elif self.linear_weight_init == 'xavier_normal':\n",
    "                    torch.nn.init.xavier_normal_(m.weight, gain=torch.nn.init.calculate_gain(self.activation))\n",
    "                elif self.linear_weight_init == 'kaiming_uniform':\n",
    "                    torch.nn.init.kaiming_uniform_(m.weight, nonlinearity=self.activation)\n",
    "                elif self.linear_weight_init == 'kaiming_normal':\n",
    "                    torch.nn.init.kaiming_normal_(m.weight, nonlinearity=self.activation)\n",
    "                    # torch.nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "                else:\n",
    "                    raise Exception(f'Unrecognised linear_weight_init {self.linear_weight_init}')\n",
    "\n",
    "                # biases\n",
    "                if m.bias is not None:\n",
    "                    if self.linear_bias_init is None:\n",
    "                        pass\n",
    "                    elif self.linear_bias_init == 'zeros':\n",
    "                        torch.nn.init.zeros_(m.bias)\n",
    "                    elif self.linear_bias_init == 'uniform':\n",
    "                        torch.nn.init.uniform_(m.bias)\n",
    "                    elif self.linear_bias_init == 'normal':\n",
    "                        torch.nn.init.normal_(m.bias)\n",
    "                    else:\n",
    "                        raise Exception(f'Unrecognised bias initialisation {self.linear_bias_init}')\n",
    "\n",
    "            elif isinstance(m, torch.nn.LayerNorm):\n",
    "                # weights\n",
    "                if self.layernorm_weight_init is None:\n",
    "                    pass\n",
    "                elif self.layernorm_weight_init == 'normal':\n",
    "                    torch.nn.init.normal_(m.weight, mean=0.0, std=0.01)\n",
    "                else:\n",
    "                    raise Exception(f'Unrecognised layernorm_weight_init {self.layernorm_weight_init}')\n",
    "\n",
    "                # biases\n",
    "                if self.layernorm_bias_init is None:\n",
    "                    pass\n",
    "                elif self.layernorm_bias_init == 'zeros':\n",
    "                    torch.nn.init.zeros_(m.bias)\n",
    "                elif self.layernorm_bias_init == 'normal':\n",
    "                    torch.nn.init.normal_(m.bias)\n",
    "                else:\n",
    "                    raise Exception(f'Unrecognised layernorm_bias_init {self.layernorm_bias_init}')\n",
    "\n",
    "        # init base GNN params\n",
    "        self.apply(init_params)\n",
    "\n",
    "        # init head output params\n",
    "        for h in self.heads_module:\n",
    "            h.apply(init_params)\n",
    "\n",
    "        \n",
    "\n",
    "    def init_nn_modules(self, \n",
    "                        emb_size=64, \n",
    "                        num_rounds=1, \n",
    "                        cons_nfeats=5, \n",
    "                        edge_nfeats=1, \n",
    "                        var_nfeats=19, \n",
    "                        aggregator='add', \n",
    "                        activation=None,\n",
    "                        num_heads=1,\n",
    "                        linear_weight_init=None,\n",
    "                        linear_bias_init=None,\n",
    "                        layernorm_weight_init=None,\n",
    "                        layernorm_bias_init=None,\n",
    "                        head_aggregator='add'):\n",
    "        self.emb_size = emb_size\n",
    "        self.num_rounds = num_rounds\n",
    "        self.cons_nfeats = cons_nfeats\n",
    "        self.edge_nfeats = edge_nfeats\n",
    "        self.var_nfeats = var_nfeats\n",
    "        self.aggregator = aggregator\n",
    "        self.activation = activation\n",
    "        self.num_heads = num_heads\n",
    "        self.linear_weight_init = linear_weight_init\n",
    "        self.linear_bias_init = linear_bias_init\n",
    "        self.layernorm_weight_init = layernorm_weight_init\n",
    "        self.layernorm_bias_init = layernorm_bias_init\n",
    "        self.head_aggregator = head_aggregator\n",
    "\n",
    "        # CONSTRAINT EMBEDDING\n",
    "        self.cons_embedding = torch.nn.Sequential(\n",
    "            torch.nn.LayerNorm(cons_nfeats),\n",
    "            torch.nn.Linear(cons_nfeats, emb_size),\n",
    "            # torch.nn.LayerNorm(emb_size, emb_size), # added\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(emb_size, emb_size),\n",
    "            # torch.nn.LayerNorm(emb_size, emb_size), # added\n",
    "            torch.nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "#         # EDGE EMBEDDING\n",
    "#         self.edge_embedding = torch.nn.Sequential(\n",
    "#             torch.nn.LayerNorm(edge_nfeats),\n",
    "#         )\n",
    "\n",
    "        # VARIABLE EMBEDDING\n",
    "        self.var_embedding = torch.nn.Sequential(\n",
    "            torch.nn.LayerNorm(var_nfeats),\n",
    "            torch.nn.Linear(var_nfeats, emb_size),\n",
    "            # torch.nn.LayerNorm(emb_size, emb_size), # added\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(emb_size, emb_size),\n",
    "            # torch.nn.LayerNorm(emb_size, emb_size), # added\n",
    "            torch.nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv_v_to_c = BipartiteGraphConvolution(emb_size=emb_size, aggregator=aggregator)\n",
    "        self.conv_c_to_v = BipartiteGraphConvolution(emb_size=emb_size, aggregator=aggregator)\n",
    "\n",
    "        # heads\n",
    "        self.heads_module = torch.nn.ModuleList([\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Linear(emb_size, emb_size),\n",
    "                torch.nn.LeakyReLU(),\n",
    "                torch.nn.Linear(emb_size, 1, bias=True)\n",
    "                )\n",
    "            for _ in range(self.num_heads)\n",
    "            ])\n",
    "\n",
    "        if self.activation is None:\n",
    "            self.activation_module = None\n",
    "        elif self.activation == 'sigmoid':\n",
    "            self.activation_module = torch.nn.Sigmoid()\n",
    "        elif self.activation == 'relu':\n",
    "            self.activation_module = torch.nn.ReLU()\n",
    "        elif self.activation == 'leaky_relu' or self.activation == 'inverse_leaky_relu':\n",
    "            self.activation_module = torch.nn.LeakyReLU()\n",
    "        elif self.activation == 'elu':\n",
    "            self.activation_module = torch.nn.ELU()\n",
    "        elif self.activation == 'hard_swish':\n",
    "            self.activation_module = torch.nn.Hardswish()\n",
    "        elif self.activation == 'softplus':\n",
    "            self.activation_module = torch.nn.Softplus()\n",
    "        elif self.activation == 'mish':\n",
    "            self.activation_module = torch.nn.Mish()\n",
    "        elif self.activation == 'softsign':\n",
    "            self.activation_module = torch.nn.Softsign()\n",
    "        else:\n",
    "            raise Exception(f'Unrecognised activation {self.activation}')\n",
    "    \n",
    "        self.init_model_parameters()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, *_obs):\n",
    "        '''Returns output of each head.'''\n",
    "        if len(_obs) > 1:\n",
    "            # no need to pre-process observation features\n",
    "#             constraint_features, edge_indices, edge_features, variable_features = _obs\n",
    "            constraint_features, edge_indices, variable_features = _obs\n",
    "        else:\n",
    "            # need to pre-process observation features\n",
    "            obs = _obs[0] # unpack\n",
    "            constraint_features = torch.from_numpy(obs.row_features.astype(np.float32)).to(self.device)\n",
    "            edge_indices = torch.from_numpy(obs.edge_features.indices.astype(np.int64)).to(self.device)\n",
    "#             edge_features = torch.from_numpy(obs.edge_features.values.astype(np.float32)).view(-1, 1).to(self.device)\n",
    "#             print(f'edge features in net forward: {edge_features.shape} {edge_features}')\n",
    "            variable_features = torch.from_numpy(obs.column_features.astype(np.float32)).to(self.device)\n",
    "\n",
    "        reversed_edge_indices = torch.stack([edge_indices[1], edge_indices[0]], dim=0)\n",
    "        \n",
    "        # First step: linear embedding layers to a common dimension (64)\n",
    "        constraint_features = self.cons_embedding(constraint_features)\n",
    "#         edge_features = self.edge_embedding(edge_features)\n",
    "        if variable_features.shape[1] != self.var_nfeats:\n",
    "            if not self.printed_warning:\n",
    "                ans = None\n",
    "                while ans not in {'y', 'n'}:\n",
    "                    ans = input(f'WARNING: variable_features is shape {variable_features.shape} but var_nfeats is {self.var_nfeats}. Will index out extra features. Continue? (y/n): ')\n",
    "                if ans == 'y':\n",
    "                    pass\n",
    "                else:\n",
    "                    raise Exception('User stopped programme.')\n",
    "                self.printed_warning = True\n",
    "            variable_features = variable_features[:, 0:self.var_nfeats]\n",
    "        variable_features = self.var_embedding(variable_features)\n",
    "\n",
    "        # Two half convolutions (message passing round)\n",
    "        for _ in range(self.num_rounds):\n",
    "#             constraint_features = self.conv_v_to_c(variable_features, reversed_edge_indices, edge_features, constraint_features)\n",
    "#             variable_features = self.conv_c_to_v(constraint_features, edge_indices, edge_features, variable_features)\n",
    "            constraint_features = self.conv_v_to_c(variable_features, reversed_edge_indices, constraint_features)\n",
    "            variable_features = self.conv_c_to_v(constraint_features, edge_indices, variable_features)\n",
    "\n",
    "        # get output for each head\n",
    "        head_output = [self.heads_module[head](variable_features).squeeze(-1) for head in range(self.num_heads)]\n",
    "        # print(f'head outputs: {head_output}')\n",
    "\n",
    "        # check if should aggregate head outputs\n",
    "        if self.head_aggregator is None:\n",
    "            # do not aggregate heads\n",
    "            pass\n",
    "        else:\n",
    "            # aggregate head outputs\n",
    "            # head_output = np.array([head_output[head].detach().cpu().numpy() for head in range(len(head_output))])\n",
    "            if self.head_aggregator == 'add':\n",
    "                head_output = [torch.stack(head_output, dim=0).sum(dim=0)]\n",
    "            elif self.head_aggregator == 'mean':\n",
    "                head_output = [torch.stack(head_output, dim=0).mean(dim=0)]\n",
    "            else:\n",
    "                raise Exception(f'Unrecognised head_aggregator {self.head_aggregator}')\n",
    "            # print(f'head outputs after aggregation: {head_output}')\n",
    "\n",
    "        # activation\n",
    "        if self.activation_module is not None:\n",
    "            head_output = [self.activation_module(head) for head in head_output]\n",
    "            if self.activation == 'inverse_leaky_relu':\n",
    "                # invert\n",
    "                head_output = [-1 * head for head in head_output]\n",
    "        # print(f'head outputs after activation: {head_output}')\n",
    "\n",
    "\n",
    "\n",
    "        # # activation\n",
    "        # if self.activation_module is not None:\n",
    "            # head_output = self.activation_module(head_output)\n",
    "\n",
    "        return head_output\n",
    "\n",
    "    def create_config(self):\n",
    "        '''Returns config dict so that can re-initialise easily.'''\n",
    "        # create network dict of self.<attribute> key-value pairs\n",
    "        network_dict = copy.deepcopy(self.__dict__)\n",
    "\n",
    "        # remove module references to avoid circular references\n",
    "        del network_dict['_modules']\n",
    "\n",
    "        # create config dict\n",
    "        config = ml_collections.ConfigDict(network_dict)\n",
    "\n",
    "        return config\n",
    "    \n",
    "    \n",
    "class BipartiteGraphConvolution(torch_geometric.nn.MessagePassing):\n",
    "    \"\"\"\n",
    "    The bipartite graph convolution is already provided by pytorch geometric and we merely need \n",
    "    to provide the exact form of the messages being passed.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 aggregator='add',\n",
    "                 emb_size=64):\n",
    "        super().__init__(aggregator)\n",
    "        \n",
    "        self.feature_module_left = torch.nn.Sequential(\n",
    "            torch.nn.Linear(emb_size, emb_size)\n",
    "        )\n",
    "#         self.feature_module_edge = torch.nn.Sequential(\n",
    "#             torch.nn.Linear(1, emb_size, bias=False)\n",
    "#         )\n",
    "        self.feature_module_right = torch.nn.Sequential(\n",
    "            torch.nn.Linear(emb_size, emb_size, bias=False)\n",
    "        )\n",
    "        self.feature_module_final = torch.nn.Sequential(\n",
    "            torch.nn.LayerNorm(emb_size),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(emb_size, emb_size)\n",
    "        )\n",
    "        \n",
    "        self.post_conv_module = torch.nn.Sequential(\n",
    "            torch.nn.LayerNorm(emb_size)\n",
    "        )\n",
    "\n",
    "        # output_layers\n",
    "        self.output_module = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2*emb_size, emb_size),\n",
    "            # torch.nn.LayerNorm(emb_size, emb_size), # added\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(emb_size, emb_size),\n",
    "            # torch.nn.LayerNorm(emb_size, emb_size), # added\n",
    "        )\n",
    "\n",
    "#     def forward(self, left_features, edge_indices, edge_features, right_features):\n",
    "    def forward(self, left_features, edge_indices, right_features):\n",
    "        \"\"\"\n",
    "        This method sends the messages, computed in the message method.\n",
    "        \"\"\"\n",
    "#         print(f'edge features in GNN forward: {edge_features.shape} {edge_features}')\n",
    "#         output = self.propagate(edge_indices, size=(left_features.shape[0], right_features.shape[0]), \n",
    "#                                 node_features=(left_features, right_features), edge_features=edge_features)\n",
    "        output = self.propagate(edge_indices, size=(left_features.shape[0], right_features.shape[0]), \n",
    "                                node_features=(self.feature_module_left(left_features), self.feature_module_right(right_features)))\n",
    "        return self.output_module(torch.cat([self.post_conv_module(output), right_features], dim=-1))\n",
    "\n",
    "#     def message(self, node_features_i, node_features_j, edge_features):\n",
    "    def message(self, node_features_i, node_features_j):\n",
    "        output = self.feature_module_final(node_features_i + node_features_j)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Batch object for input state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BipartiteNodeData(torch_geometric.data.Data):\n",
    "    \"\"\"\n",
    "    This class encode a node bipartite graph observation as returned by the `ecole.observation.NodeBipartite` \n",
    "    observation function in a format understood by the pytorch geometric data handlers.\n",
    "    \"\"\"\n",
    "    def __init__(self, obs, candidates):\n",
    "        super().__init__()\n",
    "        self.obs = obs\n",
    "        self.constraint_features = torch.FloatTensor(obs.row_features)\n",
    "        self.edge_index = torch.LongTensor(obs.edge_features.indices.astype(np.int64))\n",
    "#         self.edge_attr = torch.FloatTensor(obs.edge_features.values).unsqueeze(1)\n",
    "#         print(f'edge features in BipartiteNodeData: {self.edge_attr.shape} {self.edge_attr}')\n",
    "        self.variable_features = torch.FloatTensor(obs.column_features)\n",
    "        self.candidates = torch.from_numpy(candidates.astype(np.int64)).long()\n",
    "        self.raw_candidates = torch.from_numpy(candidates.astype(np.int64)).long()\n",
    "        \n",
    "        self.num_candidates = len(candidates)\n",
    "        self.num_variables = self.variable_features.size(0)\n",
    "        self.num_nodes = self.constraint_features.size(0) + self.variable_features.size(0)\n",
    "\n",
    "    def __inc__(self, key, value):\n",
    "        \"\"\"\n",
    "        We overload the pytorch geometric method that tells how to increment indices when concatenating graphs \n",
    "        for those entries (edge index, candidates) for which this is not obvious. This\n",
    "        enables batching.\n",
    "        \"\"\"\n",
    "        if key == 'edge_index':\n",
    "            # constraint nodes connected via edge to variable nodes\n",
    "            return torch.tensor([[self.constraint_features.size(0)], [self.variable_features.size(0)]])\n",
    "        elif key == 'candidates':\n",
    "            # actions are variable nodes\n",
    "            return self.variable_features.size(0)\n",
    "        else:\n",
    "            return super().__inc__(key, value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = EcoleBranching(observation_function='43_var_features',\n",
    "                      information_function='default',\n",
    "                      reward_function='default',\n",
    "                      scip_params='default')\n",
    "\n",
    "instances = ecole.instance.SetCoverGenerator(n_rows=500, n_cols=1000, density=0.05)\n",
    "\n",
    "def gen_obs(env, instances):\n",
    "    obs = None\n",
    "    while obs is None:\n",
    "        instance = next(instances)\n",
    "        obs, action_set, _, _, _ = env.reset(instance)\n",
    "    return (obs, action_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pass through network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:1'\n",
    "net = BipartiteGCN(device=device,\n",
    "                  emb_size=128,\n",
    "                  num_rounds=2,\n",
    "                  cons_nfeats=5,\n",
    "                  edge_nfeats=1,\n",
    "                  var_nfeats=43, # 19 20 28 45 40\n",
    "                  aggregator='add',\n",
    "                  activation='inverse_leaky_relu',\n",
    "                  num_heads=1,\n",
    "                  linear_weight_init='normal',\n",
    "                  linear_bias_init='zeros',\n",
    "                  layernorm_weight_init=None,\n",
    "                  layernorm_bias_init=None,\n",
    "                  head_aggregator=None) # None 'add'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = gen_obs(env, instances)\n",
    "state = BipartiteNodeData(*obs) \n",
    "\n",
    "state = state.to(device)\n",
    "# state = (state.constraint_features, state.edge_index, state.edge_attr, state.variable_features)\n",
    "state = (state.constraint_features, state.edge_index, state.variable_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n10 net(*state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "obs = gen_obs(env, instances)\n",
    "state = BipartiteNodeData(*obs)\n",
    "\n",
    "state = state.to(device)\n",
    "# state = (state.constraint_features, state.edge_index, state.edge_attr, state.variable_features)\n",
    "state = (state.constraint_features, state.edge_index, state.variable_features)\n",
    "\n",
    "\n",
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "    output = net(*state)\n",
    "print(prof.table(sort_by='cuda_time_total'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create batch of states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "observations = [gen_obs(env, instances) for _ in range(batch_size)]\n",
    "states = [BipartiteNodeData(*obs) for obs in observations]\n",
    "batched_state = torch_geometric.data.Batch.from_data_list(states)\n",
    "print(batched_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = batched_state.to(device)\n",
    "# state = (state.constraint_features, state.edge_index, state.edge_attr, state.variable_features)\n",
    "state = (state.constraint_features, state.edge_index, state.variable_features)\n",
    "for el in state:\n",
    "    print(el.shape)\n",
    "\n",
    "output_start = time.time()\n",
    "output = net(*state)\n",
    "print(output[0][0])\n",
    "print(output[0].shape)\n",
    "output_t = time.time() - output_start\n",
    "print(f'output_t: {output_t*1e3:.3f} ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backward_start = time.time()\n",
    "output[0].sum().backward()\n",
    "print(output[0][0])\n",
    "backward_t = time.time() - backward_start\n",
    "print(f'backward_t: {backward_t*1e3:.3f} ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlgnn",
   "language": "python",
   "name": "rlgnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
