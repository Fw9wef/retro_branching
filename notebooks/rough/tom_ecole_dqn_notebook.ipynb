{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7823dd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "from pyscipopt import Model, quicksum, SCIP_PARAMSETTING\n",
    "import ecole\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08b7055",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fcacdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCIP_PARAMETERS = {'separating/maxrounds': 0,\n",
    "                   'separating/maxroundsroot': 0,\n",
    "                   'separating/maxcuts': 0,\n",
    "                   'separating/maxcutsroot': 0,\n",
    "                   'presolving/maxrounds': 0,\n",
    "                   'presolving/maxrestarts': 0,\n",
    "                   'propagating/maxrounds':0,\n",
    "                   'propagating/maxroundsroot':0,\n",
    "                   'lp/initalgorithm':'d',\n",
    "                   'lp/resolvealgorithm':'d',\n",
    "                   'limits/time': 3600}\n",
    "\n",
    "\n",
    "env = ecole.environment.Branching(\n",
    "    observation_function=(\n",
    "        ecole.observation.NodeBipartite()\n",
    "    ),\n",
    "    information_function=(\n",
    "#         ecole.observation.StrongBranchingScores(),\n",
    "#         ecole.observation.Pseudocosts(),\n",
    "        ecole.reward.LpIterations().cumsum(),\n",
    "        ecole.reward.NNodes().cumsum(),\n",
    "        ecole.reward.SolvingTime(),\n",
    "        ecole.reward.SolvingTime().cumsum()\n",
    "       ),\n",
    "    \n",
    "    reward_function=(\n",
    "        ecole.reward.LpIterations(),\n",
    "    ),\n",
    "    scip_params=SCIP_PARAMETERS\n",
    ")\n",
    "instances = ecole.instance.SetCoverGenerator(n_rows=500, n_cols=1000, density=0.05)\n",
    "\n",
    "observation, action_set, reward_offset, done, info = env.reset(next(instances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94795a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"observation.column_features.: {observation.column_features.shape}\")\n",
    "print(f\"observation.row_features.: {observation.row_features.shape}\")\n",
    "print(f\"observation.edge_features: {observation.edge_features.values.shape}\")\n",
    "print(f\"\\tobservation.edge_features.values: {observation.edge_features.values.shape}\")\n",
    "print(f\"\\tobservation.edge_features.indices: {observation.edge_features.indices.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e399e9",
   "metadata": {},
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c617d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as gnn\n",
    "from torch_geometric.data import Batch, Data\n",
    "from torch_geometric.utils import softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14aaa505",
   "metadata": {},
   "source": [
    "First, we need to format the observation of bipartite data from ``ecole`` into a format that ``pytorch_geometric`` can handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9119374e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BipartiteData(Data):\n",
    "    \"\"\"\n",
    "    This class encode a node bipartite graph observation as returned by the `ecole.observation.NodeBipartite` \n",
    "    observation function in a format understood by the pytorch geometric data handlers.\n",
    "    \"\"\"\n",
    "    def __init__(self, bipartite_observation, candidates):\n",
    "        super().__init__()\n",
    "        self.constraint_features = torch.from_numpy(bipartite_observation.row_features).float()\n",
    "        self.edge_index = torch.from_numpy(bipartite_observation.edge_features.indices.astype(np.int64)).long()\n",
    "        self.edge_features = torch.from_numpy(bipartite_observation.edge_features.values).float()\n",
    "        self.variable_features = torch.from_numpy(bipartite_observation.column_features).float()\n",
    "        \n",
    "        if self.edge_features.dim()==1:\n",
    "            self.edge_features.unsqueeze_(-1)\n",
    "        self.edge_index_c2v = self.edge_index\n",
    "        self.edge_index_v2c = self.edge_index[[1,0]]\n",
    "        \n",
    "        self.candidates = torch.from_numpy(candidates.astype(np.int64)).long()\n",
    "        self.raw_candidates = torch.from_numpy(candidates.astype(np.int64)).long()\n",
    "        self.num_candidates = self.candidates.size(0)\n",
    "        self.num_variables = self.variable_features.size(0)\n",
    "        \n",
    "        self.num_nodes = self.constraint_features.size(0) + self.variable_features.size(0)\n",
    "\n",
    "    def __inc__(self, key, value):\n",
    "        \"\"\"\n",
    "        We overload the pytorch geometric method that tells how to increment indices when concatenating graphs \n",
    "        for those entries (edge index, candidates) for which this is not obvious.\n",
    "        \"\"\"\n",
    "        if key == 'edge_index' or key == 'edge_index_c2v':\n",
    "            return torch.tensor([[self.constraint_features.size(0)], [self.variable_features.size(0)]])\n",
    "        elif key == 'edge_index_v2c':\n",
    "            return torch.tensor([[self.variable_features.size(0)], [self.constraint_features.size(0)]])\n",
    "        elif key == 'candidates':\n",
    "            return self.variable_features.size(0)\n",
    "        else:\n",
    "            return super().__inc__(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808aba02",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = BipartiteData(observation, action_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc015d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70ed68d",
   "metadata": {},
   "source": [
    "The message passing network itself is standard, an encoder, message passing rounds, and a decoder.\n",
    "\n",
    "This is the simplest possible implementation.  For now, we are not going to do anything with the edge features.  Nor are we considering how to mask out unavailable actions, or preserve state along search tree's etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df82d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 constraint_dim=32,\n",
    "                 edge_dim=32,\n",
    "                 variable_dim=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        constraint_dim_in = 5\n",
    "        edge_dim_in = 1\n",
    "        variable_dim_in = 19\n",
    "        \n",
    "        self.constraint_embedding = nn.Sequential(\n",
    "#             nn.LayerNorm(constraint_dim_in),\n",
    "            nn.Linear(constraint_dim_in, constraint_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(constraint_dim, constraint_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.edge_embedding = nn.Sequential(\n",
    "            torch.nn.LayerNorm(edge_dim_in),\n",
    "            nn.Linear(edge_dim_in, edge_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.variable_embedding = nn.Sequential(\n",
    "#             nn.LayerNorm(variable_dim_in),\n",
    "            nn.Linear(variable_dim_in, variable_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(variable_dim, variable_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, constraint_features, edge_features, variable_features):\n",
    "        return (\n",
    "            self.constraint_embedding(constraint_features),\n",
    "            self.edge_embedding(edge_features),\n",
    "            self.variable_embedding(variable_features)\n",
    "               )\n",
    "    \n",
    "class MessagePassingLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 constraint_dim=32,\n",
    "                 edge_dim=32,\n",
    "                 variable_dim=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv_v2c = gnn.GATConv(variable_dim, constraint_dim)\n",
    "        self.conv_c2v = gnn.GATConv(constraint_dim, variable_dim)\n",
    "        \n",
    "    def forward(self,\n",
    "                constraint_features,\n",
    "                edge_features,\n",
    "                variable_features,\n",
    "                edge_index_v2c,\n",
    "                edge_index_c2v):\n",
    "        constraint_features = self.conv_v2c((variable_features, constraint_features),\n",
    "                                            edge_index_v2c,\n",
    "                                            size=(variable_features.size(0), constraint_features.size(0)))\n",
    "        variable_features = self.conv_c2v((constraint_features, variable_features),\n",
    "                                           edge_index_c2v,\n",
    "                                           size=(constraint_features.size(0), variable_features.size(0)))\n",
    "        \n",
    "        return constraint_features, edge_features, variable_features\n",
    "    \n",
    "class Readout(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 variable_dim=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.readout = nn.Sequential(\n",
    "            nn.Linear(variable_dim, variable_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(variable_dim, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, variable_features):\n",
    "        return self.readout(variable_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dda965e",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder()\n",
    "mpnn = MessagePassingLayer()\n",
    "readout = Readout()\n",
    "\n",
    "constraint_features, edge_features, variable_features = enc(data.constraint_features,\n",
    "                                                            data.edge_features,\n",
    "                                                            data.variable_features)\n",
    "\n",
    "print(constraint_features.shape, edge_features.shape, variable_features.shape)\n",
    "\n",
    "constraint_features, edge_features, variable_features = mpnn(constraint_features,\n",
    "                                                             edge_features,\n",
    "                                                             variable_features,\n",
    "                                                             data.edge_index_v2c,\n",
    "                                                             data.edge_index_c2v)\n",
    "\n",
    "print(constraint_features.shape, edge_features.shape, variable_features.shape)\n",
    "\n",
    "out = readout(variable_features)\n",
    "\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2592e5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 constraint_dim=32,\n",
    "                 edge_dim=32,\n",
    "                 variable_dim=32,\n",
    "                 num_rounds=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder(constraint_dim, edge_dim, variable_dim)\n",
    "        self.mpnn = MessagePassingLayer(constraint_dim, edge_dim, variable_dim)\n",
    "        self.readout = Readout(variable_dim)\n",
    "        self.num_rounds = 1\n",
    "        \n",
    "    def forward(self,\n",
    "                constraint_features,\n",
    "                edge_features,\n",
    "                variable_features,\n",
    "                edge_index_v2c,\n",
    "                edge_index_c2v):\n",
    "        \n",
    "        print(f'constr: {constraint_features.shape} | edge_feats: {edge_features.shape} | variable_feats: {variable_features.shape}')\n",
    "        \n",
    "        constraint_features, edge_features, variable_features = self.encoder(constraint_features,\n",
    "                                                                             edge_features,\n",
    "                                                                             variable_features)\n",
    "        for _ in range(self.num_rounds):\n",
    "            constraint_features, edge_features, variable_features = self.mpnn(constraint_features,\n",
    "                                                                              edge_features,\n",
    "                                                                              variable_features,\n",
    "                                                                              edge_index_v2c,\n",
    "                                                                              edge_index_c2v)\n",
    "            \n",
    "        return self.readout(variable_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3ac1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "qnet = QNetwork(32,1,32,num_rounds=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b6a5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = Batch.from_data_list([data])\n",
    "preds = qnet(batch.constraint_features,\n",
    "             batch.edge_features,\n",
    "             batch.variable_features,\n",
    "             batch.edge_index_v2c,\n",
    "             batch.edge_index_c2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e442dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.num_candidates.cumsum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6600f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack([q.argmax() for q in preds[batch.candidates].split_with_sizes(tuple(batch.num_candidates))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481ca853",
   "metadata": {},
   "source": [
    "### Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9244c30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "\n",
    "Transition = namedtuple('Transition', field_names=['state', # BipartiteData\n",
    "                                                   'action', # Int\n",
    "                                                   'reward', # Float\n",
    "                                                   'done', # Bool\n",
    "                                                   'new_state']) # BipartiteData\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Replay Buffer for storing past experiences allowing the agent to learn from them\n",
    "    Args:\n",
    "        capacity: size of the buffer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int) -> None:\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self) -> None:\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, transition) -> None:\n",
    "        \"\"\"\n",
    "        Add transition to the buffer\n",
    "        Args:\n",
    "            transition: tuple (state, action, reward, done, new_state)\n",
    "        \"\"\"\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
    "\n",
    "        return (Batch.from_data_list(states),\n",
    "                torch.tensor(actions),\n",
    "                torch.tensor(rewards),\n",
    "                torch.tensor(dones).float(),\n",
    "                Batch.from_data_list(next_states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e511ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "from collections.abc import Iterable\n",
    "\n",
    "class Actor:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 q_network):\n",
    "        self.policy_network = q_network\n",
    "        self.target_network = deepcopy(q_network)\n",
    "        self.update_target_network()\n",
    "        \n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.policy_network.state_dict())\n",
    "        for param in self.target_network.parameters():\n",
    "            param.requires_grad = False\n",
    "        return self.target_network\n",
    "    \n",
    "    def calc_Q_values(self, observation, use_target_network=False):\n",
    "        if not use_target_network:\n",
    "            preds = self.policy_network(observation.constraint_features,\n",
    "                                        observation.edge_features,\n",
    "                                        observation.variable_features,\n",
    "                                        observation.edge_index_v2c,\n",
    "                                        observation.edge_index_c2v)\n",
    "        else:\n",
    "            preds = self.target_network(observation.constraint_features,\n",
    "                                        observation.edge_features,\n",
    "                                        observation.variable_features,\n",
    "                                        observation.edge_index_v2c,\n",
    "                                        observation.edge_index_c2v)\n",
    "        return preds\n",
    "        \n",
    "    def action_select(self, observation_batch, epsilon=0):\n",
    "        preds = self.calc_Q_values(observation_batch)\n",
    "        valid_preds = preds[observation_batch.candidates]\n",
    "        if isinstance(observation_batch.num_candidates, Iterable):\n",
    "            valid_preds = valid_preds.split_with_sizes(tuple(observation_batch.num_candidates))\n",
    "            action_idxs = observation_batch.raw_candidates.split_with_sizes(tuple(observation_batch.num_candidates))\n",
    "        else:\n",
    "            valid_preds = [valid_preds]\n",
    "            action_idxs = [observation_batch.raw_candidates]\n",
    "        print(f'action select action idxs: {action_idxs}')\n",
    "        actions = torch.stack([idxs[q.argmax()] for q, idxs in zip(valid_preds,action_idxs)])\n",
    "                                                                           \n",
    "        if epsilon > 0:\n",
    "            act_randomly = (np.random.rand(len(actions)) < epsilon)\n",
    "            rand_actions = [np.random.choice(acts) for acts, rand in zip(action_idxs, act_randomly) if rand]\n",
    "            actions[act_randomly] = torch.LongTensor(rand_actions)\n",
    "            \n",
    "        return actions\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.policy_network.parameters()\n",
    "            \n",
    "                    \n",
    "class DQNLearner:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 \n",
    "                 actor,\n",
    "                 \n",
    "                 env,\n",
    "                 \n",
    "                 instances,\n",
    "                 \n",
    "                 buffer_capacity=1000,\n",
    "                 buffer_min_length=100,\n",
    "                 batch_size=32,\n",
    "                \n",
    "                 steps_per_update = 100,\n",
    "                 lr = 3e-4,\n",
    "                 gamma = 0.99,\n",
    "                 update_target_frequency = 100,\n",
    "                \n",
    "                # Exploration\n",
    "                initial_epsilon=1,\n",
    "                final_epsilon=0.05,\n",
    "                final_epsilon_epoch=1000,\n",
    "                 \n",
    "                log_frequency=10\n",
    "                ):\n",
    "                \n",
    "        self.actor = actor\n",
    "        self.env = env\n",
    "        self.instances = instances\n",
    "        \n",
    "        self.buffer = ReplayBuffer(buffer_capacity)\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.buffer_min_length=buffer_min_length\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.steps_per_update = steps_per_update\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.update_target_frequency = update_target_frequency\n",
    "        \n",
    "        self.optimizer = self.reset_optimizer()\n",
    "                \n",
    "        self.env_ready = False\n",
    "        self.epsilon = 0.1\n",
    "        self.num_steps = 0\n",
    "        self.num_episodes = 0\n",
    "        self.num_epochs = 0\n",
    "        \n",
    "        self.initial_epsilon = initial_epsilon\n",
    "        self.final_epsilon = final_epsilon\n",
    "        self.final_epsilon_epoch = final_epsilon_epoch\n",
    "        \n",
    "        self.log = defaultdict(list)\n",
    "        self.log_frequency = log_frequency\n",
    "        \n",
    "    def reset_optimizer(self):\n",
    "        self.optimizer = torch.optim.Adam(self.actor.parameters(), lr=self.lr)\n",
    "        return self.optimizer\n",
    "        \n",
    "    def reset_env(self, max_attempts=50):\n",
    "        self.env_ready, num_resets = False, 0\n",
    "        while not self.env_ready and num_resets < max_attempts:\n",
    "            observation, action_set, reward, done, info = self.env.reset(next(self.instances))\n",
    "            num_resets += 1\n",
    "            self.env_ready = not done\n",
    "        return BipartiteData(observation, action_set), reward, done, info\n",
    "    \n",
    "    def get_epsilon(self):\n",
    "        return self.initial_epsilon - (self.initial_epsilon-self.final_epsilon)*max(1,self.num_epochs/self.final_epsilon_epoch)\n",
    "    \n",
    "    def step_env(self, state):\n",
    "        action = actor.action_select(state, self.get_epsilon()).item()\n",
    "        observation, action_set, reward, done, info = env.step(action)\n",
    "        if not done:\n",
    "            state = BipartiteData(observation, action_set)\n",
    "        else:\n",
    "            state = None\n",
    "        return state, action, reward, done, info\n",
    "    \n",
    "    def update_log(self, info):\n",
    "        self.log['lp_iter'].append(info[0])\n",
    "        self.log['num_nodes'].append(info[1])\n",
    "        self.log['time'].append(info[3])\n",
    "        \n",
    "    def get_log_str(self):\n",
    "        log_str = f\"Epoch {self.num_epochs:5}\"\n",
    "        log_str += f\" | lp_iter : {self.log['lp_iter'][-1]:5.1f}\"\n",
    "        log_str += f\" | num_nodes : {self.log['num_nodes'][-1]:5.1f}\"\n",
    "        log_str += f\" | sovle time : {self.log['time'][-1]:5.3f}\"\n",
    "        return log_str\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def act(self, num_steps):\n",
    "        for i in range(num_steps):\n",
    "            if not self.env_ready:\n",
    "                self.prev_state = None\n",
    "                self.state, _, _, _ = self.reset_env()\n",
    "                \n",
    "            self.state, action, reward, done, info = self.step_env(self.state)\n",
    "            if self.prev_state is not None:\n",
    "                if done:\n",
    "                    self.state = self.prev_state # hack\n",
    "                self.buffer.append(Transition(self.prev_state, action, reward, done, self.state))\n",
    "                print(f'Added to buffer: action: {action} | reward: {reward} | done: {done}')\n",
    "            self.prev_state = self.state\n",
    "                \n",
    "            if done:\n",
    "                self.update_log(info)\n",
    "                self.env_ready = False\n",
    "                self.num_episodes += 1\n",
    "        self.num_steps += 1\n",
    "                \n",
    "    def update_step(self):\n",
    "        print('\\nStepping optimizer')\n",
    "        def action_to_batch_idxs(action, state):\n",
    "            return torch.cat([action[[0]], action[1:] + state.num_variables[:-1].cumsum(0)])\n",
    "        \n",
    "        # Take DQN update step\n",
    "        state, action, reward, done, next_state = self.buffer.sample(self.batch_size)\n",
    "        print(f'action: {action.shape} {action}')\n",
    "        action_batch_idxs = action_to_batch_idxs(action, state)\n",
    "        print(f'action_batch_idxs: {action_batch_idxs.shape} {action_batch_idxs}')\n",
    "        q_value = actor.calc_Q_values(state)[action_batch_idxs].squeeze()\n",
    "        print(f'q_value: {q_value.shape} {q_value}')\n",
    "            \n",
    "\n",
    "        with torch.no_grad():\n",
    "            action_next = actor.action_select(next_state, epsilon=0)\n",
    "            print(f'action_next: {action_next.shape} {action_next}')\n",
    "            action_next_batch_idxs = action_to_batch_idxs(action_next, next_state)\n",
    "            print(f'action_next_batch_idxs: {action_next_batch_idxs.shape} {action_next_batch_idxs}')\n",
    "            td_target = reward +\\\n",
    "                        (1-done)*self.gamma*actor.calc_Q_values(next_state, use_target_network=True)[action_next_batch_idxs].squeeze()\n",
    "            \n",
    "        self.optimizer.zero_grad()\n",
    "        loss = F.mse_loss(q_value, td_target)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Check to see if we should update target network\n",
    "        self.num_epochs += 1\n",
    "        if self.num_epochs % self.update_target_frequency == 0:\n",
    "            self.actor.update_target_network()\n",
    "            \n",
    "        if self.num_epochs % self.log_frequency == 0:\n",
    "            print(self.get_log_str())\n",
    "        \n",
    "    def train(self, num_epoch):\n",
    "        if len(self.buffer) < self.buffer_min_length:\n",
    "            # Fill replay buffer to minimum level.\n",
    "            print(\"Waiting for replay buffer to fill\", end=\"...\")\n",
    "            self.act(self.buffer_min_length - len(self.buffer))\n",
    "            print(\"done.\")\n",
    "        for _ in range(num_epoch):\n",
    "            self.act(self.steps_per_update)\n",
    "            self.update_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eec6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_RESET_ATTEMPTS = 10\n",
    "\n",
    "env = ecole.environment.Branching(\n",
    "    observation_function=(\n",
    "        ecole.observation.NodeBipartite()\n",
    "    ),\n",
    "    information_function=(\n",
    "#         ecole.observation.StrongBranchingScores(),\n",
    "#         ecole.observation.Pseudocosts(),\n",
    "        ecole.reward.LpIterations().cumsum(),\n",
    "        ecole.reward.NNodes().cumsum(),\n",
    "        ecole.reward.SolvingTime(),\n",
    "        ecole.reward.SolvingTime().cumsum()\n",
    "       ),\n",
    "    \n",
    "    reward_function=(\n",
    "#         -ecole.reward.NNodes(),\n",
    "        -ecole.reward.LpIterations()\n",
    "#         -ecole.reward.IsDone()\n",
    "    ),\n",
    "    scip_params=SCIP_PARAMETERS\n",
    ")\n",
    "instances = ecole.instance.SetCoverGenerator(n_rows=100, n_cols=100, density=0.05)\n",
    "actor = Actor(QNetwork(constraint_dim=64, edge_dim=1, variable_dim=64, num_rounds=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be44d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = DQNLearner(actor,\n",
    "                     env,\n",
    "                     instances,\n",
    "                     buffer_capacity=2,\n",
    "                     buffer_min_length=2,\n",
    "                     batch_size=2,\n",
    "\n",
    "                     steps_per_update = 2,\n",
    "                     lr = 1e-3,\n",
    "                     gamma = 0.99,\n",
    "                     update_target_frequency = 50,\n",
    "                    \n",
    "                     # Exploration\n",
    "                     initial_epsilon=1,\n",
    "                     final_epsilon=0.025,\n",
    "                     final_epsilon_epoch=250,\n",
    "                     \n",
    "                     log_frequency=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c530597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6215ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(learner.log['num_nodes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b4e963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b955d009",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 100\n",
    "data = np.convolve(np.array(learner.log['num_nodes']), np.ones(window)/window, mode='valid')\n",
    "\n",
    "with sns.plotting_context('paper'):\n",
    "    plt.plot(data, linewidth=0.5)\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(\"num_nodes\")\n",
    "#     plt.yscale('log')\n",
    "#     plt.legend()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4e15c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 100\n",
    "data = np.convolve(np.array(learner.log['lp_iter']), np.ones(window)/window, mode='valid')\n",
    "\n",
    "with sns.plotting_context('paper'):\n",
    "    plt.plot(data, linewidth=0.5)\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(\"lp_iter\")\n",
    "#     plt.yscale('log')\n",
    "#     plt.legend()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8874e3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 100\n",
    "data = np.convolve(np.array(learner.log['time']), np.ones(window)/window, mode='valid')\n",
    "\n",
    "with sns.plotting_context('paper'):\n",
    "    plt.plot(data, linewidth=0.5)\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(\"Solve time\")\n",
    "#     plt.yscale('log')\n",
    "#     plt.legend()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87ae0fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlgnn",
   "language": "python",
   "name": "rlgnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
