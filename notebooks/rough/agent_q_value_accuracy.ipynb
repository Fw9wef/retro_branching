{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How close is our agent to predicting the ground-truth values of the reward its Q-values should be predicting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from retro_branching.utils import get_most_recent_checkpoint_foldername, seed_stochastic_modules_globally\n",
    "from retro_branching.networks import BipartiteGCN\n",
    "from retro_branching.agents import Agent, REINFORCEAgent, PseudocostBranchingAgent, StrongBranchingAgent, RandomAgent, DQNAgent, DoubleDQNAgent\n",
    "from retro_branching.environments import EcoleBranching\n",
    "from retro_branching.validators import ReinforcementLearningValidator\n",
    "\n",
    "import ecole\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import time\n",
    "import gzip\n",
    "import pickle\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import sigfig\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "import ray\n",
    "import psutil\n",
    "num_cpus = psutil.cpu_count(logical=False)\n",
    "ray.init(num_cpus=int(num_cpus), ignore_reinit_error=True)\n",
    "\n",
    "seed = 0\n",
    "seed_stochastic_modules_globally(default_seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define agent and env params\n",
    "\n",
    "%autoreload\n",
    "learner = 'supervised_learner' # 'reinforce_learner' 'dqn_learner' 'supervised_learner'\n",
    "base_name = 'gnn' # 'rl_gnn' 'dqn_gnn' 'gnn'\n",
    "AgentClass = Agent # Agent REINFORCEAgent DQNAgent DoubleDQNAgent\n",
    "nrows = 100\n",
    "ncols = 100\n",
    "threshold_difficulty = None\n",
    "last_checkpoint_idx = None\n",
    "max_steps = int(1e12) # int(1e12) 3\n",
    "agent_reward = 'sb_scores' # 'dual_bound_frac' 'normalised_lp_gain'\n",
    "gamma = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "# load agent\n",
    "device = 'cpu'\n",
    "\n",
    "i = 326 # 569 312\n",
    "checkpoint = 37 # 25 88\n",
    "observation_function = '43_var_features' # 'default' '40_var_features' '43_var_features'\n",
    "agent_name = f'{base_name}_{i}'\n",
    "DEVICE = 'cpu'\n",
    "agents, agent_paths = {}, []\n",
    "path = f'/scratch/datasets/retro_branching/{learner}/{base_name}/{agent_name}/checkpoint_{checkpoint}/'\n",
    "config = path + '/config.json'\n",
    "agent = AgentClass(device=device, config=config)\n",
    "agent.name = f'{agent_name}_checkpoint_{checkpoint}'\n",
    "for network_name, network in agent.get_networks().items():\n",
    "    if network is not None:\n",
    "        try:\n",
    "            # see if network saved under same var as 'network_name'\n",
    "            agent.__dict__[network_name].load_state_dict(torch.load(path+f'/{network_name}_params.pkl', map_location=device))\n",
    "        except KeyError:\n",
    "            # network saved under generic 'network' var (as in Agent class)\n",
    "            agent.__dict__['network'].load_state_dict(torch.load(path+f'/{network_name}_params.pkl', map_location=device))\n",
    "        \n",
    "agent.eval() # put in test mode\n",
    "\n",
    "max_steps_agent = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "# env\n",
    "env = EcoleBranching(observation_function=observation_function, # 'default' 40_var_features'\n",
    "                      information_function='default',\n",
    "                      reward_function='default',\n",
    "                      scip_params='default')\n",
    "env.seed(seed)\n",
    "\n",
    "# instance\n",
    "files = glob.glob(f'/scratch/datasets/retro_branching/instances/set_cover_nrows_{nrows}_ncols_{ncols}_density_005_threshold_{threshold_difficulty}/*.mps')\n",
    "instances = [ecole.scip.Model.from_file(f) for f in files]\n",
    "# instance = instances[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "# init tracker\n",
    "instance_stats = defaultdict(lambda: [])\n",
    "step_stats = defaultdict(lambda: defaultdict(lambda: []))\n",
    "\n",
    "# DEBUG\n",
    "sb = StrongBranchingAgent()\n",
    "\n",
    "# solve each instance with branching agent\n",
    "for i, instance in enumerate(instances):\n",
    "    print(f'\\nInstance {i+1} of {len(instances)}')\n",
    "    # reset agent and env\n",
    "    agent.before_reset(instance)\n",
    "    sb.before_reset(instance) # DEBUG\n",
    "    obs, action_set, reward, done, info = env.reset(instance)\n",
    "    \n",
    "    # init trackers\n",
    "    predictions, rewards, returns = [], [], []\n",
    "    t = 1\n",
    "    \n",
    "    # run branching agent\n",
    "    while not done:\n",
    "        # sb score target\n",
    "        rewards.append(sb.extract(env.model, done)[action_set].max())\n",
    "        \n",
    "        # get agent q values\n",
    "        q_vals = agent.calc_Q_values(obs)\n",
    "        if type(q_vals) == list:\n",
    "            q_vals = torch.stack(q_vals).squeeze(0).detach().cpu().numpy()\n",
    "        else:\n",
    "            q_vals = q_vals.squeeze(0).detach().cpu().numpy()\n",
    "        # get q values of valid actions\n",
    "        q_vals = q_vals[action_set]\n",
    "\n",
    "        # get agent action\n",
    "#         action, action_idx = sb.action_select(action_set=action_set, model=env.model, done=done)\n",
    "        action, action_idx = agent.action_select(action_set=action_set, obs=obs, agent_idx=0)\n",
    "\n",
    "        # get q value agent predicted\n",
    "        predictions.append(q_vals[action_idx])\n",
    "\n",
    "        # take step in environment\n",
    "        obs, action_set, reward, done, info = env.step(action)\n",
    "\n",
    "        # get true next-step reward received by agent\n",
    "#         rewards.append(reward[agent_reward])\n",
    "\n",
    "\n",
    "#     # calc true discounted returns\n",
    "#     returns = []\n",
    "#     R = 0\n",
    "#     for r in rewards[::-1]:\n",
    "#         R = r + (gamma * R)\n",
    "#         returns.insert(0, R)\n",
    "    returns = rewards\n",
    "\n",
    "    for t in range(len(predictions)):\n",
    "        print(f'Step {t} | q_val prediction: {predictions[t]} | reward: {rewards[t]} | true discounted_return: {returns[t]}')\n",
    "        step_stats[t]['predictions'].append(predictions[t])\n",
    "        step_stats[t]['true_targets'].append(returns[t])\n",
    "    instance_stats['predictions'].append(predictions)\n",
    "    instance_stats['true_targets'].append(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot mean predictions and true targets at each step\n",
    "fig = plt.figure()\n",
    "colours = iter(sns.color_palette(palette='hls', n_colors=len(instance_stats.keys())))\n",
    "flat_data = {}\n",
    "for label in instance_stats.keys():\n",
    "    colour = next(colours)\n",
    "    data = defaultdict(lambda: [])\n",
    "    for t in step_stats.keys():\n",
    "        data['x_vals'].extend([t+1 for _ in range(len(step_stats[t][label]))])\n",
    "        data['y_vals'].extend(step_stats[t][label])\n",
    "    flat_data[label] = data['y_vals']\n",
    "    sns.lineplot(x='x_vals',\n",
    "                 y='y_vals',\n",
    "                 data=data,\n",
    "                 color=colour,\n",
    "                 ci=68,\n",
    "                 alpha=0.8,\n",
    "                 label=label)\n",
    "    plt.axhline(y=np.mean(data['y_vals']), color=colour, linestyle='--', alpha=0.5)\n",
    "mse = mean_squared_error(flat_data['true_targets'], flat_data['predictions'])\n",
    "plt.title(f'{agent.name} MSE: {sigfig.round(mse, sigfigs=3)}')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel(f'gamma_{gamma}_{agent_reward}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEMPORARY: Plotting strong branching gamma=0 case...\n",
    "# fig = plt.figure()\n",
    "# colours = iter(sns.color_palette(palette='hls', n_colors=len(instance_stats.keys())))\n",
    "# flat_data = {}\n",
    "# colour = next(colours)\n",
    "# data = defaultdict(lambda: [])\n",
    "# for t in step_stats.keys():\n",
    "#     data['x_vals'].extend([t+1 for _ in range(len(step_stats[t]['true_targets']))])\n",
    "#     data['y_vals'].extend(step_stats[t]['true_targets'])\n",
    "# flat_data[label] = data['y_vals']\n",
    "# sns.lineplot(x='x_vals',\n",
    "#              y='y_vals',\n",
    "#              data=data,\n",
    "#              color=colour,\n",
    "#              ci=68,\n",
    "#              alpha=0.8,\n",
    "#              label=f'{agent.name}_returns')\n",
    "# plt.axhline(y=np.mean(data['y_vals']), color=colour, linestyle='--', alpha=0.5)\n",
    "# plt.xlabel('Step')\n",
    "# plt.ylabel(f'gamma_{gamma}_{agent_reward}')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlgnn",
   "language": "python",
   "name": "rlgnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
