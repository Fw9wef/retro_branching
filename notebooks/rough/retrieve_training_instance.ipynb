{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this notebook to retrieve a training instance for which you know the seed. Useful for debugging if e.g. training stopped when encountered a certain instance, and you need to retrieve this instance to try to understand what caused the crash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from retro_branching.environments import EcoleBranching, EcoleConfiguring\n",
    "from retro_branching.agents import StrongBranchingAgent, PseudocostBranchingAgent, RandomAgent\n",
    "from retro_branching.utils import seed_stochastic_modules_globally\n",
    "\n",
    "import ecole\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from networkx.drawing.nx_pydot import graphviz_layout\n",
    "# from networkx.drawing.nx_agraph import graphviz_layout\n",
    "\n",
    "seed = 0 # 1\n",
    "seed_stochastic_modules_globally(default_seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "agent = RandomAgent()\n",
    "\n",
    "env = EcoleBranching(observation_function='43_var_features',\n",
    "                      information_function='default',\n",
    "                      reward_function='default',\n",
    "                      scip_params='default')\n",
    "env.seed(seed)\n",
    "\n",
    "instances = ecole.instance.SetCoverGenerator(n_rows=500, n_cols=1000, density=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, initialise the buffer. Assuming init_epsilon=1, the actions taken to initialise the buffer will have been taken with the random exploration agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_min_length = 20000\n",
    "buffer_curr_length = 0\n",
    "buffer_instance_counter = 0\n",
    "\n",
    "env_ready = False\n",
    "while buffer_curr_length < buffer_min_length:\n",
    "    if not env_ready:\n",
    "        # reset env\n",
    "        obs = None\n",
    "        while obs is None:\n",
    "            env.seed(seed)\n",
    "            instance = next(instances)\n",
    "            buffer_instance_counter += 1\n",
    "            agent.before_reset(instance)\n",
    "            obs, action_set, reward, done, info = env.reset(instance)\n",
    "        env_ready = True\n",
    "    \n",
    "    # act\n",
    "    action, action_idx = agent.action_select(action_set)\n",
    "    obs, action_set, reward, done, info = env.step(action)\n",
    "    \n",
    "    # experience added to buffer\n",
    "    buffer_curr_length += 1\n",
    "    print(f'Buffer: {buffer_curr_length}/{buffer_min_length} | Instance counter: {buffer_instance_counter}')\n",
    "    \n",
    "    if done:\n",
    "        env_ready = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have filled the buffer, we can simply loop through the instances until we reach the instance of the target episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "episode_counter = 0\n",
    "target_episode = 2642\n",
    "reset_instance_counter = 0\n",
    "\n",
    "env_ready = False\n",
    "while episode_counter <= target_episode:\n",
    "    if not env_ready:\n",
    "        # reset env\n",
    "        obs = None\n",
    "        while obs is None:\n",
    "            env.seed(seed)\n",
    "            instance = next(instances)\n",
    "            if episode_counter == target_episode:\n",
    "                print(f'Reached episode {episode_counter}, saving instance before reset')\n",
    "                instance.write_problem(f'instance_{episode_counter}.mps')\n",
    "            agent.before_reset(instance)\n",
    "            obs, action_set, reward, done, info = env.reset(instance)\n",
    "            reset_instance_counter += 1\n",
    "        # env reset, this is an episode\n",
    "        print(f'Episode: {episode_counter}/{target_episode} | Instance counter: {reset_instance_counter}')\n",
    "        episode_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total number of instances needed to be looped through to reach episode {target_episode}: {buffer_instance_counter+reset_instance_counter}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(obs)\n",
    "print(done)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlgnn",
   "language": "python",
   "name": "rlgnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
