{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we overfit to 100x100 instances and show that we can beat strong branching, or at least a network which has tried to generalise to 100x100 instances?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from retro_branching.agents import StrongBranchingAgent, REINFORCEAgent\n",
    "from retro_branching.environments import EcoleBranching\n",
    "from retro_branching.networks import BipartiteGCN \n",
    "from retro_branching.utils import plot_val_line, get_most_recent_checkpoint_foldername, sns_plot_val_line\n",
    "\n",
    "import torch\n",
    "import ecole\n",
    "import pyscipopt\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import gzip\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 100 # 100 500\n",
    "ncols = 100 # 500 1000\n",
    "instances_path = f'/home/zciccwf/phd_project/projects/retro_branching/scripts/instances_to_overfit/nrows_{nrows}_ncols_{ncols}/'\n",
    "instance_paths = list(sorted(glob.glob(f'{instances_path}/*.mps')))\n",
    "print(f'instance paths: {instance_paths}')\n",
    "instance_names = [instance_path.split('/')[-1].split('.')[0] for instance_path in instance_paths]\n",
    "print(f'\\ninstance names: {instance_names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['num_nodes', 'lp_iterations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve each instance with strong branching\n",
    "instance_name_to_sb_metrics = {metric: {instance_name: None for instance_name in instance_names} for metric in metrics}\n",
    "for instance_path, instance_name in zip(instance_paths, instance_names):\n",
    "    print(f'\\n> Instance {instance_name} <')\n",
    "    \n",
    "    # init env\n",
    "    env = EcoleBranching(observation_function='default',\n",
    "                         information_function='default',\n",
    "                         reward_function='default',\n",
    "                         scip_params='default')\n",
    "    env.seed(0)\n",
    "    \n",
    "    # init agent\n",
    "    agent = StrongBranchingAgent()\n",
    "    \n",
    "    # init instance\n",
    "    instance = pyscipopt.Model()\n",
    "    instance.readProblem(instance_path)\n",
    "    instance = ecole.scip.Model.from_pyscipopt(instance)\n",
    "    \n",
    "    # reset env with instance\n",
    "    agent.before_reset(instance)\n",
    "    obs, action_set, reward, done, info = env.reset(instance)\n",
    "    \n",
    "#     raise Exception() # comment if want to gen new data\n",
    "    \n",
    "    # solve\n",
    "    t = 0\n",
    "    print(f'Step {t} | Num nodes: {info[\"num_nodes\"]}')\n",
    "    while not done:\n",
    "        action, action_idx = agent.action_select(action_set, env.model, done)\n",
    "        obs, action_set, reward, done, info = env.step(action)\n",
    "        t += 1\n",
    "        print(f'Step {t} | Num nodes: {info[\"num_nodes\"]} | LP iterations: {info[\"lp_iterations\"]}')\n",
    "    for metric in metrics:\n",
    "        instance_name_to_sb_metrics[metric][instance_name] = info[metric]\n",
    "    \n",
    "print(f'\\nStrong branching metrics for each instance:\\n{instance_name_to_sb_metrics}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "with open(f'{instances_path}/instance_name_to_sb_metrics.json', 'w') as fp:\n",
    "    json.dump(instance_name_to_sb_metrics, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "with open(f'{instances_path}/instance_name_to_sb_metrics.json', 'r') as fp:\n",
    "    instance_name_to_sb_metrics = json.load(fp)\n",
    "print(instance_name_to_sb_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve each instance with baseline rl agent\n",
    "instance_name_to_baseline_metrics = {metric: {instance_name: None for instance_name in instance_names} for metric in metrics}\n",
    "for instance_path, instance_name in zip(instance_paths, instance_names):\n",
    "    print(f'\\n> Instance {instance_name} <')\n",
    "    \n",
    "    # init env\n",
    "    env = EcoleBranching(observation_function='default',\n",
    "                         information_function='default',\n",
    "                         reward_function='default',\n",
    "                         scip_params='default')\n",
    "    env.seed(0)\n",
    "    \n",
    "    # init agent\n",
    "    if nrows == 100 and ncols == 100:\n",
    "        emb_size, num_rounds = 128, 2\n",
    "        policy_network_path = '/scratch/datasets/retro_branching/dqn_learner/dqn_gnn/dqn_gnn_161/checkpoint_11/value_network_1_params.pkl'\n",
    "        name = 'dqn_gnn_161'\n",
    "    elif nrows == 500 and ncols == 1000:\n",
    "        emb_size, num_rounds = 64, 1\n",
    "        policy_network_path = '/scratch/datasets/retro_branching/supervised_learner/gnn/gnn_21/checkpoint_275/trained_params.pkl'\n",
    "        name = 'gnn_21_checkpoint_275'\n",
    "    else:\n",
    "        raise Exception('Not implemented')\n",
    "    policy_network = BipartiteGCN(device='cpu',\n",
    "                                  emb_size=emb_size, # 64 128\n",
    "                                  num_rounds=num_rounds, # 1 2\n",
    "                                  cons_nfeats=5,\n",
    "                                  edge_nfeats=1,\n",
    "                                  var_nfeats=19,\n",
    "                                  aggregator='add')\n",
    "    policy_network.load_state_dict(torch.load(policy_network_path, map_location='cpu'))\n",
    "    \n",
    "    agent = REINFORCEAgent(policy_network=policy_network,\n",
    "                           device='cpu',\n",
    "                           temperature=1.0,\n",
    "                           name=name)\n",
    "    agent.eval()\n",
    "    \n",
    "#     raise Exception() # comment if want to gen new data\n",
    "    \n",
    "    # init instance\n",
    "    instance = pyscipopt.Model()\n",
    "    instance.readProblem(instance_path)\n",
    "    instance = ecole.scip.Model.from_pyscipopt(instance)\n",
    "    \n",
    "    # reset env with instance\n",
    "    obs, action_set, reward, done, info = env.reset(instance)\n",
    "    \n",
    "    # solve\n",
    "    t = 0\n",
    "    print(f'Step {t} | Num nodes: {info[\"num_nodes\"]}')\n",
    "    while not done:\n",
    "        action, action_idx = agent.action_select(action_set=action_set, obs=obs)\n",
    "        obs, action_set, reward, done, info = env.step(action)\n",
    "        t += 1\n",
    "        print(f'Step {t} | Num nodes: {info[\"num_nodes\"]} | LP iterations: {info[\"lp_iterations\"]}')\n",
    "    for metric in metrics:\n",
    "        instance_name_to_baseline_metrics[metric][instance_name] = info[metric]\n",
    "    \n",
    "print(f'\\nAgent {agent.name} metrics for each instance:\\n{instance_name_to_baseline_metrics}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "with open(f'{instances_path}/instance_name_to_baseline_metrics.json', 'w') as fp:\n",
    "    json.dump(instance_name_to_baseline_metrics, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "with open(f'{instances_path}/instance_name_to_baseline_metrics.json', 'r') as fp:\n",
    "    instance_name_to_baseline_metrics = json.load(fp)\n",
    "print(instance_name_to_baseline_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot learning curves relative to above metrics for each instance\n",
    "\n",
    "learner = 'reinforce_learner' # 'reinforce_learner' 'dqn_learner'\n",
    "base_name = 'rl_gnn' # 'rl_gnn' 'dqn_gnn'\n",
    "net_name = 'policy_network_params' # 'policy_network_params' 'value_network_1_params'\n",
    "\n",
    "ids = [i for i in range(1, 11)]\n",
    "id_to_instance_name = {ids[idx]: instance_names[idx] for idx in range(len(ids))}\n",
    "\n",
    "instance_name_to_agent_metrics = {metric: {instance_name: None for instance_name in id_to_instance_name.values()} for metric in metrics}\n",
    "nested_dict = lambda: defaultdict(nested_dict)\n",
    "\n",
    "logs_dict = {'overfit_agent': {}, f'{agent.name}': {}, 'strong_branching': {}}\n",
    "for metric in metrics:\n",
    "    for key in logs_dict.keys():\n",
    "        logs_dict[key][metric] = []\n",
    "\n",
    "for i in ids:\n",
    "    plot_dict = nested_dict()\n",
    "    \n",
    "    instance_name = id_to_instance_name[i]\n",
    "    print(f'\\nInstance {instance_name}')\n",
    "    \n",
    "    _agent = '{}_{}'.format(base_name, i)\n",
    "    agent_path = f'{instances_path}/{learner}/{base_name}/{_agent}/'\n",
    "    \n",
    "    foldername = get_most_recent_checkpoint_foldername(agent_path, idx=-1)\n",
    "    path = '{}{}/'.format(agent_path, foldername)\n",
    "        \n",
    "    with gzip.open(*glob.glob(path+'episodes_log.pkl'), 'rb') as f:\n",
    "        log = pickle.load(f)\n",
    "        \n",
    "    for metric in metrics:\n",
    "        plot_dict[_agent]['y_values'] = log[metric]\n",
    "        plot_dict[_agent]['x_values'] = list(range(len(log[metric])))\n",
    "        \n",
    "        # store mean of last 100 episodes\n",
    "        instance_name_to_agent_metrics[metric][instance_name] = np.mean(log[metric][-100:])\n",
    "        \n",
    "        # update logs_dict\n",
    "        logs_dict['overfit_agent'][metric].append(instance_name_to_agent_metrics[metric][instance_name]) \n",
    "        logs_dict[f'{agent.name}'][metric].append(instance_name_to_baseline_metrics[metric][instance_name])\n",
    "        logs_dict['strong_branching'][metric].append(instance_name_to_sb_metrics[metric][instance_name])\n",
    "        \n",
    "        horizontal_lines = {'strong_branching': instance_name_to_sb_metrics[metric][instance_name], f'{agent.name}': instance_name_to_baseline_metrics[metric][instance_name]}\n",
    "        _ = sns_plot_val_line(plot_dict,\n",
    "                              moving_average_window=100, # 7 60 200 800\n",
    "                              plot_unfiltered_data=True,\n",
    "                              horizontal_lines=horizontal_lines,\n",
    "                              unfiltered_data_alpha=0.3,\n",
    "                              xlabel='Episode',\n",
    "                              ylabel=metric,\n",
    "                              title=f'{instance_name} {metric}',\n",
    "                              show_fig=True)\n",
    "        \n",
    "print(f'\\nAgent {_agent} metrics for each instance:\\n{instance_name_to_agent_metrics}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve each instance with final agent network\n",
    "instance_name_to_agent_metrics = {metric: {instance_name: None for instance_name in instance_names} for metric in metrics}\n",
    "for instance_path, instance_name in zip(instance_paths, instance_names):\n",
    "    print(f'\\n> Instance {instance_name} <')\n",
    "    \n",
    "    # init env\n",
    "    env = EcoleBranching(observation_function='default',\n",
    "                         information_function='default',\n",
    "                         reward_function='default',\n",
    "                         scip_params='default')\n",
    "    env.seed(0)\n",
    "    \n",
    "    # init agent\n",
    "    _agent = '{}_{}'.format(base_name, i)\n",
    "    agent_path = f'{instances_path}/{learner}/{base_name}/{_agent}/'\n",
    "    agent_path += get_most_recent_checkpoint_foldername(agent_path, idx=-1)\n",
    "    \n",
    "    policy_network = BipartiteGCN(device='cpu',\n",
    "                                  emb_size=emb_size, # 64 128\n",
    "                                  num_rounds=num_rounds, # 1 2\n",
    "                                  cons_nfeats=5,\n",
    "                                  edge_nfeats=1,\n",
    "                                  var_nfeats=19,\n",
    "                                  aggregator='add')\n",
    "    policy_network.load_state_dict(torch.load(agent_path+f'/{net_name}.pkl', map_location='cpu'))\n",
    "    \n",
    "    agent = REINFORCEAgent(policy_network=policy_network,\n",
    "                           device='cpu',\n",
    "                           temperature=1.0,\n",
    "                           name='overfit_agent')\n",
    "    agent.eval()\n",
    "    \n",
    "#     raise Exception() # comment if want to gen new data\n",
    "    \n",
    "    # init instance\n",
    "    instance = pyscipopt.Model()\n",
    "    instance.readProblem(instance_path)\n",
    "    instance = ecole.scip.Model.from_pyscipopt(instance)\n",
    "    \n",
    "    # reset env with instance\n",
    "    obs, action_set, reward, done, info = env.reset(instance)\n",
    "    \n",
    "    # solve\n",
    "    t = 0\n",
    "    print(f'Step {t} | Num nodes: {info[\"num_nodes\"]}')\n",
    "    while not done:\n",
    "        action, action_idx = agent.action_select(action_set=action_set, obs=obs)\n",
    "        obs, action_set, reward, done, info = env.step(action)\n",
    "        t += 1\n",
    "        print(f'Step {t} | Num nodes: {info[\"num_nodes\"]} | LP iterations: {info[\"lp_iterations\"]}')\n",
    "    for metric in metrics:\n",
    "        instance_name_to_agent_metrics[metric][instance_name] = info[metric]\n",
    "    \n",
    "print(f'\\nAgent {agent.name} metrics for each instance:\\n{instance_name_to_agent_metrics}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logs_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot hist\n",
    "\n",
    "x_tick_freq = 1\n",
    "for metric in metrics:\n",
    "    data = {'Instance': [], f'{metric}': [], 'Agent': []}\n",
    "    fig = plt.figure()\n",
    "    \n",
    "    for baseline_name in logs_dict.keys():\n",
    "        data['Instance'] += [i for i in range(len(logs_dict[baseline_name][metric]))]\n",
    "        data[f'{metric}'] += logs_dict[baseline_name][metric]\n",
    "        data['Agent'] += [baseline_name for _ in range(len(logs_dict[baseline_name][metric]))]\n",
    "        \n",
    "    data = pd.DataFrame(data)\n",
    "    g = sns.catplot(data=data, kind='bar', x='Instance', y=f'{metric}', hue='Agent', palette='hls')\n",
    "    \n",
    "    colours = iter(sns.color_palette(palette='hls', n_colors=len(list(logs_dict.keys())), desat=None))\n",
    "    for baseline_name in logs_dict.keys():\n",
    "        plt.axhline(y=np.mean(logs_dict[baseline_name][metric]), color=next(colours), linestyle='--', alpha=1)\n",
    "        \n",
    "    for counter, label in enumerate(g.ax.xaxis.get_ticklabels()):\n",
    "        if counter % x_tick_freq == 0:\n",
    "            label.set_visible(True)\n",
    "        else:\n",
    "            label.set_visible(False)  \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# horizontal bar of agent score normalsied w.r.t. some other baseline (e.g. strong branching)\n",
    "agent_name = 'overfit_agent'\n",
    "baseline_agent_name = 'strong_branching'\n",
    "for metric in metrics:\n",
    "    fig = plt.figure()\n",
    "    \n",
    "    # gather data\n",
    "    agent_data = {'Instance': [i for i in range(len(logs_dict[agent_name][metric]))], \n",
    "                  f'{metric}': np.array(logs_dict[agent_name][metric]),\n",
    "                  'Agent': [agent_name for i in range(len(logs_dict[agent_name][metric]))]}\n",
    "    baseline_data = {'Instance': [i for i in range(len(logs_dict[baseline_agent_name]))], \n",
    "                     f'{metric}': np.array(logs_dict[baseline_agent_name][metric])}\n",
    "    \n",
    "    # normalise agent metric data w.r.t. baseline metric data\n",
    "    agent_data[f'{metric}'] /= baseline_data[f'{metric}']\n",
    "    \n",
    "    # count % of instances agent metric was lesser/equal/greater than baseline agent metric\n",
    "    percent_lesser = 100 * np.count_nonzero(agent_data[f'{metric}'] < 1) / len(agent_data[f'{metric}'])\n",
    "    percent_equal = 100 * np.count_nonzero(agent_data[f'{metric}'] == 1) / len(agent_data[f'{metric}'])\n",
    "    percent_greater = 100 * np.count_nonzero(agent_data[f'{metric}'] > 1) / len(agent_data[f'{metric}'])\n",
    "    \n",
    "    # plot\n",
    "    agent_data = pd.DataFrame(agent_data)\n",
    "    sns.barplot(data=agent_data, x=f'{metric}', y='Instance', hue='Agent', palette='pastel', orient='h')\n",
    "    plt.axvline(x=1, linestyle='--', label=baseline_agent_name, alpha=1, color='g')\n",
    "    \n",
    "    # legend, informative axes titles, title\n",
    "    ax = plt.gca()\n",
    "    ax.legend(ncol=1, frameon=True)\n",
    "    ax.set(xlim=None, ylabel='Instance', xlabel=f'{baseline_agent_name}-normalised {metric}')\n",
    "    plt.title(f'{agent_name} <, ==, > {baseline_agent_name}: {percent_lesser:.1f}%, {percent_equal:.1f}%, {percent_greater:.1f}%')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logs_dict)\n",
    "with open(f'{instances_path}/logs_dict.json', 'w') as fp:\n",
    "    json.dump(logs_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can now re-load as desired\n",
    "with open(f'{instances_path}/logs_dict.json', 'r') as fp:\n",
    "    loaded_logs_dict = json.load(fp)\n",
    "print(loaded_logs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlgnn",
   "language": "python",
   "name": "rlgnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
