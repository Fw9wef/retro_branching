{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much does step run time vary across an episode? \n",
    "\n",
    "If it varies by a lot, this suggests that actions have an impact on the backend SCIP/ecole run times, and therefore that in order to reduce overall run time, we should somehow incorporate run time into the agent's reward. If step times are relatively constant, then regardless of what action we take we will have similar run times, and so we just want to e.g. converge on a solution with as few steps as possible, therefore we can just use e.g. dual bound, number of nodes etc. rewards to optimise our overall objective of faster solving times.\n",
    "\n",
    "We will use 2 agents; imitation_1k and imitation_100k. Both agents have the same model architecture and so should have the same inference time. We will set imitation_100k as the 'base agent' and imitation_1k as the 'rollout agent'. At each step, we will take a step with both agents and record their step times. We will then reset the rollout agent's environment and bring it back to the same state as the base agent before taking a step with both agents. In this way, each agent's step times are directly comparable, and any difference between them should be due to different action selection (assuming hardware performance does not vary between steps).\n",
    "\n",
    "**N.B. Whenever doing rollouts where you want to have environments resetting the env in the same way, you MUST set env.seed(seed) just before calling env.reset(instance_before_reset.copy_orig()) for ALL envs, or will get different env.reset() behaviour!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import retro_branching\n",
    "from retro_branching.learners import REINFORCELearner\n",
    "from retro_branching.agents import REINFORCEAgent, StrongBranchingAgent\n",
    "from retro_branching.networks import BipartiteGCN\n",
    "from retro_branching.environments import EcoleBranching\n",
    "\n",
    "import ecole\n",
    "import torch\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base agent\n",
    "device = 'cpu'\n",
    "policy = BipartiteGCN(device=device,\n",
    "                   emb_size=64,\n",
    "                   num_rounds=1,\n",
    "                   aggregator='add')\n",
    "policy.load_state_dict(torch.load('/scratch/datasets/retro_branching/supervised_learner/gnn/gnn_21/checkpoint_275/trained_params.pkl'))\n",
    "policy.eval()\n",
    "base_agent = REINFORCEAgent(policy,\n",
    "                            device=device,\n",
    "                            temperature=1.0,\n",
    "                            name='base_agent')\n",
    "\n",
    "# rollout agent\n",
    "policy = BipartiteGCN(device=device,\n",
    "                   emb_size=64,\n",
    "                   num_rounds=1,\n",
    "                   aggregator='add')\n",
    "policy.load_state_dict(torch.load('/scratch/datasets/retro_branching/supervised_learner/gnn/gnn_1/checkpoint_1/trained_params.pkl'))\n",
    "policy.eval()\n",
    "rollout_agent = REINFORCEAgent(policy,\n",
    "                               device=device,\n",
    "                               temperature=1.0,\n",
    "                               name='rollout_agent')\n",
    "\n",
    "\n",
    "# base env\n",
    "base_env = EcoleBranching(observation_function='default',\n",
    "                          information_function='default',\n",
    "                          reward_function='default',\n",
    "                          scip_params='default')\n",
    "base_env.seed(0)\n",
    "\n",
    "# instances\n",
    "instances = ecole.instance.SetCoverGenerator(n_rows=500, n_cols=1000, density=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_episodes = 10\n",
    "\n",
    "plot_dicts = {'base': {'step_times': []},\n",
    "              'rollout': {'step_times': []}}\n",
    "for i in range(num_episodes):\n",
    "    print(f'\\nEpisode {i}')\n",
    "    base_actions, base_action_sets = [], [] # store base agent history for rollouts\n",
    "    \n",
    "    # find an instance not pre-solved by environment\n",
    "    base_obs = None\n",
    "    while base_obs is None:\n",
    "        base_env.seed(0)\n",
    "        instance = next(instances)\n",
    "        instance_before_reset = instance.copy_orig()\n",
    "        base_obs, base_action_set, base_reward, base_done, base_info = base_env.reset(instance)\n",
    "    \n",
    "    # run episode\n",
    "    with torch.no_grad():\n",
    "        while not base_done:\n",
    "            # get rollout env to same state as base env\n",
    "            rollout_env = EcoleBranching(observation_function=base_env.str_observation_function,\n",
    "                                         information_function=base_env.str_information_function,\n",
    "                                         reward_function=base_env.str_reward_function,\n",
    "                                         scip_params=base_env.str_scip_params)\n",
    "            rollout_env.seed(0)\n",
    "            rollout_obs, rollout_action_set, rollout_reward, rollout_done, rollout_info = rollout_env.reset(instance_before_reset.copy_orig())\n",
    "            for action, action_set in zip(base_actions, base_action_sets):\n",
    "                rollout_obs, rollout_action_set, rollout_reward, rollout_done, rollout_info = rollout_env.step(action_set[action.item()])\n",
    "        \n",
    "            # take action with base agent\n",
    "            base_action = base_agent.action_select(base_action_set, base_obs)\n",
    "            base_actions.append(base_action)\n",
    "            base_action_sets.append(base_action_set)\n",
    "            base_action = base_action_set[base_action.item()]\n",
    "            base_start_step = time.time()\n",
    "            base_obs, base_action_set, base_reward, base_done, base_info = base_env.step(base_action)\n",
    "            base_end_step = time.time()\n",
    "            base_time = base_end_step - base_start_step\n",
    "            plot_dicts['base']['step_times'].append(base_time)\n",
    "        \n",
    "            # take action with rollout agent\n",
    "            rollout_action = rollout_agent.action_select(rollout_action_set, rollout_obs)\n",
    "            rollout_action = rollout_action_set[rollout_action.item()]\n",
    "            rollout_start_step = time.time()\n",
    "            rollout_obs, rollout_action_set, rollout_reward, rollout_done, rollout_info = rollout_env.step(rollout_action)\n",
    "            rollout_end_step = time.time()\n",
    "            rollout_time = rollout_end_step - rollout_start_step\n",
    "            plot_dicts['rollout']['step_times'].append(rollout_time)\n",
    "            \n",
    "            print(f'Base env step time: {round(base_time, 3)} | Rollout env step time: {round(rollout_time, 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in plot_dicts.keys():\n",
    "    for stat in plot_dicts[name]:\n",
    "        fig = plt.figure()\n",
    "        std, mean = np.std(plot_dicts[name][stat]), np.mean(plot_dicts[name][stat])\n",
    "        print(f'mean: {mean}, std: {std}')\n",
    "        title = f'Agent \\'{name}\\' episode {stat} (mean={round(mean,3)}, std={round(std,3)})'\n",
    "        sns.histplot(plot_dicts[name][stat], edgecolor='k')\n",
    "        plt.title(title)\n",
    "        plt.xlabel(f'{stat}')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlgnn",
   "language": "python",
   "name": "rlgnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
