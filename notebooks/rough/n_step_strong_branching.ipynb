{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strong branching works by (for a minimisation problem) calculating the reduction in the dual bound gained by branching on each variable, and then branching on the variable with the largest reduction in the dual bound. This is equivalent to a '1-step lookahead' in the dual bound reduction for each candidate variable.\n",
    "\n",
    "If we were too lookahead n-steps, as n tends towards the number of steps needed to converge on an optimal solution (where the primal-dual gap is 0), you will converge on the globally optimum solution. However, strong branching is expensive, and even n=1 (1-step lookahead) SB does not scale.\n",
    "\n",
    "An open question might be; how much better can we be than 1-step strong branching? We can test this by implementing n-step strong branching, and seeing if e.g. n=2 gives better results (in terms of e.g. number of nodes in search tree) than for n=1. If it makes little difference for a given problem, this means that for this particular problem, n=1 strong branching is close to the optimal branching policy. In this case, we wouldn't expect our agent to even beat SB, only tend towards imitating it. If n!=1 does result in improvement, we would expect our RL agent to be able to work out how to beat strong branching, or at least find actions which are different to SB.\n",
    "\n",
    "In this notebook, we will implement n-step strong branching and apply it to small 100x100 set cover instances to try to answer the question; how much scope is there to improve beyond 1-step strong branching for these instance sizes?\n",
    "\n",
    "To implement n-step strong branching, at each step in the episode, we will:\n",
    "\n",
    "1. Save the dual bound value at the current step\n",
    "2. For i in range(n): do strong branching at each step. \n",
    "3. After the n-th step, calculate the dual bound reduction from branching at each variable relative to the initial dual bound saved at step 1. The variable which resulted in the largest dual bound decrease \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import retro_branching\n",
    "from retro_branching.environments import EcoleBranching\n",
    "\n",
    "import ecole\n",
    "\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrongBranchingAgent:\n",
    "    def __init__(self, name='sb'):\n",
    "        self.name = name\n",
    "        self.strong_branching_function = ecole.observation.StrongBranchingScores()\n",
    "\n",
    "    def before_reset(self, model):\n",
    "        \"\"\"\n",
    "        This function will be called at initialization of the environment (before dynamics are reset).\n",
    "        \"\"\"\n",
    "        self.strong_branching_function.before_reset(model)\n",
    "    \n",
    "    def extract(self, model, done, **kwargs):\n",
    "        return self.strong_branching_function.extract(model, done)\n",
    "\n",
    "    def action_select(self, action_set, model, done):\n",
    "        scores = self.extract(model, done)[action_set]\n",
    "        action = scores.argmax()\n",
    "        \n",
    "        # DEBUG\n",
    "        print('sb action set: {}'.format(action_set))\n",
    "        print('sb scores: {}'.format({action: score for action, score in zip(action_set, scores)}))\n",
    "        print('sb action: idx={} (action={})'.format(action, action_set[action]))\n",
    "        \n",
    "        return action\n",
    "    \n",
    "class CustomStrongBranchingAgent:\n",
    "    def __init__(self, name='sb'):\n",
    "        self.name = name\n",
    "\n",
    "    def before_reset(self, model):\n",
    "        \"\"\"\n",
    "        This function will be called at initialization of the environment (before dynamics are reset).\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def extract(self, model, done, action_set, action_history, instance_before_reset):\n",
    "        action_to_score = {var: 0 for var in range(model.as_pyscipopt().getNVars())}\n",
    "        for action in action_set:\n",
    "            env = EcoleBranching(observation_function='default',\n",
    "                                  information_function='default',\n",
    "                                  reward_function='default',\n",
    "                                  scip_params='default')\n",
    "            env.seed(0)\n",
    "            _ = env.reset(instance_before_reset.copy_orig())\n",
    "            \n",
    "            # rollout to current state\n",
    "            for a in action_history:\n",
    "                _ = env.step(a)\n",
    "            \n",
    "            m = env.model.as_pyscipopt()\n",
    "            init_dual_bound = m.getDualbound()\n",
    "            env.step(action)\n",
    "            m = env.model.as_pyscipopt()\n",
    "            final_dual_bound = m.getDualbound()\n",
    "            action_to_score[action] = abs(init_dual_bound-final_dual_bound)\n",
    "        return list(action_to_score.values())\n",
    "\n",
    "    def action_select(self, action_set, model, done):\n",
    "        scores = self.extract(model, done, action_set)[action_set]\n",
    "        action = scores.argmax()\n",
    "        return action\n",
    "    \n",
    "class TwoStepStrongBranchingAgent:\n",
    "    def __init__(self, name='2_step_sb'):\n",
    "        '''Have not yet implemented N>2 since have many trajectories to track. For now use N=2.'''\n",
    "        self.N = 2\n",
    "        self.name = name\n",
    "    \n",
    "    def before_reset(self, model):\n",
    "        # initialises and resets sb functions when self.extract() called so no need to do anything here\n",
    "        pass\n",
    "        \n",
    "    def extract(self, instance_before_reset, env, _action_history, action_set, done):\n",
    "        action_history = copy.deepcopy(_action_history)\n",
    "            \n",
    "        # init tracking of step -> action -> scores trajectories\n",
    "        step_scores = {step: defaultdict(lambda: 0) for step in range(1, self.N+1)}\n",
    "            \n",
    "        # init env for each candidate branching variable\n",
    "        obs_func, info_func, reward_func, scip_params = env.str_observation_function, env.str_information_function, env.str_reward_function, env.str_scip_params\n",
    "        envs = {action: EcoleBranching(observation_function=obs_func,\n",
    "                                        information_function=info_func,\n",
    "                                        reward_function=reward_func,\n",
    "                                        scip_params=scip_params)\n",
    "               for action in action_set}\n",
    "        \n",
    "        # init env to <step_return_params> maps\n",
    "        env_to_action_set = {e: None for e in envs.keys()}\n",
    "        env_to_done = {e: None for e in envs.keys()}\n",
    "        env_to_action_history = {e: copy.deepcopy(action_history) for e in envs.keys()}\n",
    "\n",
    "        # init corresponding sb agents to use for each env\n",
    "        sb_agents = {e: CustomStrongBranchingAgent() # CustomStrongBranchingAgent() StrongBranchingAgent()\n",
    "               for e in envs.keys()}\n",
    "\n",
    "        # reset envs and sb agents and rollout to current state\n",
    "        for key in envs.keys():\n",
    "            envs[key].seed(0)\n",
    "            sb_agents[key].before_reset(instance_before_reset.copy_orig())\n",
    "            _, env_to_action_set[key], _, env_to_done[key], _ = envs[key].reset(instance_before_reset.copy_orig())\n",
    "            for a in action_history:\n",
    "                _, env_to_action_set[key], _, env_to_done[key], _ = envs[key].step(a)\n",
    "\n",
    "        # get 1-step sb scores for each action, and branch on this action\n",
    "        scores = sb_agents[key].extract(model=envs[key].model, done=done, action_set=env_to_action_set[key], action_history=env_to_action_history[key], instance_before_reset=instance_before_reset.copy_orig())\n",
    "        scores = np.nan_to_num(scores)[env_to_action_set[key]]\n",
    "        keep_running = True\n",
    "        for idx, action in enumerate(env_to_action_set[key]):\n",
    "            step_scores[1][action] += scores[idx]\n",
    "            _, env_to_action_set[action], _, env_to_done[action], _ = envs[action].step(action)\n",
    "            env_to_action_history[action].append(action)\n",
    "            if env_to_done[action]:\n",
    "                # this was first agent to solve problem, no need to do 2-step lookahead after\n",
    "                keep_running = False\n",
    "        \n",
    "        # get range(2, N+1)-step scores for each action\n",
    "        for n in range(2, self.N+1):\n",
    "            if keep_running:\n",
    "                for key in envs.keys():\n",
    "\n",
    "                    if not env_to_done[key]:\n",
    "                        # get sb scores\n",
    "                        s = sb_agents[key].extract(model=envs[key].model, done=env_to_done[key], action_set=env_to_action_set[key], action_history=env_to_action_history[key], instance_before_reset=instance_before_reset.copy_orig())\n",
    "                        # set nans to 0\n",
    "                        s = np.nan_to_num(s)\n",
    "                        # filter out invalid actions from scores\n",
    "                        s = s[env_to_action_set[key]]\n",
    "                        # get sb action index\n",
    "                        a_idx = np.argmax(s)\n",
    "                        env_to_action_history[key].append(env_to_action_set[key][a_idx])\n",
    "                        # store action score\n",
    "                        step_scores[n][key] += s[a_idx]\n",
    "                        # step env with SB score\n",
    "                        _, env_to_action_set[key], _, env_to_done[key], _ = envs[key].step(env_to_action_set[key][a_idx])\n",
    "                        if env_to_done[key]:\n",
    "                            # this was first agent to solve problem, no need to keep running other agents\n",
    "                            keep_running = False\n",
    "                            break\n",
    "        \n",
    "        # calc total dual bound reduction for each action's trajectory\n",
    "        action_to_score = {action: 0 for action in action_set}\n",
    "        for action in action_set:\n",
    "            for n in step_scores.keys():\n",
    "                if action in step_scores[n]:\n",
    "                    action_to_score[action] += step_scores[n][action]\n",
    "\n",
    "        return action_to_score, step_scores\n",
    "        \n",
    "    def action_select(self, instance_before_reset, env, action_history, action_set, done):\n",
    "        scores, step_scores = self.extract(instance_before_reset=instance_before_reset, env=env, _action_history=action_history, action_set=action_set, done=done)\n",
    "        action = np.where(max(scores, key=scores.get)==action_set)\n",
    "        \n",
    "        # DEBUG\n",
    "        print('2-step sb action set: {}'.format(action_set))\n",
    "        print('2-step sb step -> action -> scores: {}'.format(step_scores))\n",
    "        print('2-step sb action: idx={} (action={})'.format(action, max(scores, key=scores.get)))\n",
    "        \n",
    "        return action\n",
    "            \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agents\n",
    "one_agent = StrongBranchingAgent()\n",
    "two_agent = TwoStepStrongBranchingAgent()\n",
    "\n",
    "# envs\n",
    "one_env = EcoleBranching(observation_function='default',\n",
    "                          information_function='default',\n",
    "                          reward_function='default',\n",
    "                          scip_params='default')\n",
    "one_env.seed(0)\n",
    "two_env = EcoleBranching(observation_function='default',\n",
    "                          information_function='default',\n",
    "                          reward_function='default',\n",
    "                          scip_params='default')\n",
    "two_env.seed(0)\n",
    "\n",
    "# instances\n",
    "instances = ecole.instance.SetCoverGenerator(n_rows=100, n_cols=100, density=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_episodes = 100\n",
    "metrics = ['num_nodes', 'solving_time', 'lp_iterations']\n",
    "\n",
    "plot_dict = {'1_step_sb': {metric: [] for metric in metrics},\n",
    "             '2_step_sb': {metric: [] for metric in metrics}}\n",
    "for ep in range(num_episodes):\n",
    "    print('\\n\\n>>> Episode {} <<<'.format(ep))\n",
    "    \n",
    "    # find an instance not pre-solved by environment\n",
    "    one_obs = None\n",
    "    while one_obs is None:\n",
    "        one_env.seed(0)\n",
    "        instance = next(instances)\n",
    "        instance_before_reset = instance.copy_orig()\n",
    "        one_agent.before_reset(instance_before_reset.copy_orig())\n",
    "        one_obs, one_action_set, one_reward, one_done, one_info = one_env.reset(instance)\n",
    "    two_env.seed(0)\n",
    "    two_obs, two_action_set, two_reward, two_done, two_info = two_env.reset(instance_before_reset.copy_orig())\n",
    "    \n",
    "    # 1-step SB agent\n",
    "    # DEBUG\n",
    "    m = one_env.model.as_pyscipopt()\n",
    "    print('\\ninit 1-step sb dual/primal/gap: {}/{}/{}'.format(m.getDualbound(), m.getPrimalbound(), m.getGap()))\n",
    "    t = 1\n",
    "    while not one_done:\n",
    "        print(f'> t={t}')\n",
    "        prev_dual = m.getDualbound()\n",
    "        one_action = one_agent.action_select(one_action_set, one_env.model, one_done)\n",
    "        one_action = one_action_set[one_action]\n",
    "        one_obs, one_action_set, one_reward, one_done, one_info = one_env.step(one_action)\n",
    "        # DEBUG\n",
    "        m = one_env.model.as_pyscipopt()\n",
    "        curr_dual = m.getDualbound()\n",
    "        print('1-step sb dual/primal/gap: {}/{}/{} (dual change = {})'.format(m.getDualbound(), m.getPrimalbound(), m.getGap(), curr_dual-prev_dual))\n",
    "        t += 1\n",
    "    for metric in metrics:\n",
    "        plot_dict['1_step_sb'][metric].append(one_info[metric])\n",
    "    print('>> 1-step SB num nodes: {}'.format(one_info['num_nodes']))\n",
    "        \n",
    "    # 2-step SB agent\n",
    "    # DEBUG\n",
    "    m = two_env.model.as_pyscipopt()\n",
    "    print('\\ninit 2-step sb dual/primal/gap: {}/{}/{}'.format(m.getDualbound(), m.getPrimalbound(), m.getGap()))\n",
    "    action_history = [] # store history of 2-step sb actions taken so can rollout envs to current state\n",
    "    t = 1\n",
    "    while not two_done:\n",
    "        print(f'> t={t}')\n",
    "        prev_dual = m.getDualbound()\n",
    "        two_action = two_agent.action_select(instance_before_reset=instance_before_reset.copy_orig(), env=two_env, action_history=action_history, action_set=two_action_set, done=two_done)\n",
    "        two_action = two_action_set[two_action]\n",
    "        action_history.append(two_action)\n",
    "        two_obs, two_action_set, two_reward, two_done, two_info = two_env.step(two_action)\n",
    "        # DEBUG\n",
    "        m = two_env.model.as_pyscipopt()\n",
    "        curr_dual = m.getDualbound()\n",
    "        print('2-step sb dual/primal/gap: {}/{}/{} (dual change = {})'.format(m.getDualbound(), m.getPrimalbound(), m.getGap(), curr_dual-prev_dual))\n",
    "        t += 1\n",
    "    for metric in metrics:\n",
    "        plot_dict['2_step_sb'][metric].append(two_info[metric])\n",
    "    print('>> 2-step SB num nodes: {}'.format(two_info['num_nodes']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent in plot_dict.keys():\n",
    "    for metric in plot_dict[agent].keys():\n",
    "        fig = plt.figure()\n",
    "        mean, std = np.mean(plot_dict[agent][metric]), np.std(plot_dict[agent][metric])\n",
    "        title = f'{agent} {metric} (mean={round(mean, 3)}, std={round(std, 3)})'\n",
    "        sns.histplot(plot_dict[agent][metric], edgecolor='k')\n",
    "        plt.title(title)\n",
    "        plt.xlabel(metric)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 key observations\n",
    "\n",
    "From above code, there are 2 key observations:\n",
    "\n",
    "1. SCIP strong branching scores are not the same as the total change in the dual bound - e.g. variables which have a 0 change in dual bound might still have highest strong branching score. As such, suspect that there is something under the hood which is doing more than just a 1-step lookahead to predict which variables will be better long term.\n",
    "\n",
    "2. On an instance-by-instance basis, due to SCIP backend controlling node selection, pruning, etc., 2-step SB can end up with more nodes than 1-step SB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation 1: SCIP SB scores != change in dual bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation 2: 1-step SB can have fewer nodes than 2-step SB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlgnn",
   "language": "python",
   "name": "rlgnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
