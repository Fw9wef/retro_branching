{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Strong Branching Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from retro_branching.environments import EcoleBranching\n",
    "from retro_branching.agents import StrongBranchingAgent\n",
    "\n",
    "import ecole\n",
    "import pyscipopt\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = StrongBranchingAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = EcoleBranching()\n",
    "env.seed(0)\n",
    "env2 = EcoleBranching()\n",
    "env2.seed(0)\n",
    "\n",
    "instances = ecole.instance.SetCoverGenerator(n_rows=100, n_cols=100, density=0.05)\n",
    "\n",
    "# find an instance\n",
    "obs = None\n",
    "while obs is None:\n",
    "    instance = next(instances)\n",
    "    instance_before_reset = instance.copy_orig()\n",
    "    obs, action_set, reward, done, info = env.reset(instance)\n",
    "    \n",
    "_ = env2.reset(instance_before_reset.copy_orig())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ecole\n",
    "import pyscipopt\n",
    "from retro_branching.environments import EcoleBranching\n",
    "\n",
    "# ecole branching environment\n",
    "default_scip_params = {'separating/maxrounds': 0,\n",
    "                       'separating/maxroundsroot': 0,\n",
    "                       'separating/maxcuts': 0,\n",
    "                       'separating/maxcutsroot': 0,\n",
    "                       'presolving/maxrounds': 0,\n",
    "                       'presolving/maxrestarts': 0,\n",
    "                       'propagating/maxrounds':0,\n",
    "                       'propagating/maxroundsroot':0,\n",
    "                       'lp/initalgorithm':'d',\n",
    "                       'lp/resolvealgorithm':'d',\n",
    "                       'limits/time': 3600}\n",
    "\n",
    "# class EcoleBranching(ecole.environment.Branching):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         observation_function=\"default\",\n",
    "#         information_function=\"default\",\n",
    "#         reward_function=\"default\",\n",
    "#         scip_params=\"default\",\n",
    "#     ):\n",
    "#         if reward_function == \"default\":\n",
    "#             reward_function = reward_function=({\n",
    "#                      'num_nodes': -ecole.reward.NNodes(),\n",
    "#                      'lp_iterations': -ecole.reward.LpIterations(),\n",
    "#                      'primal_integral': -ecole.reward.PrimalIntegral(),\n",
    "#                      'dual_integral': -ecole.reward.DualIntegral(),\n",
    "#                      'primal_dual_integral': -ecole.reward.PrimalDualIntegral(),\n",
    "#                  })\n",
    "#         if information_function == 'default':\n",
    "#             information_function=({\n",
    "#                      'lp_iterations': ecole.reward.LpIterations().cumsum(),\n",
    "#                      'num_nodes': ecole.reward.NNodes().cumsum(),\n",
    "#                      'solving_time': ecole.reward.SolvingTime().cumsum(),\n",
    "#                      'dual_integral': ecole.reward.DualIntegral(),\n",
    "#                      'primal_dual_integral': ecole.reward.PrimalDualIntegral(),\n",
    "#                  }),\n",
    "#         if observation_function == 'default':    \n",
    "#             observation_function=(\n",
    "#                      ecole.observation.NodeBipartite()\n",
    "#                  ),\n",
    "#         if scip_params == 'default':\n",
    "#             scip_params=default_scip_params\n",
    "        \n",
    "#         super(EcoleBranching, self).__init__(\n",
    "#             observation_function=observation_function,\n",
    "#             information_function=information_function,\n",
    "#             reward_function=reward_function,\n",
    "#             scip_params=scip_params,\n",
    "#         )\n",
    "\n",
    "# branching agent\n",
    "class PseudocostBranchingAgent:\n",
    "    def __init__(self, name='pc'):\n",
    "        self.name = name\n",
    "        self.pc_branching_function = ecole.observation.Pseudocosts()\n",
    "\n",
    "    def extract(self, model, done):\n",
    "        return self.pc_branching_function.extract(model, done)\n",
    "\n",
    "    def action_select(self, action_set, model, done):\n",
    "        scores = self.extract(model, done)\n",
    "        action = scores[action_set].argmax()\n",
    "        return action\n",
    "\n",
    "\n",
    "instances = ecole.instance.SetCoverGenerator(n_rows=500, n_cols=1000, density=0.05)\n",
    "base_env = EcoleBranching()\n",
    "base_env.seed(0)\n",
    "agent = PseudocostBranchingAgent()\n",
    "num_instances = 0\n",
    "while True:\n",
    "    # find an instance not pre-solved by environment\n",
    "    print('\\n >>> New Instance <<<')\n",
    "    obs, counter = None, 0\n",
    "    while obs is None:\n",
    "        counter += 1\n",
    "        print(counter)\n",
    "        instance = next(instances)\n",
    "        instance_before_reset = instance.copy_orig()\n",
    "        obs, action_set, reward, done, info = base_env.reset(instance)\n",
    "    dual_integrals = []\n",
    "    primal_integrals = []\n",
    "        \n",
    "    # use an agent to solve the instance in base_env\n",
    "    while not done:\n",
    "\n",
    "#         # rollout agent from current step to episode termination\n",
    "#         print('>> New Rollout <<')\n",
    "#         curr_state = base_env.model.as_pyscipopt()\n",
    "#         rollout_m = pyscipopt.Model(sourceModel=curr_state, globalcopy=True)\n",
    "#         rollout_env = EcoleBranching()\n",
    "#         rollout_env.seed(0)\n",
    "#         rollout_m = ecole.scip.Model.from_pyscipopt(rollout_m)\n",
    "#         rollout_obs, rollout_action_set, rollout_reward, rollout_done, rollout_info = rollout_env.reset(rollout_m)\n",
    "#         while not rollout_done:\n",
    "#             rollout_action = agent.action_select(rollout_action_set, rollout_env.model, rollout_done)\n",
    "#             rollout_action = rollout_action_set[rollout_action]\n",
    "#             rollout_obs, rollout_action_set, rollout_reward, rollout_done, rollout_info = rollout_env.step(rollout_action)\n",
    "#             rollout_m = rollout_env.model.as_pyscipopt()\n",
    "            \n",
    "#             print('rollout reward: {} | total num nodes: {} | reward primal-dual integral: {} | reward integral: {}'.format(rollout_reward['num_nodes'], rollout_m.getNTotalNodes(), rollout_reward['primal_dual_integral'], rollout_reward['dual_integral']))\n",
    "            \n",
    "            \n",
    "        # get agent action at current step\n",
    "        action = agent.action_select(action_set, base_env.model, done)\n",
    "        action = action_set[action]\n",
    "            \n",
    "        # take step in agent environment\n",
    "        obs, action_set, reward, done, info = base_env.step(action)\n",
    "        print('reward: {}'.format(reward))\n",
    "        print('info: {}'.format(info))\n",
    "        base_m = base_env.model.as_pyscipopt()\n",
    "        print('base reward: {} | total num nodes: {}'.format(reward['num_nodes'], base_m.getNTotalNodes()))\n",
    "        \n",
    "        dual_integrals.append(reward['dual_integral'])\n",
    "        primal_integrals.append(abs(reward['primal_integral']))\n",
    "        \n",
    "        if reward['num_nodes'] < -1e10:\n",
    "            raise Exception('Agent reward {} invalid'.format(reward['num_nodes']))\n",
    "    print(dual_integrals)\n",
    "    dual_integrals.append(0)\n",
    "    primal_integrals.append(0)\n",
    "    dual_integrals_vs_time = [sum(dual_integrals[i:]) for i in range(len(dual_integrals))]\n",
    "    primal_integrals_vs_time = [sum(primal_integrals[i:]) for i in range(len(primal_integrals))]\n",
    "    fig = plt.figure()\n",
    "    plt.plot(dual_integrals_vs_time, label='Dual Integral')\n",
    "    plt.plot(primal_integrals_vs_time, label='Primal Integral')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    num_instances += 1\n",
    "    if num_instances == 10:\n",
    "        raise Exception(f'{num_instances} instances reached')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed\n",
    "x = np.random.randint(1, 10, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import ecole\n",
    "import pyscipopt\n",
    "\n",
    "import copy\n",
    "import pickle\n",
    "import dill\n",
    "\n",
    "# ecole branching environment\n",
    "default_scip_params = {'separating/maxrounds': 0,\n",
    "                       'separating/maxroundsroot': 0,\n",
    "                       'separating/maxcuts': 0,\n",
    "                       'separating/maxcutsroot': 0,\n",
    "                       'presolving/maxrounds': 0,\n",
    "                       'presolving/maxrestarts': 0,\n",
    "                       'propagating/maxrounds':0,\n",
    "                       'propagating/maxroundsroot':0,\n",
    "                       'lp/initalgorithm':'d',\n",
    "                       'lp/resolvealgorithm':'d',\n",
    "                       'limits/time': 3600}\n",
    "\n",
    "class EcoleBranching(ecole.environment.Branching):\n",
    "    def __init__(self,\n",
    "                 observation_function=(\n",
    "                     ecole.observation.NodeBipartite()\n",
    "                 ),\n",
    "                 information_function=({\n",
    "                     'lp_iterations': ecole.reward.LpIterations().cumsum(),\n",
    "                     'num_nodes': ecole.reward.NNodes().cumsum(),\n",
    "                     'solving_time': ecole.reward.SolvingTime().cumsum(),\n",
    "                     'dual_integral': ecole.reward.DualIntegral(),\n",
    "                     'primal_dual_integral': ecole.reward.PrimalDualIntegral(),\n",
    "                 }),\n",
    "                 reward_function=({\n",
    "                     'num_nodes': -ecole.reward.NNodes(),\n",
    "                     'lp_iterations': -ecole.reward.LpIterations(),\n",
    "                     'primal_integral': -ecole.reward.PrimalIntegral(),\n",
    "                     'dual_integral': -ecole.reward.DualIntegral(),\n",
    "                     'primal_dual_integral': -ecole.reward.PrimalDualIntegral(),\n",
    "                 }),\n",
    "                 scip_params=default_scip_params):\n",
    "        super(EcoleBranching, self).__init__(observation_function=observation_function,\n",
    "                                             information_function=information_function,\n",
    "                                             reward_function=reward_function,\n",
    "                                             scip_params=scip_params)\n",
    "#         m = self.model.as_pyscipopt()\n",
    "#         self.prev_num_nodes = m.getNTotalNodes()\n",
    "    \n",
    "#     def scip_total_num_nodes(self):\n",
    "#         m = self.model.as_pyscipopt()\n",
    "#         return m.getNTotalNodes()\n",
    "    \n",
    "#     def scip_change_num_nodes(self):\n",
    "#         m = self.model.as_pyscipopt()\n",
    "#         change_num_nodes = m.getNTotalNodes() - self.prev_num_nodes\n",
    "#         self.prev_num_nodes = mgetNTotalNodes()\n",
    "#         return change_num_nodes\n",
    "    \n",
    "# branching agent\n",
    "class PseudocostBranchingAgent:\n",
    "    def __init__(self, name='pc'):\n",
    "        self.name = name\n",
    "        self.pc_branching_function = ecole.observation.Pseudocosts()\n",
    "\n",
    "    def extract(self, model, done):\n",
    "        return self.pc_branching_function.extract(model, done)\n",
    "\n",
    "    def action_select(self, action_set, model, done):\n",
    "        scores = self.extract(model, done)\n",
    "        action = scores[action_set].argmax()\n",
    "        return action\n",
    "\n",
    "\n",
    "instances = ecole.instance.SetCoverGenerator(n_rows=100, n_cols=100, density=0.05)\n",
    "base_env = EcoleBranching()\n",
    "base_env.seed(0)\n",
    "agent = PseudocostBranchingAgent()\n",
    "while True:\n",
    "    # find an instance not pre-solved by environment\n",
    "    print('\\n >>> New Instance <<<')\n",
    "    obs, counter = None, 0\n",
    "    while obs is None:\n",
    "        counter += 1\n",
    "        print(counter)\n",
    "        instance = next(instances)\n",
    "        instance_before_reset = instance.copy_orig()\n",
    "        obs, action_set, reward, done, info = base_env.reset(instance)\n",
    "        \n",
    "    # use an agent to solve the instance in base_env\n",
    "    while not done:\n",
    "        # check base env state before rollout\n",
    "        base_m = base_env.model.as_pyscipopt()\n",
    "        print(f'base_env before rollout num nodes: {base_m.getNTotalNodes()} | best primal bound: {base_m.getPrimalbound()} | best dual bound: {base_m.getDualbound()} | primal-dual gap: {base_m.getGap()}')\n",
    "        \n",
    "        # rollout agent from current step to episode termination\n",
    "        print('>> New Rollout <<')\n",
    "        curr_state = base_env.model.as_pyscipopt()\n",
    "        rollout_m = pyscipopt.Model(sourceModel=curr_state, globalcopy=True)\n",
    "        \n",
    "#         rollout_env = copy.deepcopy(base_env)\n",
    "#         print('copied!')\n",
    "#         pickle_test = pickle.dumps(base_env)\n",
    "#         pickle_test = pickle.lodas(pickle_test)\n",
    "#         print('pickled!')\n",
    "        dill_test = dill.dumps(base_env)\n",
    "        dill_test = dill.dill(dill_test)\n",
    "        print('dilld!')\n",
    "        \n",
    "        rollout_env = EcoleBranching()\n",
    "        rollout_env.seed(0)\n",
    "        rollout_m = ecole.scip.Model.from_pyscipopt(rollout_m)\n",
    "        rollout_obs, rollout_action_set, rollout_reward, rollout_done, rollout_info = rollout_env.reset(rollout_m)\n",
    "        rollout_m = rollout_env.model.as_pyscipopt()\n",
    "        prev_num_nodes = rollout_m.getNTotalNodes()\n",
    "        while not rollout_done:\n",
    "            rollout_action = agent.action_select(rollout_action_set, rollout_env.model, rollout_done)\n",
    "            rollout_action = rollout_action_set[rollout_action]\n",
    "            rollout_obs, rollout_action_set, rollout_reward, rollout_done, rollout_info = rollout_env.step(rollout_action)\n",
    "            print('ecole rollout reward (num nodes): {}'.format(rollout_reward['num_nodes']))\n",
    "            rollout_m = rollout_env.model.as_pyscipopt()\n",
    "            print('scip rollout reward (num nodes): {}'.format(-(rollout_m.getNTotalNodes()-prev_num_nodes)))\n",
    "            prev_num_nodes = rollout_m.getNTotalNodes()\n",
    "        rollout_m = rollout_env.model.as_pyscipopt()\n",
    "        print(f'rollout_env final total nodes: {rollout_m.getNTotalNodes()} | best primal bound: {rollout_m.getPrimalbound()} | best dual bound: {rollout_m.getDualbound()} | primal-dual gap: {rollout_m.getGap()}')\n",
    "            \n",
    "        # check base env state after rollout\n",
    "        base_m = base_env.model.as_pyscipopt()\n",
    "        print(f'base_env after rollout num nodes: {base_m.getNTotalNodes()} | best primal bound: {base_m.getPrimalbound()} | best dual bound: {base_m.getDualbound()} | primal-dual gap: {base_m.getGap()}')\n",
    "            \n",
    "        # get agent action at current step\n",
    "        action = agent.action_select(action_set, base_env.model, done)\n",
    "        action = action_set[action]\n",
    "            \n",
    "        # take step in agent environment\n",
    "        obs, action_set, reward, done, info = base_env.step(action)\n",
    "        print('ecole agent reward (num nodes): {}'.format(reward['num_nodes']))\n",
    "        base_m = base_env.model.as_pyscipopt()\n",
    "        print('scip agent reward (num nodes): {}'.format(-base_m.getNNodes()))\n",
    "        print('scip agent total nodes: {}'.format(-base_m.getNTotalNodes()))\n",
    "        \n",
    "#         # check if agent reward is valid\n",
    "        if reward['num_nodes'] < -1e10:\n",
    "            raise Exception('Agent reward {} invalid'.format(reward['num_nodes']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Multiple Envs using Same Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from retro_branching.environments import EcoleBranching\n",
    "import ecole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env1 = EcoleBranching()\n",
    "# env1.seed(0)\n",
    "\n",
    "env2 = EcoleBranching()\n",
    "# env2.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = ecole.instance.SetCoverGenerator(n_rows=100, n_cols=200, density=0.05)\n",
    "\n",
    "# find an instance\n",
    "obs, counter = None, 1\n",
    "while obs is None:\n",
    "    print(counter)\n",
    "    instance = next(instances)\n",
    "    instance_before_reset = instance.copy_orig()\n",
    "    obs, action_set, reward, done, info = env1.reset(instance)\n",
    "    counter += 1\n",
    "\n",
    "env2.reset(instance_before_reset)\n",
    "\n",
    "m = env1.model.as_pyscipopt()\n",
    "m2 = env2.model.as_pyscipopt()\n",
    "\n",
    "print('\\nafter reset')\n",
    "print('vars: {} {}'.format(len(m.getVars()), m.getVars()))\n",
    "print('vars2: {} {}'.format(len(m2.getVars()), m2.getVars()))\n",
    "\n",
    "print('\\nconss: {} {}'.format(len(m.getConss()), m.getConss()))\n",
    "print('conss2: {} {}'.format(len(m2.getConss()), m2.getConss()))\n",
    "\n",
    "print(f'\\nbest primal bound: {m.getPrimalbound()} | best dual bound: {m.getDualbound()} | primal-dual gap: {m.getGap()}')\n",
    "print(f'best primal bound: {m2.getPrimalbound()} | best dual bound: {m2.getDualbound()} | primal-dual gap: {m2.getGap()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances1 = ecole.instance.SetCoverGenerator(n_rows=100, n_cols=100, density=0.05)\n",
    "instances2 = ecole.instance.SetCoverGenerator(n_rows=100, n_cols=100, density=0.05)\n",
    "\n",
    "instance1 = next(instances1)\n",
    "instance2 = next(instances2)\n",
    "\n",
    "env1 = EcoleBranching()\n",
    "env2 = EcoleBranching()\n",
    "\n",
    "env1.reset(instance1)\n",
    "env2.reset(instance2)\n",
    "\n",
    "m = env1.model.as_pyscipopt()\n",
    "m2 = env2.model.as_pyscipopt()\n",
    "\n",
    "print(f'\\nbest primal bound: {m.getPrimalbound()} | best dual bound: {m.getDualbound()} | primal-dual gap: {m.getGap()}')\n",
    "print(f'best primal bound: {m2.getPrimalbound()} | best dual bound: {m2.getDualbound()} | primal-dual gap: {m2.getGap()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from retro_branching.environments import EcoleBranching\n",
    "from retro_branching.networks import BipartiteGCN\n",
    "from retro_branching.agents import REINFORCEAgent, StrongBranchingAgent\n",
    "from retro_branching.validators import ReinforcementLearningValidator\n",
    "\n",
    "import ecole\n",
    "import torch\n",
    "import os\n",
    "import gzip\n",
    "import pickle\n",
    "\n",
    "# # load RL agents\n",
    "# ids = [57, 58, 60, 65]\n",
    "# DEVICE = 'cuda:1'\n",
    "# agents = {}\n",
    "# for i in ids:\n",
    "#     # load agent params\n",
    "#     agent_name = 'rl_gnn_{}'.format(i)\n",
    "#     agent_path = '../scripts/reinforce_learner/rl_gnn/{}/'.format(agent_name)\n",
    "#     path = '{}{}/'.format(agent_path, [name for name in os.listdir(agent_path)][-1])\n",
    "#     policy_network = BipartiteGCN(DEVICE)\n",
    "#     policy_network.load_state_dict(torch.load(path+'trained_params.pkl'))\n",
    "\n",
    "#     # init agent\n",
    "#     agent = REINFORCEAgent(policy_network=policy_network, device=DEVICE, name=agent_name)\n",
    "#     agent.eval() # turn on evaluation mode\n",
    "#     agents[agent_name] = agent\n",
    "# load gnn agent (no RL)\n",
    "policy_network = BipartiteGCN(DEVICE)\n",
    "policy_network.load_state_dict(torch.load('../scripts/trained_params_dict_disabler.pkl'))\n",
    "agent = REINFORCEAgent(policy_network=policy_network, device=DEVICE, name='gnn')\n",
    "agent.eval() # turn on evaluation mode\n",
    "agents['gnn'] = agent\n",
    "print(agents)\n",
    "# load strong branching agent\n",
    "agent = StrongBranchingAgent(name='sb')\n",
    "agents['sb'] = agent\n",
    "print('Initialised agents {}'.format(agents.keys()))\n",
    "\n",
    "\n",
    "\n",
    "# init instances generator\n",
    "# instances = ecole.instance.SetCoverGenerator(n_rows=100, n_cols=100, density=0.05)\n",
    "instances = ecole.instance.SetCoverGenerator(n_rows=500, n_cols=1000, density=0.05)\n",
    "print('Initialised instances.')\n",
    "\n",
    "\n",
    "# init envs\n",
    "envs = {}\n",
    "for agent_name in agents.keys():\n",
    "    envs[agent_name] = EcoleBranching()\n",
    "print('Initialised agent envs.')\n",
    "\n",
    "# metrics\n",
    "metrics = ['num_nodes', 'solving_time', 'lp_iterations']\n",
    "\n",
    "\n",
    "validator = ReinforcementLearningValidator(agents=agents,\n",
    "                                           envs=envs,\n",
    "                                           instances=instances,\n",
    "                                           metrics=metrics,\n",
    "                                           seed=0,\n",
    "                                           turn_off_heuristics=False,\n",
    "                                           threshold_difficulty=None,\n",
    "                                           epoch_log_frequency=1,\n",
    "                                           path_to_save='.')\n",
    "validator.test(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from retro_branching.utils import plot_val_line\n",
    "\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "nested_dict = lambda: defaultdict(nested_dict)\n",
    "plot_dicts = {metric: nested_dict() for metric in validator.metrics}\n",
    "\n",
    "for agent in validator.epochs_log.keys():\n",
    "    for metric in validator.metrics:\n",
    "        plot_dicts[metric][agent]['y_values'] = validator.epochs_log[agent][metric]\n",
    "        plot_dicts[metric][agent]['x_values'] = list(range(len(validator.epochs_log[agent][metric])))\n",
    "        print('Agent {} mean {}: {}'.format(agent, metric, np.mean(validator.epochs_log[agent][metric])))\n",
    "        \n",
    "res = 1\n",
    "for metric in plot_dicts.keys():\n",
    "    _ = plot_val_line(plot_dicts[metric],\n",
    "                      xlabel='Epoch',\n",
    "                      ylabel='{}'.format(metric),\n",
    "                      show_fig=True)\n",
    "    _ = plot_val_line(plot_dicts[metric],\n",
    "                      xlabel='Epoch',\n",
    "                      ylabel='Mean {}'.format(metric),\n",
    "                      smooth_data_res=res,\n",
    "                      show_fig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlgnn",
   "language": "python",
   "name": "rlgnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
