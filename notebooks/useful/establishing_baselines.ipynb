{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from retro_branching.utils import get_most_recent_checkpoint_foldername, gen_co_name\n",
    "from retro_branching.networks import BipartiteGCN\n",
    "from retro_branching.agents import Agent, REINFORCEAgent, PseudocostBranchingAgent, StrongBranchingAgent, RandomAgent, DQNAgent, DoubleDQNAgent\n",
    "from retro_branching.environments import EcoleBranching, EcoleConfiguring\n",
    "from retro_branching.validators import ReinforcementLearningValidator\n",
    "\n",
    "import ecole\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import time\n",
    "import gzip\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# import ray\n",
    "# import psutil\n",
    "# num_cpus = psutil.cpu_count(logical=False)\n",
    "# ray.init(num_cpus=int(num_cpus), ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "    \n",
    "\n",
    "# @ray.remote\n",
    "def run_rl_validator(path,\n",
    "                     agents,\n",
    "                     device,\n",
    "#                      nrows=100, \n",
    "#                      ncols=100, \n",
    "                     co_class,\n",
    "                     co_class_kwargs,\n",
    "                     observation_function='default',\n",
    "                     scip_params='default',\n",
    "                     threshold_difficulty=None, \n",
    "                     max_steps=int(1e12), \n",
    "                     max_steps_agent=None,\n",
    "                     overwrite=False):\n",
    "    '''\n",
    "    Cannot pickle ecole objects, so if agent is e.g. 'strong_branching' or 'pseudocost', need to give agent as str so\n",
    "    can initialise inside this ray remote function.\n",
    "    '''\n",
    "    start = time.time()\n",
    "    agent = agents[path]\n",
    "    \n",
    "    if type(agent) == str:\n",
    "        if agent == 'pseudocost':\n",
    "            agent = PseudocostBranchingAgent(name='pseudocost')\n",
    "        elif agent == 'strong_branching':\n",
    "            agent = StrongBranchingAgent(name='strong_branching')\n",
    "        elif agent == 'scip_branching':\n",
    "            class SCIPBranchingAgent:\n",
    "                def __init__(self):\n",
    "                    self.name = 'scip_branching'\n",
    "            agent = SCIPBranchingAgent()\n",
    "        else:\n",
    "            raise Exception(f'Unrecognised agent str {agent}, cannot initialise.')\n",
    "    \n",
    "    if overwrite:\n",
    "        # clear all old rl_validator/ folders even if would not be overwritten with current config to prevent testing inconsistencies\n",
    "        paths = sorted(glob.glob(path+'rl_validator*'))\n",
    "        for p in paths:\n",
    "            print('Removing old {}'.format(p))\n",
    "            shutil.rmtree(p)\n",
    "\n",
    "    # instances\n",
    "#     files = glob.glob(f'/scratch/datasets/retro_branching/instances/set_cover_nrows_{nrows}_ncols_{ncols}_density_005_threshold_{threshold_difficulty}/*.mps')\n",
    "    instances_path = f'/scratch/datasets/retro_branching/instances/{co_class}'\n",
    "    for key, val in co_class_kwargs.items():\n",
    "        instances_path += f'_{key}_{val}'\n",
    "#     instances_path += f'_threshold_{threshold_difficulty}'\n",
    "#     files = glob.glob(f'/scratch/datasets/retro_branching/instances/{co_class}_nrows_{nrows}_ncols_{ncols}_density_005_threshold_{threshold_difficulty}/*.mps')\n",
    "    files = glob.glob(instances_path+f'/scip_{scip_params}/*.mps') # CHANGE: Added scip_params to distinguish baselines and validation instances\n",
    "    instances = iter([ecole.scip.Model.from_file(f) for f in files])\n",
    "    print('Initialised instances.')\n",
    "\n",
    "    # env\n",
    "    if agent.name == 'scip_branching':\n",
    "        env = EcoleConfiguring(observation_function=observation_function,\n",
    "                                  information_function='default',\n",
    "                                  scip_params=scip_params)\n",
    "    else:\n",
    "        env = EcoleBranching(observation_function=observation_function, # 'default' '40_var_features'\n",
    "                             information_function='default',\n",
    "                             reward_function='default',\n",
    "                             scip_params=scip_params)\n",
    "    env.seed(0)\n",
    "    print('Initialised env.')\n",
    "\n",
    "    # metrics\n",
    "    # metrics = ['num_nodes', 'solving_time', 'lp_iterations', 'primal_dual_integral', 'primal_integral', 'dual_integral']\n",
    "    metrics = ['num_nodes', 'solving_time', 'lp_iterations']\n",
    "    print('Initialised metrics: {}'.format(metrics))\n",
    "\n",
    "    # validator\n",
    "    validator = ReinforcementLearningValidator(agents={agent.name: agent},\n",
    "                                               envs={agent.name: env},\n",
    "                                               instances=instances,\n",
    "                                               metrics=metrics,\n",
    "                                               calibration_config_path=None,\n",
    "#                                                calibration_config_path='/home/zciccwf/phd_project/projects/retro_branching/scripts/',\n",
    "                                               seed=0,\n",
    "                                               max_steps=max_steps, # int(1e12), 10, 5, 3\n",
    "                                               max_steps_agent=max_steps_agent,\n",
    "                                               turn_off_heuristics=False,\n",
    "                                               min_threshold_difficulty=None,\n",
    "                                               max_threshold_difficulty=None, # None 250\n",
    "                                               threshold_agent=None,\n",
    "                                               threshold_env=None,\n",
    "                                               episode_log_frequency=1,\n",
    "                                               path_to_save=path,\n",
    "                                               overwrite=overwrite,\n",
    "                                               checkpoint_frequency=10)\n",
    "    print('Initialised validator. Will save to {}'.format(validator.path_to_save))\n",
    "\n",
    "    # run validation tests\n",
    "    validator.test(len(files))\n",
    "    end = time.time()\n",
    "    print('Finished path {} validator in {} s'.format(path, round(end-start, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "learner = 'dqn_learner' # 'reinforce_learner' 'dqn_learner' 'supervised_learner'\n",
    "base_name = 'dqn_gnn' # 'rl_gnn' 'dqn_gnn' 'gnn'\n",
    "AgentClass = Agent # Agent REINFORCEAgent DQNAgent DoubleDQNAgent\n",
    "\n",
    "# CHANGE: Added scip_params to distinguish baselines and validation instances\n",
    "# scip_params = 'default'\n",
    "# scip_params = 'gasse_2019'\n",
    "scip_params = 'dfs'\n",
    "# scip_params = 'bfs'\n",
    "# scip_params = 'uct'\n",
    "\n",
    "# # SC\n",
    "co_class = 'set_covering'\n",
    "# co_class_kwargs = {'n_rows': 100, 'n_cols': 100}\n",
    "# co_class_kwargs = {'n_rows': 165, 'n_cols': 230}\n",
    "# co_class_kwargs = {'n_rows': 250, 'n_cols': 500}\n",
    "# co_class_kwargs = {'n_rows': 300, 'n_cols': 500}\n",
    "co_class_kwargs = {'n_rows': 500, 'n_cols': 1000}\n",
    "# co_class_kwargs = {'n_rows': 1000, 'n_cols': 1000}\n",
    "\n",
    "# CA\n",
    "# co_class = 'combinatorial_auction'\n",
    "# co_class_kwargs = {'n_items': 10, 'n_bids': 50}\n",
    "# co_class_kwargs = {'n_items': 23, 'n_bids': 67}\n",
    "# co_class_kwargs = {'n_items': 37, 'n_bids': 83}\n",
    "\n",
    "# # CFL\n",
    "# co_class = 'capacitated_facility_location'\n",
    "# co_class_kwargs = {'n_customers': 5, 'n_facilities': 5}\n",
    "# co_class_kwargs = {'n_customers': 5, 'n_facilities': 8}\n",
    "# co_class_kwargs = {'n_customers': 5, 'n_facilities': 12}\n",
    "\n",
    "# # MIS\n",
    "# co_class = 'maximum_independent_set'\n",
    "# co_class_kwargs = {'n_nodes': 25}\n",
    "# co_class_kwargs = {'n_nodes': 42}\n",
    "# co_class_kwargs = {'n_nodes': 58}\n",
    "\n",
    "\n",
    "threshold_difficulty = None\n",
    "last_checkpoint_idx = None\n",
    "max_steps = int(1e12) # int(1e12) 3\n",
    "overwrite = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "# agent\n",
    "agents = {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ####### NEW (using config.json file(s) to initialise) ####\n",
    "i = 1481 # 1236 343 1094 341\n",
    "checkpoint = 166 # 457 233 108 120\n",
    "observation_function = '43_var_features' # 'default' '43_var_features'\n",
    "agent_name = f'{base_name}_{i}'\n",
    "device = 'cuda:3' # 'cpu' 'cuda:0'\n",
    "agents, agent_paths = {}, []\n",
    "path = f'/scratch/datasets/retro_branching/{learner}/{base_name}/{agent_name}/checkpoint_{checkpoint}/'\n",
    "config = path + '/config.json'\n",
    "agent = AgentClass(device=device, config=config)\n",
    "agent.name = f'{agent_name}_checkpoint_{checkpoint}'\n",
    "for network_name, network in agent.get_networks().items():\n",
    "#     if network_name == 'networks':\n",
    "#         # TEMPORARY: Fix typo\n",
    "#         network_name = 'network'\n",
    "    if network is not None:\n",
    "        try:\n",
    "            # see if network saved under same var as 'network_name'\n",
    "            agent.__dict__[network_name].load_state_dict(torch.load(path+f'/{network_name}_params.pkl', map_location=device))\n",
    "        except KeyError:\n",
    "            # network saved under generic 'network' var (as in Agent class)\n",
    "            agent.__dict__['network'].load_state_dict(torch.load(path+f'/{network_name}_params.pkl', map_location=device))\n",
    "    else:\n",
    "        print(f'{network_name} is None.')\n",
    "        \n",
    "# # TEMPORARY\n",
    "# agent.default_epsilon = 0\n",
    "# agent.name = agent.name + f'_eps_{agent.default_epsilon}'\n",
    "        \n",
    "agent.eval() # put in test mode\n",
    "path_to_save_baseline = f'/scratch/datasets/retro_branching/instances/{gen_co_name(co_class, co_class_kwargs)}/scip_{scip_params}/baselines/{agent.name}/'\n",
    "agents[path_to_save_baseline] = agent\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # # ######### gnn\n",
    "# gnn = 'gnn_302' # 'gnn_1' 'gnn_21' 'gnn_265'\n",
    "# checkpoint = 'checkpoint_36' # 'checkpoint_1' 'checkpoint_275' 'checkpoint_305'\n",
    "# policy_network = BipartiteGCN(device=device,\n",
    "#                            config=None,\n",
    "#                            emb_size=128,\n",
    "#                            num_rounds=2,\n",
    "#                            cons_nfeats=5,\n",
    "#                            edge_nfeats=1,\n",
    "#                            var_nfeats=19,\n",
    "#                            aggregator='add',\n",
    "#                            name=gnn+'_'+checkpoint)\n",
    "# try:\n",
    "#     policy_network.load_state_dict(torch.load(f'/scratch/datasets/retro_branching/supervised_learner/gnn/{gnn}/{checkpoint}/trained_params.pkl'))\n",
    "# except FileNotFoundError:\n",
    "#     policy_network.load_state_dict(torch.load(f'/scratch/datasets/retro_branching/supervised_learner/gnn/{gnn}/{checkpoint}/network_params.pkl'))\n",
    "# # agent = REINFORCEAgent(policy_network=policy_network,\n",
    "# #                        device=device,\n",
    "# #                        name=policy_network.name)\n",
    "# # agent = DQNAgent(value_network=policy_network,\n",
    "# #                        device=device,\n",
    "# #                        name=policy_network.name)\n",
    "# agent = DoubleDQNAgent(value_network_1=policy_network,\n",
    "#                        value_network_2=copy.deepcopy(policy_network),\n",
    "#                        device=device,\n",
    "#                        name=policy_network.name)\n",
    "# agent.eval()\n",
    "# agent_path = f'/scratch/datasets/retro_branching/instances/set_cover_nrows_{nrows}_ncols_{ncols}_density_005_threshold_{threshold_difficulty}/baselines/{agent.name}/'\n",
    "# agents[agent_path] = agent\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############ rand\n",
    "# device = 'cpu'\n",
    "# num_rand_agents = 20\n",
    "# policy_networks = {f'random_{i}': {'filter_network': None, # None 'gnn_235\n",
    "#                                    'filter_method': 'method_2',\n",
    "#                                    'checkpoints': [1],\n",
    "#                                    'emb_size': 64,\n",
    "#                                    'num_rounds': 1,\n",
    "#                                    'cons_nfeats': 5,\n",
    "#                                    'edge_nfeats': 1,\n",
    "#                                    'var_nfeats': 19,\n",
    "#                                    'aggregator': 'add'} for i in range(1, num_rand_agents+1)}\n",
    "\n",
    "# # load and initialise agents\n",
    "# agents, envs, agent_paths = {}, {}, []\n",
    "\n",
    "\n",
    "# for agent_name in policy_networks.keys():\n",
    "#     agent_path = f'/scratch/datasets/retro_branching/instances/set_cover_nrows_{nrows}_ncols_{ncols}_density_005_threshold_{threshold_difficulty}/baselines/{agent_name}/'\n",
    "#     agent_paths.append(agent_path) # useful for overwriting later in script\n",
    "\n",
    "#     # collect agent NN training checkpoint parameters\n",
    "#     policy_network = BipartiteGCN(device,\n",
    "#                                 emb_size=policy_networks[agent_name]['emb_size'],\n",
    "#                                 num_rounds=policy_networks[agent_name]['num_rounds'],\n",
    "#                                 cons_nfeats=policy_networks[agent_name]['cons_nfeats'],\n",
    "#                                 edge_nfeats=policy_networks[agent_name]['edge_nfeats'],\n",
    "#                                 var_nfeats=policy_networks[agent_name]['var_nfeats'],\n",
    "#                                 aggregator=policy_networks[agent_name]['aggregator'])\n",
    "\n",
    "#     if policy_networks[agent_name]['filter_network'] is not None:\n",
    "#         filter_name = policy_networks[agent_name]['filter_network']\n",
    "#         filter_network = BipartiteGCN(device,\n",
    "#                                 emb_size=filter_networks[filter_name]['emb_size'],\n",
    "#                                 num_rounds=filter_networks[filter_name]['num_rounds'],\n",
    "#                                 cons_nfeats=filter_networks[filter_name]['cons_nfeats'],\n",
    "#                                 edge_nfeats=filter_networks[filter_name]['edge_nfeats'],\n",
    "#                                 var_nfeats=filter_networks[filter_name]['var_nfeats'],\n",
    "#                                 aggregator=filter_networks[filter_name]['aggregator'])\n",
    "# #             filter_network.load_state_dict(torch.load('/scratch/datasets/retro_branching/supervised_learner/gnn/{}/checkpoint_{}/trained_params.pkl'.format(filter_name, filter_networks[filter_name]['checkpoint']), map_location=device))\n",
    "#     else:\n",
    "#         filter_network = None\n",
    "\n",
    "#     path = '{}/'.format(agent_path)\n",
    "# #     policy_network.load_state_dict(torch.load(path+'trained_params.pkl', map_location=device))\n",
    "# #     print('Loaded params from {}'.format(path))\n",
    "#     agent = REINFORCEAgent(policy_network=policy_network, filter_network=filter_network, device=device, name=agent_name, filter_method=policy_networks[agent_name]['filter_method'])\n",
    "#     agent.eval() # turn on evaluation mode\n",
    "#     agents[path] = agent\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "############### random agent\n",
    "# device = 'cpu'\n",
    "# for i in range(1, num_rand_agents+1):\n",
    "#     agents[f'/scratch/datasets/retro_branching/instances/set_cover_nrows_{nrows}_ncols_{ncols}_density_005_threshold_{threshold_difficulty}/baselines/random_agent_{i}/'] = RandomAgent(name=f'random_agent_{i}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ############# pseudocost agent\n",
    "# device = 'cpu'\n",
    "# agent = 'pseudocost'\n",
    "# observation_function = 'default'\n",
    "# agent_path = f'/scratch/datasets/retro_branching/instances/{gen_co_name(co_class, co_class_kwargs)}/scip_{scip_params}/baselines/pseudocost/'\n",
    "# agents[agent_path] = agent\n",
    "\n",
    "# ############ strong branching agent\n",
    "# device = 'cpu'\n",
    "# agent = 'strong_branching'\n",
    "# observation_function = 'default'\n",
    "# agent_path = f'/scratch/datasets/retro_branching/instances/{gen_co_name(co_class, co_class_kwargs)}/scip_{scip_params}/baselines/strong_branching/'\n",
    "# agents[agent_path] = agent\n",
    "\n",
    "# ############ scip branching agent\n",
    "# device = 'cpu'\n",
    "# agent = 'scip_branching'\n",
    "# observation_function = 'default'\n",
    "# agent_path = f'/scratch/datasets/retro_branching/instances/{gen_co_name(co_class, co_class_kwargs)}/scip_{scip_params}/baselines/{agent}/'\n",
    "# agents[agent_path] = agent\n",
    "\n",
    "\n",
    "\n",
    "# path = '/scratch/datasets/retro_branching/reinforce_learner/rl_gnn/rl_gnn_634/checkpoint_1'\n",
    "# device = 'cpu'\n",
    "# config = f'{path}/config.json'\n",
    "# # config = f'/scratch/datasets/retro_branching/reinforce_learner/rl_gnn/rl_gnn_634/checkpoint_1/config.json'\n",
    "# print(config)\n",
    "# agent = REINFORCEAgent(device=device, config=config)\n",
    "# agent.policy_network.load_state_dict(torch.load(f'{path}/trained_params.pkl', map_location=device))\n",
    "# if agent.filter_network is not None:\n",
    "#     agent.filter_network.load_state_dict(torch.load(f'{path}/filter_params.pkl', map_location=device))\n",
    "# agent.eval()\n",
    "\n",
    "\n",
    "max_steps_agent = None\n",
    "\n",
    "\n",
    "\n",
    "print(agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "# # RAY\n",
    "# result_ids = []\n",
    "# for path in agents.keys():\n",
    "#     result_ids.append(run_rl_validator.remote(path, \n",
    "#                                               agents, \n",
    "#                                               device,\n",
    "#                                               nrows, \n",
    "#                                               ncols, \n",
    "#                                               observation_function, \n",
    "#                                               threshold_difficulty, \n",
    "#                                               max_steps, \n",
    "#                                               max_steps_agent, \n",
    "#                                               overwrite))\n",
    "    \n",
    "\n",
    "# start = time.time()\n",
    "# _ = ray.get(result_ids)\n",
    "# end = time.time()\n",
    "\n",
    "\n",
    "# NON-RAY\n",
    "start = time.time()\n",
    "for path in agents.keys():\n",
    "    run_rl_validator(path, \n",
    "                      agents, \n",
    "                      device,\n",
    "#                       nrows, \n",
    "#                       ncols, \n",
    "                      co_class,\n",
    "                      co_class_kwargs,\n",
    "                      observation_function, \n",
    "                      scip_params,\n",
    "                      threshold_difficulty, \n",
    "                      max_steps, \n",
    "                      max_steps_agent, \n",
    "                      overwrite)\n",
    "end = time.time()\n",
    "\n",
    "print('Finished in {}'.format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tick_freq = 10 # frequency with which to draw x-axis labels on graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "if agent in ['scip_branching', 'pseudocost', 'strong_branching']:\n",
    "    class SCIPBranchingAgent:\n",
    "        def __init__(self):\n",
    "            self.name = agent\n",
    "    agent = SCIPBranchingAgent()\n",
    "\n",
    "baselines = sorted(glob.glob(f'/scratch/datasets/retro_branching/instances/{gen_co_name(co_class, co_class_kwargs)}/scip_{scip_params}/baselines/*'))\n",
    "print(f'Saved baselines available: {baselines}')\n",
    "if co_class == 'set_covering':\n",
    "    if co_class_kwargs['n_rows'] == 100:\n",
    "    #     baselines_to_show = ['strong_branching', 'pseudocost', 'rl_gnn_641_checkpoint_1997', 'gnn_341_checkpoint_120', 'gnn_339_checkpoint_90', 'dqn_gnn_1094_checkpoint_108', 'scip_branching']\n",
    "        baselines_to_show = ['strong_branching', 'pseudocost', 'gnn_341_checkpoint_120', 'gnn_339_checkpoint_90', 'dqn_gnn_1318_checkpoint_58']\n",
    "    elif co_class_kwargs['n_rows'] == 500:\n",
    "    #     baselines_to_show = ['gnn_21_checkpoint_275', 'dqn_gnn_1094_checkpoint_108', 'gnn_341_checkpoint_120', 'gnn_343_checkpoint_233']\n",
    "    #     baselines_to_show = ['gnn_343_checkpoint_233', 'dqn_gnn_1147_checkpoint_64', 'pseudocost', 'strong_branching', 'dqn_gnn_1226_checkpoint_200', 'scip_branching']\n",
    "        baselines_to_show = ['gnn_343_checkpoint_233', 'dqn_gnn_1147_checkpoint_64', 'pseudocost', 'strong_branching', 'dqn_gnn_1226_checkpoint_200', 'dqn_gnn_1484_checkpoint_79', 'gnn_361_checkpoint_139']\n",
    "    elif co_class_kwargs['n_rows'] == 1000:\n",
    "        baselines_to_show = ['gnn_343_checkpoint_233']\n",
    "    elif co_class_kwargs['n_rows'] == 165:\n",
    "        baselines_to_show = ['gnn_356_checkpoint_104', 'pseudocost', 'strong_branching']\n",
    "    elif co_class_kwargs['n_rows'] == 250:\n",
    "        baselines_to_show = ['pseudocost', 'strong_branching', 'gnn_358_checkpoint_268']\n",
    "    elif co_class_kwargs['n_rows'] == 300:\n",
    "        baselines_to_show = ['gnn_357_checkpoint_173']\n",
    "    else:\n",
    "        raise Exception(f'Unrecognised n_rows {co_class_kwargs[\"n_rows\"]}')\n",
    "elif co_class == 'combinatorial_auction':\n",
    "    if co_class_kwargs['n_items'] == 10:\n",
    "        baselines_to_show = ['pseudocost', 'strong_branching', 'gnn_347_checkpoint_124']\n",
    "    elif co_class_kwargs['n_items'] == 23:\n",
    "        baselines_to_show = ['pseudocost', 'strong_branching', 'gnn_348_checkpoint_128']\n",
    "    elif co_class_kwargs['n_items'] == 37:\n",
    "        baselines_to_show = ['pseudocost', 'strong_branching', 'gnn_349_checkpoint_98']\n",
    "    else:\n",
    "        raise Exception(f'Unrecognised n_items {co_class_kwargs[\"n_items\"]}')\n",
    "elif co_class == 'capacitated_facility_location':\n",
    "    if co_class_kwargs['n_facilities'] == 5:\n",
    "        baselines_to_show = ['pseudocost', 'strong_branching', 'gnn_350_checkpoint_104']\n",
    "    elif co_class_kwargs['n_facilities'] == 8:\n",
    "        baselines_to_show = ['pseudocost', 'strong_branching', 'gnn_351_checkpoint_69']\n",
    "    elif co_class_kwargs['n_facilities'] == 12:\n",
    "        baselines_to_show = ['pseudocost', 'strong_branching', 'gnn_352_checkpoint_131']\n",
    "    else:\n",
    "        raise Exception(f'Unrecognised n_facilities {co_class_kwargs[\"n_facilities\"]}')\n",
    "elif co_class == 'maximum_independent_set':\n",
    "    if co_class_kwargs['n_nodes'] == 25:\n",
    "        baselines_to_show = ['pseudocost', 'strong_branching', 'gnn_353_checkpoint_209']\n",
    "    elif co_class_kwargs['n_nodes'] == 42:\n",
    "        baselines_to_show = ['pseudocost', 'strong_branching', 'gnn_354_checkpoint_193']\n",
    "    elif co_class_kwargs['n_nodes'] == 58:\n",
    "        baselines_to_show = ['pseudocost', 'strong_branching', 'gnn_355_checkpoint_158']\n",
    "    else:\n",
    "        raise Exception(f'Unrecognised n_nodes {co_class_kwargs[\"n_nodes\"]}')\n",
    "else:\n",
    "    print(f'Not yet configured which baselines to show for co_class {co_class}')\n",
    "baselines_to_show += [agent.name]\n",
    "\n",
    "baselines_logs_dict = {}\n",
    "baseline_to_mean = {}\n",
    "for baseline in baselines:\n",
    "    baseline_name = baseline.split('/')[-1]\n",
    "    if baseline_name in baselines_to_show:\n",
    "        print('')\n",
    "        baselines_logs_dict[baseline_name] = {}\n",
    "        path = baseline + '/rl_validator/rl_validator_1/'\n",
    "        path += get_most_recent_checkpoint_foldername(path)\n",
    "        with gzip.open(*glob.glob(path+'/*log.pkl'), 'rb') as f:\n",
    "            log = pickle.load(f)\n",
    "            for metric in log['metrics']:\n",
    "                baselines_logs_dict[baseline_name][metric] = [abs(np.sum(rewards)) for rewards in log[baseline_name][metric]]\n",
    "                metric_mean = np.mean(baselines_logs_dict[baseline_name][metric])\n",
    "                print('{} mean {}: {}'.format(baseline_name, metric, metric_mean))\n",
    "            \n",
    "\n",
    "for metric in log['metrics']:\n",
    "    fig = plt.figure()\n",
    "    class_colours = iter(sns.color_palette(palette='hls', n_colors=len(list(baselines_logs_dict.keys())), desat=None))\n",
    "    for baseline_name in sorted(baselines_logs_dict.keys()):\n",
    "        metric_mean = np.mean(baselines_logs_dict[baseline_name][metric])\n",
    "        plt.axhline(y=metric_mean, color=next(class_colours), linestyle='--', label=baseline_name)\n",
    "    frame = plt.gca()\n",
    "    frame.axes.get_xaxis().set_visible(False)\n",
    "    plt.ylabel(f'mean {metric}')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics Hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if co_class == 'set_covering':\n",
    "    if co_class_kwargs['n_rows'] == 100:\n",
    "    #     baseline_agent_name = 'strong_branching' # agent to normalise performance with respect to\n",
    "    #     baseline_agent_name = 'gnn_339_checkpoint_90'\n",
    "        baseline_agent_name = 'gnn_341_checkpoint_120'\n",
    "    elif co_class_kwargs['n_rows'] == 500 or co_class_kwargs['n_rows'] == 1000:\n",
    "    #     baseline_agent_name = 'gnn_21_checkpoint_275'\n",
    "    #     baseline_agent_name = 'gnn_341_checkpoint_120'\n",
    "    #     baseline_agent_name = 'dqn_gnn_1094_checkpoint_108'\n",
    "        baseline_agent_name = 'gnn_343_checkpoint_233'\n",
    "    else:\n",
    "        raise Exception(f'Unrecognised nrows {co_class_kwargs[\"n_rows\"]}')\n",
    "else:\n",
    "    print(f'Not yet configured which baselines to show for co_class {co_class}')\n",
    "print(baselines_logs_dict.keys())\n",
    "\n",
    "for metric in log['metrics']:\n",
    "    data = {'Instance': [], f'{metric}': [], 'Agent': []}\n",
    "    fig = plt.figure()\n",
    "#     for baseline_name in sorted(baselines_logs_dict.keys()):\n",
    "#         data['Instance'] += [i for i in range(len(baselines_logs_dict[baseline_name][metric]))]\n",
    "#         data[f'{metric}'] += baselines_logs_dict[baseline_name][metric]\n",
    "#         data['Agent'] += [baseline_name for _ in range(len(baselines_logs_dict[baseline_name][metric]))]\n",
    "\n",
    "    data['Instance'] += [i for i in range(len(baselines_logs_dict[baseline_agent_name][metric]))]\n",
    "    data[f'{metric}'] += baselines_logs_dict[baseline_agent_name][metric]\n",
    "    data['Agent'] += [baseline_agent_name for _ in range(len(baselines_logs_dict[baseline_agent_name][metric]))]\n",
    "    \n",
    "    data['Instance'] += [i for i in range(len(baselines_logs_dict[agent.name][metric]))]\n",
    "    data[f'{metric}'] += baselines_logs_dict[agent.name][metric]\n",
    "    data['Agent'] += [agent.name for _ in range(len(baselines_logs_dict[agent.name][metric]))]\n",
    "    \n",
    "    data = pd.DataFrame(data)\n",
    "    g = sns.catplot(data=data, kind='bar', x='Instance', y=f'{metric}', hue='Agent', palette='hls')\n",
    "    for counter, label in enumerate(g.ax.xaxis.get_ticklabels()):\n",
    "        if counter % x_tick_freq == 0:\n",
    "            label.set_visible(True)\n",
    "        else:\n",
    "            label.set_visible(False)  \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Horizontal Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# if nrows == 100:\n",
    "# #     baseline_agent_name = 'strong_branching' # agent to normalise performance with respect to\n",
    "# #     baseline_agent_name = 'gnn_339_checkpoint_90'\n",
    "#     baseline_agent_name = 'gnn_341_checkpoint_120'\n",
    "# elif nrows == 500:\n",
    "# #     baseline_agent_name = 'gnn_21_checkpoint_275'\n",
    "#     baseline_agent_name = 'gnn_341_checkpoint_120'\n",
    "# else:\n",
    "#     raise Exception(f'Unrecognised nrows {nrows}')\n",
    "# print(baselines_logs_dict.keys())\n",
    "\n",
    "for metric in log['metrics']:\n",
    "    \n",
    "    for agent_name in sorted(baselines_logs_dict.keys()):\n",
    "        if agent_name != baseline_agent_name:\n",
    "            fig = plt.figure()\n",
    "            \n",
    "            # gather data\n",
    "            agent_data = {'Instance': [i for i in range(len(baselines_logs_dict[baseline_agent_name][metric]))], \n",
    "                          f'{metric}': np.array(baselines_logs_dict[agent_name][metric]), \n",
    "                          'Agent': [agent_name for _ in range(len(baselines_logs_dict[agent_name][metric]))]}\n",
    "            \n",
    "            # normalise agent metric data w.r.t. baseline metric data\n",
    "            agent_data[f'{metric}'] /= baselines_logs_dict[baseline_agent_name][f'{metric}']\n",
    "\n",
    "            # count % of instances agent metric was lesser/equal/greater than baseline agent metric\n",
    "            percent_lesser = 100 * np.count_nonzero(agent_data[f'{metric}'] < 1) / len(agent_data[f'{metric}'])\n",
    "            percent_equal = 100 * np.count_nonzero(agent_data[f'{metric}'] == 1) / len(agent_data[f'{metric}'])\n",
    "            percent_greater = 100 * np.count_nonzero(agent_data[f'{metric}'] > 1) / len(agent_data[f'{metric}'])\n",
    "            \n",
    "            # plot\n",
    "            agent_data = pd.DataFrame(agent_data)\n",
    "            sns.barplot(data=agent_data, x=f'{metric}', y='Instance', hue='Agent', palette='pastel', orient='h')\n",
    "            plt.axvline(x=1, linestyle='--', label=baseline_agent_name, alpha=1, color='g')\n",
    "\n",
    "            # legend, informative axes titles, title\n",
    "            ax = plt.gca()\n",
    "            ax.legend(ncol=1, frameon=True)\n",
    "            ax.set(xlim=None, ylabel='Instance', xlabel=f'{baseline_agent_name}-normalised {metric}')\n",
    "            plt.title(f'{agent_name} <, ==, > {baseline_agent_name}: {percent_lesser:.1f}%, {percent_equal:.1f}%, {percent_greater:.1f}%')\n",
    "\n",
    "            for counter, label in enumerate(ax.yaxis.get_ticklabels()):\n",
    "                if counter % x_tick_freq == 0:\n",
    "                    label.set_visible(True)\n",
    "                else:\n",
    "                    label.set_visible(False) \n",
    "            \n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlgnn",
   "language": "python",
   "name": "rlgnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
