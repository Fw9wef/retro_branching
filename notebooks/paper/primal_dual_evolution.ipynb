{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create primal dual evolution plots similar to DeepMind paper https://arxiv.org/pdf/2012.13349.pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "from retro_branching.utils import get_most_recent_checkpoint_foldername, sns_plot_val_line\n",
    "\n",
    "import glob\n",
    "import gzip\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config\n",
    "\n",
    "- `agent_to_path` (*dict*): Dictionary mapping what you want your agent(s) to be labelled as in the plots' legends to where to load the validation log.pkl file of the agent in question.\n",
    "- `evolution_metric_decimals` (*int*): How many decimal places to round the calibrated evolution_metric to when creating evolution_metric bins. This is needed in order to gather multiple gap values into the same evolution_metric bin to take their mean. A higher value will give greater resolution, but may produce noisy graphs and make general trends difficult to identify.\n",
    "- `verbose` (*bool*): Whether to display the current state of the data frame at each data manipulation stage in the core script. Is useful for debugging and sanity checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 100x100\n",
    "agent_to_path = {'RL': '/scratch/datasets/retro_branching/instances/set_cover_nrows_100_ncols_100_density_005_threshold_None/baselines/dqn_gnn_1094_checkpoint_108/rl_validator/rl_validator_1/',\n",
    "                 'SL': '/scratch/datasets/retro_branching/instances/set_cover_nrows_100_ncols_100_density_005_threshold_None/baselines/gnn_341_checkpoint_120/rl_validator/rl_validator_1/'}\n",
    "evolution_metric_decimals = 3 # how to round calibrated evolution_metric to create evolution_metric bins so can get mean gap at each evolution_metric point\n",
    "# evolution_metric_decimals = 0\n",
    "\n",
    "\n",
    "# # # 500x1000\n",
    "# agent_to_path = {'RL': '/scratch/datasets/retro_branching/instances/set_cover_nrows_500_ncols_1000_density_005_threshold_None/baselines/dqn_gnn_1236_checkpoint_457/rl_validator/rl_validator_1/',\n",
    "#                  'SL': '/scratch/datasets/retro_branching/instances/set_cover_nrows_500_ncols_1000_density_005_threshold_None/baselines/gnn_343_checkpoint_233/rl_validator/rl_validator_1/'}\n",
    "# agent_to_path = {'RL': '/scratch/datasets/retro_branching/instances/set_cover_nrows_500_ncols_1000_density_005_threshold_None/baselines/dqn_gnn_1094_checkpoint_108/rl_validator/rl_validator_1/',\n",
    "#                  'SL': '/scratch/datasets/retro_branching/instances/set_cover_nrows_500_ncols_1000_density_005_threshold_None/baselines/gnn_341_checkpoint_120/rl_validator/rl_validator_1/'}\n",
    "# evolution_metric_decimals = 1 # how to round calibrated evolution_metric so can get mean gap at each evolution_metric point\n",
    "\n",
    "\n",
    "# evolution_metric = 'elapsed_calibrated_solve_time'\n",
    "evolution_metric = 'lp_iterations'\n",
    "\n",
    "if evolution_metric == 'lp_iterations':\n",
    "    evolution_metric_decimals = 0\n",
    "\n",
    "verbose = True # True False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core scripts\n",
    "\n",
    "Given the above config, these should run without needing to be edited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load agent logs\n",
    "agent_to_log = {}\n",
    "agent_name_to_display_name = {}\n",
    "for display_name, path in agent_to_path.items():\n",
    "    path += get_most_recent_checkpoint_foldername(path)\n",
    "    with gzip.open(*glob.glob(path+'/*log.pkl'), 'rb') as f:\n",
    "        log = pickle.load(f)\n",
    "    agent_name = log['agent_names'][0]\n",
    "    agent_to_log[agent_name] = log\n",
    "    agent_name_to_display_name[agent_name] = display_name\n",
    "    \n",
    "    print(f'Num evaluation instances solved in {display_name} agent log file: {len(log[agent_name][\"num_nodes\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sol_gap(sol, best_sol, eps=1e-12):\n",
    "    if sol * best_sol < 0:\n",
    "        sol_gap = 1\n",
    "    else:\n",
    "        if sol > best_sol:\n",
    "            sol_gap = (sol - best_sol) / max(sol, best_sol, eps)\n",
    "        else:\n",
    "            sol_gap = (best_sol - sol) / max(sol, best_sol, eps)\n",
    "    return sol_gap\n",
    "\n",
    "def calc_sol_gaps(sols, best_sols):\n",
    "    if type(best_sols) != list:\n",
    "        return [calc_sol_gap(sol, best_sols) for sol in sols]\n",
    "    else:\n",
    "        return [calc_sol_gap(sol, best_sol) for sol, best_sol in zip(sols, best_sols)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get elapsed calibrated solve evolution_metric and gaps\n",
    "agent_to_stats = {agent: defaultdict(lambda: None) for agent in agent_to_log.keys()}\n",
    "for agent, log in agent_to_log.items():\n",
    "    if evolution_metric == 'elapsed_calibrated_solve_time':\n",
    "        agent_to_stats[agent]['evolution_metric'] = [np.cumsum(elapsed_time) for elapsed_time in log[agent]['elapsed_calibrated_solve_time']]\n",
    "    elif evolution_metric == 'lp_iterations':\n",
    "        agent_to_stats[agent]['evolution_metric'] = [np.cumsum(np.abs(lp_iters)) for lp_iters in log[agent]['lp_iterations']]\n",
    "    agent_to_stats[agent]['primal_gaps'] = [calc_sol_gaps(primals, primals[-1]) for primals in log[agent]['primal_bound']]\n",
    "    agent_to_stats[agent]['dual_gaps'] = [calc_sol_gaps(duals, duals[-1]) for duals in log[agent]['dual_bound']]\n",
    "    agent_to_stats[agent]['primal_dual_gaps'] = [calc_sol_gaps(primals, duals) for primals, duals in zip(log[agent]['primal_bound'], log[agent]['dual_bound'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map each step to instance it was in and flatten stats arrays for plotting\n",
    "for agent in agent_to_stats.keys():\n",
    "    stats = agent_to_stats[agent]\n",
    "    # index each step to its instance\n",
    "    stats['instance'] = []\n",
    "    for stat in stats.keys():\n",
    "        if stat != 'instance':\n",
    "            if len(stats['instance']) == 0:\n",
    "                for idx in range(len(stats[stat])):\n",
    "                    for _ in range(len(stats[stat][idx])):\n",
    "                        stats['instance'].append(idx)\n",
    "            stats[stat] = [item for sublist in stats[stat] for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create dataframes (from above dict) for data manipulation and seaborn plotting (use display_name for legends)\n",
    "_df = defaultdict(list)\n",
    "for agent in agent_to_stats.keys():\n",
    "    for stat in agent_to_stats[agent].keys():\n",
    "        for el in agent_to_stats[agent][stat]:\n",
    "            _df[stat].append(el)\n",
    "\n",
    "for agent in agent_to_stats.keys():\n",
    "    for _ in range(len(agent_to_stats[agent]['evolution_metric'])):\n",
    "        _df['Agent'].append(agent_name_to_display_name[agent])\n",
    "                \n",
    "df = pd.DataFrame(_df)\n",
    "if verbose:\n",
    "    print(tabulate(df, headers='keys', tablefmt='psql', showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# group data into evolution_metric bins so can calc mean gap at each evolution_metric bin\n",
    "max_evolution_metric = max(df['evolution_metric'])\n",
    "if evolution_metric_decimals == 0:\n",
    "    num_bins = math.ceil(max_evolution_metric)\n",
    "else:\n",
    "    num_bins = math.ceil(max_evolution_metric / 10**(-evolution_metric_decimals))\n",
    "\n",
    "bins = 10**(-evolution_metric_decimals) * np.arange(0, num_bins+1)\n",
    "\n",
    "grouped_evolution_metric = bins[:-1]\n",
    "\n",
    "df['grouped_evolution_metric'] = pd.cut(df['evolution_metric'], bins=bins, labels=grouped_evolution_metric)\n",
    "if verbose:\n",
    "    print(f'num_bins: {num_bins}')\n",
    "    print(f'evolution_metric_bins: {bins}')\n",
    "    print(f'bin labels: {grouped_evolution_metric}')\n",
    "    print(tabulate(df, headers='keys', tablefmt='psql', showindex=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for any rows with multiple entries for the same evolution_metric bin in a given instance, remove the rows up to the \n",
    "# final row, since agent was able to close the gap up to the final gap value within the set evolution_metric bin\n",
    "\n",
    "_df = copy.deepcopy(df)\n",
    "prev_instance = None\n",
    "df_idx = 0\n",
    "for idx, instance in enumerate(_df['instance']):\n",
    "    if prev_instance == instance:\n",
    "        # still on same instance\n",
    "        if idx > 0:\n",
    "            if _df['grouped_evolution_metric'][idx] == _df['grouped_evolution_metric'][idx-1]:\n",
    "                # repeated grouped evolution_metric, remove previous row\n",
    "                df = df.drop(labels=df_idx-1, axis=0)\n",
    "    prev_instance = copy.deepcopy(instance)\n",
    "    df_idx += 1\n",
    "df = df.reset_index() # reset indices since drop so can properly index again\n",
    "df = df.drop(labels='index', axis=1) # remove annoying index column for below visualisation of dataframe\n",
    "if verbose:\n",
    "    print(tabulate(df, headers='keys', tablefmt='psql', showindex=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in grouped_evolution_metric up to max_grouped_evolution_metric for each instance so all instances have same length\n",
    "\n",
    "\n",
    "def insert_rows_in_df(df, new_df, instance, new_df_offset, grouped_evolution_metric_to_add, evolution_metric_decimals):\n",
    "    # fix any python floating point arithmetic errors\n",
    "    grouped_evolution_metric_to_add = [round(t, evolution_metric_decimals) for t in grouped_evolution_metric_to_add]\n",
    "    \n",
    "    # get instance index in new_df\n",
    "    new_df_indices = new_df.index[new_df['instance'] == instance].tolist()\n",
    "    for new_df_idx in new_df_indices:\n",
    "        if new_df['Agent'][new_df_idx] == agent and new_df['evolution_metric'][new_df_idx] == df['evolution_metric'][idx]:\n",
    "            # found new_df idx to insert values\n",
    "            break\n",
    "        else:\n",
    "            # not current agent or curr evolution_metric, move to next\n",
    "            pass\n",
    "    for i in range(len(grouped_evolution_metric_to_add)):\n",
    "        values = []\n",
    "        for header in df.columns:\n",
    "            if header == 'grouped_evolution_metric' or header == 'evolution_metric':\n",
    "                values.append(grouped_evolution_metric_to_add[i])\n",
    "            else:\n",
    "                values.append(df[header][idx])\n",
    "        new_df = pd.DataFrame(np.insert(new_df.values, new_df_idx+new_df_offset+i, values=values, axis=0), columns=df.columns)\n",
    "    return new_df\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "max_grouped_evolution_metric = max(df['grouped_evolution_metric'])\n",
    "new_df = copy.deepcopy(df)\n",
    "prev_instance = None\n",
    "for idx in range(len(df['instance'])):\n",
    "    instance = df['instance'][idx]\n",
    "    curr_evolution_metric, agent = df['grouped_evolution_metric'][idx], df[\"Agent\"][idx]\n",
    "    if verbose:\n",
    "        print(f'\\n>> instance: {instance} | idx: {idx} | curr_evolution_metric: {curr_evolution_metric} | agent: {agent} <<')\n",
    "    \n",
    "    # use an idx+adder trick so do not get IndexError for last instance in column\n",
    "    if idx < len(df['instance'])-1:\n",
    "        adder = 1\n",
    "    else:\n",
    "        adder = 0\n",
    "        \n",
    "    # 1. check if need to insert rows before current row\n",
    "    new_df_offset = 0 # insert before current row\n",
    "    if instance != prev_instance:\n",
    "        # new instance, add grouped evolution_metric up to first recorded grouped evolution_metric\n",
    "        if verbose:\n",
    "            print('new instance started, add grouped evolution_metric up to first recorded grouped evolution_metric')\n",
    "        start_grouped_evolution_metric = df['grouped_evolution_metric'][idx]\n",
    "        num_rows_to_add = round(start_grouped_evolution_metric / 10**(-evolution_metric_decimals), 0)\n",
    "        grouped_evolution_metric_to_add = 10**(-evolution_metric_decimals) * np.arange(0, num_rows_to_add)\n",
    "        if verbose:\n",
    "            print('making insertions into new_df with params:')\n",
    "            print(f' new_df_offset: {new_df_offset}')\n",
    "            print(f' num_rows_to_add: {num_rows_to_add}')\n",
    "            print(f' grouped_evolution_metric_to_add: {grouped_evolution_metric_to_add}')\n",
    "        new_df = insert_rows_in_df(df, new_df, instance, new_df_offset, grouped_evolution_metric_to_add, evolution_metric_decimals)\n",
    "        \n",
    "    # 2. check if need to insert rows after current row\n",
    "    new_df_offset = 1 # insert after current row\n",
    "    if df['instance'][idx+adder] == instance and adder != 0:\n",
    "        # next row is part of same instance, check next row's evolution_metric value\n",
    "        next_evolution_metric = df['grouped_evolution_metric'][idx+adder]\n",
    "        if curr_evolution_metric + 10**(-evolution_metric_decimals) == next_evolution_metric:\n",
    "            # next row has next grouped evolution_metric given evolution_metric_decimals resolution, no need to fill table\n",
    "            if verbose:\n",
    "                print(f'next row has next grouped evolution_metric, no need to fill table')\n",
    "            grouped_evolution_metric_to_add = []\n",
    "        else:\n",
    "            # missing grouped evolution_metric rows, need to fill table up to next recorded grouped evolution_metric\n",
    "            if verbose:\n",
    "                print('missing grouped rows, need to fill table up to next recorded grouped evolution_metric')\n",
    "            num_rows_to_add = round((next_evolution_metric - curr_evolution_metric) / 10**(-evolution_metric_decimals), 0) - 1\n",
    "            start = (curr_evolution_metric / (10**(-evolution_metric_decimals))) + 1\n",
    "            grouped_evolution_metric_to_add = 10**(-evolution_metric_decimals) * np.arange(start, num_rows_to_add+start)\n",
    "            if verbose:\n",
    "                print('making insertions into new_df with params:')\n",
    "                print(f' new_df_offset: {new_df_offset}')\n",
    "                print(f' num_rows_to_add: {num_rows_to_add}')\n",
    "                print(f' grouped_evolution_metric_to_add: {grouped_evolution_metric_to_add}')\n",
    "            new_df = insert_rows_in_df(df, new_df, instance, new_df_offset, grouped_evolution_metric_to_add, evolution_metric_decimals)\n",
    "    else:\n",
    "        # next row is end of instance, fill rows up to max evolution_metric\n",
    "        if verbose:\n",
    "            print('next row moves to a new instance, need to fill current instance up to max evolution_metric')\n",
    "        num_rows_to_add = round((max_grouped_evolution_metric - curr_evolution_metric) / 10**(-evolution_metric_decimals), 0)\n",
    "        start = (curr_evolution_metric / (10**(-evolution_metric_decimals)))+1\n",
    "        grouped_evolution_metric_to_add = 10**(-evolution_metric_decimals) * np.arange(start, num_rows_to_add+start)\n",
    "        if verbose:\n",
    "            print('making insertions into new_df with params:')\n",
    "            print(f' new_df_offset: {new_df_offset}')\n",
    "            print(f' num_rows_to_add: {num_rows_to_add}')\n",
    "            print(f' grouped_evolution_metric_to_add: {grouped_evolution_metric_to_add}')\n",
    "        new_df = insert_rows_in_df(df, new_df, instance, new_df_offset, grouped_evolution_metric_to_add, evolution_metric_decimals)        \n",
    "\n",
    "    prev_instance = copy.deepcopy(instance)\n",
    "    \n",
    "if verbose:\n",
    "    print(tabulate(new_df, headers='keys', tablefmt='psql', showindex=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix any python floating point arithmetic errors\n",
    "for idx, t in enumerate(new_df['grouped_evolution_metric']):\n",
    "    new_df['grouped_evolution_metric'][idx] = round(t, evolution_metric_decimals)\n",
    "\n",
    "# check if any consequtive evolution_metric are identical\n",
    "for idx in range(len(new_df)):\n",
    "    if idx > 0:\n",
    "        if new_df['grouped_evolution_metric'][idx] == new_df['grouped_evolution_metric'][idx-1]:\n",
    "            raise Exception(f'Found duplicate evolution_metric at idx {idx}')\n",
    "            \n",
    "# check if any consequtive duals within instance do not make sense\n",
    "prev_instance = None\n",
    "for idx, instance in enumerate(new_df['instance']):\n",
    "    if instance == prev_instance:\n",
    "        if new_df['dual_gaps'][idx] > new_df['dual_gaps'][idx-1]:\n",
    "            raise Exception(f'Found consequtive steps in instance with increasing dual at idx {idx}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug zone\n",
    "\n",
    "Set to `debug = True` if needed :,("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False # True False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    print(new_df.head())\n",
    "\n",
    "    tmp_df = new_df[['dual_gaps', 'instance', 'grouped_evolution_metric']]\n",
    "    if verbose:\n",
    "        print(tabulate(tmp_df, headers='keys', tablefmt='psql', showindex=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    evolution_metric_bin_to_dual_gaps = defaultdict(list)\n",
    "    instance_to_evolution_metric_bin = {instance: set() for instance in new_df['instance']}\n",
    "    for idx in range(len(new_df)):\n",
    "        evolution_metric_bin_to_dual_gaps[new_df['grouped_evolution_metric'][idx]].append(new_df['dual_gaps'][idx])\n",
    "        if new_df['grouped_evolution_metric'][idx] not in instance_to_evolution_metric_bin[new_df['instance'][idx]]:\n",
    "            instance_to_evolution_metric_bin[new_df['instance'][idx]].add(new_df['grouped_evolution_metric'][idx])\n",
    "    for idx, evolution_metric_bin in enumerate(list(evolution_metric_bin_to_dual_gaps.keys())):\n",
    "        # all evolution_metric bins should have the same (num_agents*num_instances) number of entries\n",
    "        print(f'evolution_metric bin {evolution_metric_bin} idx {idx} # entries: {len(evolution_metric_bin_to_dual_gaps[evolution_metric_bin])}')\n",
    "    for instance in instance_to_evolution_metric_bin.keys():\n",
    "        for t in evolution_metric_bin_to_dual_gaps.keys():\n",
    "            if t not in instance_to_evolution_metric_bin[instance]:\n",
    "                print(f'instance {instance} does not contain evolution_metric bin {t} ')\n",
    "\n",
    "    evolution_metric_bin_to_mean_dual_gap = {evolution_metric_bin: np.mean(dual_gaps) for evolution_metric_bin, dual_gaps in evolution_metric_bin_to_dual_gaps.items()}\n",
    "    # print(evolution_metric_bin_to_mean_dual_gap)\n",
    "    print(f'\\nevolution_metric bins: {list(evolution_metric_bin_to_mean_dual_gap.keys())}')\n",
    "    print(f'\\ndual gaps: {list(evolution_metric_bin_to_mean_dual_gap.values())}')\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.plot(list(evolution_metric_bin_to_mean_dual_gap.keys()), list(evolution_metric_bin_to_mean_dual_gap.values()))\n",
    "    plt.show()\n",
    "\n",
    "    dual_gaps = list(evolution_metric_bin_to_mean_dual_gap.values())\n",
    "    for idx, dual_gap in enumerate(dual_gaps):\n",
    "        if idx > 0:\n",
    "            if dual_gaps[idx] > dual_gaps[idx-1]:\n",
    "                raise Exception(f'inconsistent at idx {idx}, goes from dual gap {dual_gaps[idx-1]} to {dual_gaps[idx]}')\n",
    "\n",
    "    for idx, t in enumerate(list(evolution_metric_bin_to_mean_dual_gap.keys())):\n",
    "        if t != round(idx * 10**(-evolution_metric_decimals), evolution_metric_decimals):\n",
    "            raise Exception(f'evolution_metric {t} at idx {idx} inconsistent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "\n",
    "estimator = 'mean'\n",
    "# estimator = np.median\n",
    "# estimator = np.mode\n",
    "# estimator = None\n",
    "\n",
    "context = 'paper'\n",
    "style = 'whitegrid'\n",
    "font_scale = 2\n",
    "palette = 'hls'\n",
    "ci = 68\n",
    "errcolor = 'gray'\n",
    "capsize = 0.05\n",
    "fig_size = (7.5, 3)\n",
    "\n",
    "xlog = True # False True\n",
    "ylog = True # False True\n",
    "\n",
    "if evolution_metric == 'elapsed_calibrated_solve_time':\n",
    "    xlabel = 'Cal. Time (s)'\n",
    "elif evolution_metric == 'lp_iterations':\n",
    "    xlabel = '# LP Iterations'\n",
    "else:\n",
    "    raise Exception(f'Unrecognised xlabel evolution_metric {evolution_metric}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure dataframe entries are numerical types for seaborn plotting\n",
    "for col in new_df.columns:\n",
    "    try:\n",
    "        new_df[col] = pd.to_numeric(new_df[col])\n",
    "    except ValueError:\n",
    "        # cannot convert string to numerical type\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(font_scale=font_scale,\n",
    "              context=context,\n",
    "              style=style,\n",
    "              palette=palette)\n",
    "\n",
    "f, ax = plt.subplots(figsize=fig_size)\n",
    "g = sns.lineplot(data=new_df, x='grouped_evolution_metric', y='primal_dual_gaps', hue='Agent', estimator=estimator, ci=ci)\n",
    "g.set_xlabel(xlabel)\n",
    "g.set_ylabel('Primal-Dual Gap')\n",
    "if xlog:\n",
    "    g.set(xscale='log')\n",
    "if ylog:\n",
    "    g.set(yscale='log')\n",
    "plt.show()\n",
    "\n",
    "f, ax = plt.subplots(figsize=fig_size)\n",
    "g = sns.lineplot(data=new_df, x='grouped_evolution_metric', y='primal_gaps', hue='Agent', estimator=estimator, ci=ci)\n",
    "g.set_xlabel(xlabel)\n",
    "g.set_ylabel('Primal Gap')\n",
    "if xlog:\n",
    "    g.set(xscale='log')\n",
    "if ylog:\n",
    "    g.set(yscale='log')\n",
    "plt.show()\n",
    "\n",
    "f, ax = plt.subplots(figsize=fig_size)\n",
    "g = sns.lineplot(data=new_df, x='grouped_evolution_metric', y='dual_gaps', hue='Agent', estimator=estimator, ci=ci)\n",
    "g.set_xlabel(xlabel)\n",
    "g.set_ylabel('Dual Gap')\n",
    "if xlog:\n",
    "    g.set(xscale='log')\n",
    "if ylog:\n",
    "    g.set(yscale='log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlgnn",
   "language": "python",
   "name": "rlgnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
