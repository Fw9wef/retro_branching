{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a policy evaluation table similar to Table 2 in Gasse et al. 2019 https://arxiv.org/pdf/1906.01629.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- FSB\n",
    "- RPB\n",
    "- PCB\n",
    "- SL\n",
    "- RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "from retro_branching.utils import get_most_recent_checkpoint_foldername\n",
    "\n",
    "import glob\n",
    "import gzip\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "\n",
    "import pandas as pd\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "difficulty_levels = ['easy'] # ['easy', 'medium', 'hard']\n",
    "problem_classes = ['sc'] # ['sc', 'ca', 'cfl', 'mis']\n",
    "rl_type = 'dqn_gnn'\n",
    "rl_id = 1236\n",
    "rl_cp = 224\n",
    "win_determinator = 'Nodes' # 'Time' 'Nodes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "# get rl agent name\n",
    "rl_agent_name = f'{rl_type}_{rl_id}_checkpoint_{rl_cp}'\n",
    "\n",
    "# map difficulty to problem type and size\n",
    "difficulty_to_problem_to_size = {'easy': \n",
    "                                     {'sc': [500, 1000],\n",
    "                                      'ca': [100, 500],\n",
    "                                      'cfl': [100],\n",
    "                                      'mis': [500]},\n",
    "                                 'medium': \n",
    "                                     {'sc': [1000, 1000],\n",
    "                                      'ca': [200, 1000],\n",
    "                                      'cfl': [200],\n",
    "                                      'mis': [1000]},\n",
    "                                 'hard': \n",
    "                                     {'sc': [2000, 1000],\n",
    "                                      'ca': [300, 1500],\n",
    "                                      'cfl': [400],\n",
    "                                      'mis': [1500]}}\n",
    "\n",
    "# map problem class to saved imitation agent\n",
    "problem_to_imitation_agent = {'sc': 'gnn_343_checkpoint_233',\n",
    "                              'ca': None,\n",
    "                              'cfl': None,\n",
    "                              'mis': None}\n",
    "\n",
    "# map saved agent names to their acronyms to display in table\n",
    "agent_name_to_acronym = {'strong_branching': 'FSB',\n",
    "                         'scip_branching': 'RPB',\n",
    "                         'pseudocost': 'PCB',\n",
    "                         f'{rl_agent_name}': 'RL'}\n",
    "for imitation_agent in problem_to_imitation_agent.values():\n",
    "    if imitation_agent is not None:\n",
    "        agent_name_to_acronym[imitation_agent] = 'SL'\n",
    "\n",
    "# get base path(s) for where validation data is stored for each benchmark\n",
    "def get_problem_validation_path(difficulty_level, problem_class):\n",
    "    params = difficulty_to_problem_to_size[difficulty_level][problem_class]\n",
    "    if problem_class == 'sc':\n",
    "        return f'/scratch/datasets/retro_branching/instances/set_cover_nrows_{params[0]}_ncols_{params[1]}_density_005_threshold_None/baselines/'\n",
    "    else:\n",
    "        raise NotImplemented(f'Not yet implemented validation path retrieval for problem_class {problem_class}')\n",
    "\n",
    "validation_paths = {difficulty_level: {problem_class: [] for problem_class in problem_classes} for difficulty_level in difficulty_levels}\n",
    "for difficulty_level in difficulty_levels:\n",
    "    for problem_class in problem_classes:\n",
    "        problem_baseline_path = get_problem_validation_path(difficulty_level, problem_class)\n",
    "        validation_paths[difficulty_level][problem_class].append(problem_baseline_path)\n",
    "\n",
    "# initialise table\n",
    "headers = ['Method', 'Time', 'Wins', 'Nodes']\n",
    "policy_evaluation_dict = {problem_class: \n",
    "                              {difficulty_level: \n",
    "                                   {header: [] for header in headers} \n",
    "                               for difficulty_level in difficulty_levels} \n",
    "                          for problem_class in problem_classes}\n",
    "for difficulty_level in difficulty_levels:\n",
    "    if difficulty_level != 'easy':\n",
    "        # pop model since can share model column across different problem classes in table\n",
    "        _ = policy_evaluation_dict[difficulty_level].pop('Method', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "# get agent paths\n",
    "difficulty_to_problem_to_agent_to_path = {difficulty_level:\n",
    "                                             {problem_class: {}\n",
    "                                             for problem_class in problem_classes}\n",
    "                                          for difficulty_level in difficulty_levels}\n",
    "for difficulty_level in validation_paths.keys():\n",
    "    for problem_class in validation_paths[difficulty_level].keys():\n",
    "        for validation_path in validation_paths[difficulty_level][problem_class]:\n",
    "            agent_names = [path.split('/')[-1] for path in glob.glob(validation_path+'/*')]\n",
    "            for agent_name in agent_names:\n",
    "                difficulty_to_problem_to_agent_to_path[difficulty_level][problem_class][agent_name] = validation_path+f'/{agent_name}/'\n",
    "\n",
    "# get agent names to show\n",
    "agent_names = [name for name in agent_names if name in agent_name_to_acronym.keys()]\n",
    "all_names = list(difficulty_to_problem_to_agent_to_path[difficulty_level][problem_class].keys())\n",
    "for difficulty_level in difficulty_levels:\n",
    "    for problem_class in problem_classes:\n",
    "        for agent_name in all_names:\n",
    "            if agent_name not in agent_names:\n",
    "                _ = difficulty_to_problem_to_agent_to_path[difficulty_level][problem_class].pop(agent_name, None)\n",
    "        \n",
    "# load data\n",
    "all_data = {problem_class: \n",
    "              {difficulty_level: \n",
    "                   {'raw_data': {header: [] for header in headers},\n",
    "                     'mean_data': {header: [] for header in headers},\n",
    "                     'ci_data': {header: [] for header in headers}} \n",
    "               for difficulty_level in difficulty_levels} \n",
    "          for problem_class in problem_classes}\n",
    "\n",
    "for problem_class in problem_classes:\n",
    "    for difficulty_level in difficulty_levels:\n",
    "        for agent_name in agent_names:\n",
    "            # load data\n",
    "            path = difficulty_to_problem_to_agent_to_path[difficulty_level][problem_class][agent_name] + 'rl_validator/rl_validator_1/'\n",
    "            path += get_most_recent_checkpoint_foldername(path)\n",
    "            with gzip.open(*glob.glob(path+'/*log.pkl'), 'rb') as f:\n",
    "                log = pickle.load(f)\n",
    "                \n",
    "            # get method acronym\n",
    "            try:\n",
    "                all_data[problem_class][difficulty_level]['raw_data']['Method'].append(agent_name_to_acronym[agent_name])\n",
    "                all_data[problem_class][difficulty_level]['mean_data']['Method'].append(agent_name_to_acronym[agent_name])\n",
    "                all_data[problem_class][difficulty_level]['ci_data']['Method'].append(agent_name_to_acronym[agent_name])\n",
    "            except KeyError:\n",
    "                # sharing Method column across difficulty levels in table\n",
    "                pass\n",
    "                \n",
    "            # collect solving time data\n",
    "            solving_times = [abs(np.sum(times)) for times in log[agent_name]['solving_time']]\n",
    "            all_data[problem_class][difficulty_level]['raw_data']['Time'].append(solving_times)\n",
    "            all_data[problem_class][difficulty_level]['mean_data']['Time'].append(np.mean(solving_times))\n",
    "            ci = st.norm.interval(alpha=0.68, loc=np.mean(solving_times), scale=st.sem(solving_times))\n",
    "            all_data[problem_class][difficulty_level]['ci_data']['Time'].append(ci)\n",
    "        \n",
    "            # collect num nodes data\n",
    "            num_nodes = [abs(np.sum(nodes)) for nodes in log[agent_name]['num_nodes']]\n",
    "            all_data[problem_class][difficulty_level]['raw_data']['Nodes'].append(num_nodes)\n",
    "            all_data[problem_class][difficulty_level]['mean_data']['Nodes'].append(np.mean(num_nodes))\n",
    "            ci = st.norm.interval(alpha=0.68, loc=np.mean(num_nodes), scale=st.sem(num_nodes))\n",
    "            all_data[problem_class][difficulty_level]['ci_data']['Nodes'].append(ci)\n",
    "            \n",
    "        # collect % wins data (in terms of win_determinator, assume lower is better)\n",
    "        win_counter = {name: 0 for name in agent_names}\n",
    "        agent_idx_to_name = {idx: agent_name for idx, agent_name in enumerate(agent_names)}\n",
    "        for instance_idx in range(len(solving_times)):\n",
    "            agent_solving_times = [all_data[problem_class][difficulty_level]['raw_data'][win_determinator][agent_idx][instance_idx] for agent_idx in agent_idx_to_name.keys()]\n",
    "            winner_agent = agent_idx_to_name[np.argmin(agent_solving_times)]\n",
    "            win_counter[winner_agent] += 1\n",
    "        for agent_name in agent_names:\n",
    "            all_data[problem_class][difficulty_level]['raw_data']['Wins'].append(win_counter[agent_name])\n",
    "            all_data[problem_class][difficulty_level]['mean_data']['Wins'].append(win_counter[agent_name])\n",
    "            all_data[problem_class][difficulty_level]['ci_data']['Wins'].append(win_counter[agent_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> sc <<<\n",
      "{'Method': ['SL', 'RL', 'FSB', 'PCB'], 'Time': [4.31815555247, 6.23711978466, 11.50239698041, 0.8240305886200001], 'Wins': [19, 3, 77, 1], 'Nodes': [66.2, 88.85, 48.84, 121.14]}\n",
      "+----------+-----------+--------+---------+\n",
      "| Method   |      Time |   Wins |   Nodes |\n",
      "|----------+-----------+--------+---------|\n",
      "| SL       |  4.31816  |     19 |   66.2  |\n",
      "| RL       |  6.23712  |      3 |   88.85 |\n",
      "| FSB      | 11.5024   |     77 |   48.84 |\n",
      "| PCB      |  0.824031 |      1 |  121.14 |\n",
      "+----------+-----------+--------+---------+\n",
      "  Method       Time  Wins   Nodes\n",
      "0     SL   4.318156    19   66.20\n",
      "1     RL   6.237120     3   88.85\n",
      "2    FSB  11.502397    77   48.84\n",
      "3    PCB   0.824031     1  121.14\n"
     ]
    }
   ],
   "source": [
    "for problem_class in problem_classes:\n",
    "    print(f'\\n>>> {problem_class} <<<')\n",
    "    for difficulty_level in difficulty_levels:\n",
    "        print(all_data[problem_class][difficulty_level]['mean_data'])\n",
    "        df = pd.DataFrame(all_data[problem_class][difficulty_level]['mean_data'])\n",
    "        print(tabulate(df, headers='keys', tablefmt='psql', showindex=False))\n",
    "        \n",
    "        latex_df = copy.deepcopy(df)\n",
    "#         latex_df.to_latex(index=False, multicolumn=True, buf=self.path_to_save+'/latex_summary_table', escape=False)\n",
    "        latex_df.to_latex(index=False, multicolumn=True, escape=False)\n",
    "        print(latex_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlgnn",
   "language": "python",
   "name": "rlgnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
